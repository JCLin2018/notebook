# zookeeper基本使用

## 一. ZooKeeper是什么

ZooKeeper由雅虎研究院开发，是Google Chubby的开源实现，后来托管到Apache，于2010年11月正式成为Apache的顶级项目。

ZooKeeper是一个经典的分布式数据一致性解决方案，致力于为分布式应用提供一个高性能、高可用，且具有严格顺序访问控制能力的分布式协调服务。

分布式应用程序可以基于ZooKeeper实现数据发布与订阅、负载均衡、命名服务、分布式协调与通知、集群管理、Leader选举、分布式锁、分布式队列等功能。

## 二. ZooKeeper目标

ZooKeeper致力于为分布式应用提供一个高性能、高可用，且具有严格顺序访问控制能力的分布式协调服务

### 2.1 高性能

ZooKeeper将全量**数据存储在内存**中，并直接服务于客户端的所有非事务请求，尤其适用于以读为主的应用场景

### 2.2 高可用

ZooKeeper一般以集群的方式对外提供服务，一般3 ~ 5台机器就可以组成一个可用的Zookeeper集群了，每台机器都会在内存中维护当前的服务器状态，并且每台机器之间都相互保持着通信。只要集群中超过一半的机器都能够正常工作，那么整个集群就能够正常对外服务

### 2.3 严格顺序访问

对于来自客户端的每个更新请求，ZooKeeper都会分配一个**全局唯一的递增**编号，这个编号反映了所有事务操作的先后顺序

## 三. ZooKeeper五大特性

ZooKeeper一般以集群的方式对外提供服务，一个集群包含多个节点，每个节点对应一台ZooKeeper服务器，所有的节点共同对外提供服务，整个集群环境对分布式数据一致性提供了全面的支持，具体包括以下五大特性：

### 3.1 顺序一致性

从同一个客户端发起的请求，最终将会严格按照其发送顺序进入ZooKeeper中

### 3.2 原子性

所有请求的响应结果在整个分布式集群环境中具备原子性，即要么整个集群中所有机器都成功的处理了某个请求，要么就都没有处理，绝对不会出现集群中一部分机器处理了某一个请求，而另一部分机器却没有处理的情况

### 3.3 单一性

无论客户端连接到ZooKeeper集群中哪个服务器，每个客户端所看到的服务端模型都是一致的，不可能出现两种不同的数据状态，因为ZooKeeper集群中每台服务器之间会进行数据同步

### 3.4 可靠性

一旦服务端数据的状态发送了变化，就会立即存储起来，除非此时有另一个请求对其进行了变更，否则数据一定是可靠的

### 3.5 实时性

当某个请求被成功处理后，ZooKeeper仅仅保证在一定的时间段内，客户端最终一定能从服务端上读取到最新的数据状态，即ZooKeeper保证数据的**最终一致性**

## 四. ZooKeeper集群角色

在分布式系统中，集群中每台机器都有自己的角色，ZooKeeper没有沿用传统的Master/Slave模式（主备模式），而是引入了Leader、Follower和Observer三种角色

### 4.1 Leader

集群通过一个Leader选举过程从所有的机器中选举一台机器作为”Leader”，Leader能为客户端提供读和写服务
Leader服务器是整个集群工作机制的核心，主要工作：

1. 事务请求的唯一调度者和处理者，保证集群事务处理的顺序性
2. 集群内部各服务器的调度者

### 4.2 Follower

> 当follower节点越多时，服务性能就下降，因为每一次事务处理，leader都要给所有follower发送复制请求，等到过半follower完成同步时，leader才能提交事务。

顾名思义，Follower是追随者，主要工作：

1. 参与Leader选举投票
2. 处理客户端非事务请求 - 即读服务
3. 转发事务请求给Leader服务器
4. 参与事务请求Proposal的投票

### 4.3 Observer

Observer是ZooKeeper自3.3.0版本开始引入的一个全新的服务器角色，充当一个观察者角色，工作原理和Follower基本是一致的，和Follower唯一的区别是Observer不参与任何形式的投票

1. 处理客户端非事务请求 - 即读服务
2. 转发事务请求给Leader服务器
3. 不参与Leader选举投票
4. 参与事务请求Proposal的投票

所以Observer可以在不影响写性能的情况下提升集群的读性能

## 五. 原子广播协议 - Zab

ZooKeeper并非采用经典的分布式一致性协议 - Paxos，而是参考了Paxos设计了一种更加轻量级的支持崩溃可恢复的原子广播协议-Zab（ZooKeeper Atomic Broadcast）。
ZAB协议分为两个阶段 - Leader Election（领导选举）和Atomic Broadcast（原子广播）

### 5.1 领导选举 - Leader Election

当集群启动时，会选举一台节点为Leader，而其他节点为Follower，当Leader节点出现网络中断、崩溃退出与重启等异常情况，ZAB会进入恢复模式并选举产生新的Leader服务器，当集群中已有过半机器与该Leader服务器完成数据状态同步，退出恢复模式

### 5.2 原子广播 - Zookeeper Atomic Broadcast (ZAB协议)

当领导选举完成后，就进入原子广播阶段。此时集群中已存在一个Leader服务器在进行消息广播，当一台同样遵循ZAB协议的服务器启动后加入到集群中，新加的服务器会自动进入数据恢复阶段

- 崩溃恢复
- 原子广播

**数据同步**

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128124530.jpg)

## 六. 事务请求

在ZooKeeper中，事务是指能够改变ZooKeeper服务器状态的请求，一般指创建节点、更新数据、删除节点以及创建会话操作

### 6.1 事务转发

为了保证事务请求被顺序执行，从而确保ZooKeeper集群的数据一致性，所有的事务请求必须由Leader服务器处理，ZooKeeper实现了非常特别的事务请求转发机制：
所有非Leader服务器如果接收到来自客户端的事务请求，必须将其转发给Leader服务器来处理

### 6.2 事务ID - ZXID

在分布式系统中，事务请求可能存在依赖关系，如变更C需要依赖变更A和变更B，这样就要求ZAB协议能够保证如果一个状态变更成功被处理了，那么其所有依赖的状态变更都应该已经提前被处理掉了。
在ZooKeeper中对每一个事务请求，都会为其分配一个全局唯一的事务ID，使用ZXID表示，通常是一个64位的数字。每一个ZXID对应一次事务，从这些ZXID可以间接识别出ZooKeeper处理这些事务请求的全局顺序

## 七. 数据节点 - ZNode

ZooKeeper内部拥有一个树状的内存模型，类似文件系统，只是在ZooKeeper中将这些目录与文件系统统称为ZNode，ZNode是ZooKeeper中数据的最小单元，每个ZNode上可以保存数据，还可以挂载子节点，因此构成了一个层次化的命名空间

### 7.1 节点路径

ZooKeeper中使用斜杠（/）分割的路径表示ZNode路径，斜杠（/）表示根节点

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128102451.jpg)

### 7.2 节点特性

在ZooKeeper中，每个数据节点ZNode都是有生命周期的，其生命周期的长短取决于ZNode的节点类型

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20180713122359628.png)

### 7.3 权限控制 - ACL

为了有效保障ZooKeeper中数据的安全，避免因误操作而带来数据随意变更导致分布式系统异常，ZooKeeper提供了一套完善的ACL（Access Contro List）权限控制机制来保障数据的安全。
可以从三个方面理解ACL机制，分别是：权限模式（Scheme）、授权对象（ID）和权限（Permission），通常使用” `scheme:id:permission` ”来标识一个有效的ACL信息

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128123549.jpg)

### 7.4 节点状态信息（Node Metadata）

每个数据节点ZNode除了存储数据内容外，还存储了数据节点本身的一些状态信息

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128123614.jpg)

### 7.5 节点版本（Node Metadata）

ZooKeeper为数据节点引入版本的概念，对个数据节点都具有三种类型的版本信息，对数据节点的任何更新操作都会引起版本号的变化

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128123640.jpg)

在分布式系统中，在运行过程中往往需要保证数据访问的排他性。Java并发中是实现了对CAS的指令支持，即对于值V，每次更新前都会比对其值是否是预期值A，只有符合预期，才会将V原子化的更新到新值B
而ZooKeeper每个节点都有数据版本的概念，在调用更新操作的时候，先从请求中获取当前请求的版本version，同时获取服务器上该数据最新版本currentVersion，如果无法匹配，就无法更新成功，这样可以有效避免一些分布式更新的并发问题

## 八. Watcher - 数据变更的通知

在ZooKeeper中，引入Watcher机制来实现分布式数据的发布/订阅功能。ZooKeeper允许客户端向服务器注册一个Watcher监听，当服务器的一些指定事件触发了这个Watcher，那么就会向指定客户端发送一个事件通知来实现分布式的通知功能

Watcher机制为以下三个过程：

### 8.1 客户端注册Watcher

在创建一个ZooKeeper客户端对象实例时，可以向构造方法中传入一个Watcher，这个Watcher将作为整个ZooKeeper会话期间的默认Watcher，一致保存在客户端，并向ZooKeeper服务器注册Watcher
客户端并不会把真实的Watcher对象传递到服务器，仅仅只是在客户端请求中使用boolean类型属性进行标记，降低网络开销和服务器内存开销

### 8.2 服务端处理Watcher

服务端执行数据变更，当Watcher监听的对应数据节点的数据内容发生变更，如果找到对应的Watcher，会将其提取出来，同时从管理中将其删除（说明Watcher在服务端是一次性的，即触发一次就失效了），触发Watcher，向客户端发送通知

### 8.3 客户端回调Watcher

客户端获取通知，识别出事件类型，从相应的Watcher存储中去除对应的Watcher（说明客户端也是一次性的，即一旦触发就会失效）

#### 8.4 总结

1. 一致性：无论是客户端还是服务器，一旦一个Watcher被处罚，ZooKeeper都会将其从相应的存储中移除，因此开发人员在Watcher使用上要反复注册，这样可以有效减轻服务器压力
2. 客户端串行执行：客户端Watcher回调的过程是一个串行同步的过程，这保证了顺序
3. 轻量：客户端并不会把真实的Watcher对象传递到服务器，仅仅只是在客户端请求中使用boolean类型属性进行标记，降低网络开销和服务器内存开销

## 九. Session - 会话

Session是指客户端连接 - 客户端和服务器之间的一个TCP长连接

### 9.1 会话状态

会话在整个生命周期中，会在不同的会话转态之间进行切换

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128123711.jpg)

### 9.2 Session属性

Session是ZooKeeper中的会话实体，代表了一个客户端会话，其包含4个属性：

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201128123721.jpg)

### 9.3 心跳检测

为了保证客户端会话的有效性，客户端会在会话超时时间范围内向服务器发送PING请求来保持会话的有效性，即心跳检测。
服务器接收到客户端的这个心跳检测，就会重新激活对应的客户端会话

### 9.4 会话清理

服务器的超级检查线程会在指定时间点进行检查，整理出一些已经过期的会话后，就要开始进行会话清理了：

1. 关闭会话
2. 清理相关的临时节点

### 9.5 重连

当客户端和服务器之间网络连接断开，客户端会自动进行反复的重连，直到最终成功连接上ZooKeeper集群中的一台机器

1. 在会话超时时间内重新连接上，被视为重连成功
2. 在会话超时时间外重新连接上，此时服务器已经进行了会话清理，但客户端不知道会话已经失效，重新连接服务器会告诉客户端会话已失效，被视为非法会话

## 十.简单部署

**单机部署**

1. 修改zoo.cfg 
2. sh zkServer.sh start

```sh
# 常用命令
#1. 启动ZK服务:
sh zkServer.sh start
#2. 查看ZK服务状态:
sh zkServer.sh status
#3. 停止ZK服务:
sh zkServer.sh stop
#4. 重启ZK服务:
sh zkServer.sh restart
#5. 连接服务器
sh zkCli.sh -timeout 0 -r -server ip:port
```

**集群部署**

三台机器同时操作

1. 修改zoo.cfg配置文件

   ```sh
   # 相同一台机器部署三个zookeeper则需要修改2181端口
   # 【2888：访问zookeeper的端口；3888：重新选举leader的端口】
   server.1=IP1:2888:3888
   server.2=IP2.2888:3888
   server.3=IP3.2888:2888
   
   # =========================================
   server.A=B:C:D  
   其中
   1. A 是一个数字，表示这个是第几号服务器；
   2. B 是这个服务器的 ip地址；
   3. C 表示的是这个服务器与集群中的 Leader 服务器交换信息的端口；
   4. D 表示的是万一集群中的 Leader 服务器挂了，需要一个端口来重新进行选举，选出一个新
   的 Leader，而这个端口就是用来执行选举时服务器相互通信的端口。如果是伪集群的配置方
   式，由于 B 都是一样，所以不同的 Zookeeper 实例通信端口号不能一样，所以要给它们分配
   不同的端口号。
   5. 在集群模式下，集群中每台机器都需要感知到整个集群是由哪几台机器组成的，在配置文件
   中，按照格式server.id=host:port:port，每一行代表一个机器配置。id: 指的是server ID,用
   来标识该机器在集群中的机器序号
   ```

   ![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/1120165-20181027123636869-436165376.png)

2.  新建datadir目录，设置myid

   在每台zookeeper机器上，我们都需要在数据目录(dataDir)下创建一个myid文件，该文件只有一 行内容，对应每台机器的Server ID数字；比如server.1的myid文件内容就是1。【必须确保每个服 务器的myid文件中的数字不同，并且和自己所在机器的zoo.cfg中server.id的id值一致，id的范围 是1~255】 

   ![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/1120165-20181027124711410-1015969668.png)

3. 启动zookeeper



## 为什么zookeeper节点是奇数？

1. zookeeper集群的写操作，由leader节点负责，它会把通知所有节进行写入操作，只有收到半数以上节点的成功反馈，才算成功。如果是部署2个节点的话，那就必须都成功。
2. zookeeper的选举策略也是需要半数以上的节点同意才能当选leader，如果是偶数节点可能导致票数相同的情况
3. 只有当半数以上的节点存活时 zookeeper集群才能对外服务，维持正常状态，如果是2个节点，只要其中一个挂掉，那么剩下的1个并不满足半数以上规则。

- **容错率**

  ```sh
  首先从容错率来说明：（需要保证集群能够有半数进行投票）
  (1) 2台服务器，至少2台正常运行才行（2的半数为1，半数以上最少为2），宕掉其中一台集群无法工作，所以直接排除了。
  (2) 3台服务器，至少2台正常运行才行（3的半数为1.5，半数以上最少为2），正常运行可以允许1台服务器挂掉
  (3) 4台服务器，至少3台正常运行才行（4的半数为2，半数以上最少为3），正常运行可以允许1台服务器挂掉
  (4) 5台服务器，至少3台正常运行才行（5的半数为2.5，半数以上最少为3），正常运行可以允许2台服务器挂掉
  ```

- **防脑裂**

  ```sh
  脑裂集群的脑裂通常是发生在节点之间通信不可达的情况下，集群会分裂成不同的小集群，小集群各自选出自己的leader节点，导致原有的集群出现多个leader节点的情况，这就是脑裂。
  
  (1) 3台服务器，投票选举半数为1.5，一台服务裂开，和另外两台服务器无法通行，这时候2台服务器的集群（2票大于半数1.5票），所以可以选举出leader，而 1 台服务器的集群无法选举。
  (2) 4台服务器，投票选举半数为2，可以分成1(该集群只有一个节点),3(该集群只有三个节点)两个集群或者2(该集群只有两个节点),2(该集群只有两个节点)两个集群，对于 1,3集群可以选举（因为4的过半是3，只能有三个节点的集群才能选举）;对于2,2集群，则能选举出leader
  (3) 5台服务器，投票选举半数为2.5，可以分成1,4两个集群，或者2,3两集群，这两个集群分别都只能选举一个集群，满足zookeeper集群搭建数目。
  
  以上分析，我们从容错率以及防止脑裂两方面说明了3台服务器是搭建集群的最少数目，4台发生脑裂时会造成没有leader节点的错误。
  ```



## 十一.API使用

针对zookeeper，比较常用的Java客户端有zkclient、curator。由于Curator对于zookeeper的抽象层次比较高，简化了zookeeper客户端的开发量。使得curator逐步被广泛应用。 

1. 封装zookeeper client与zookeeper server之间的连接处理 
2. 提供了一套fluent风格的操作api 
3. 提供zookeeper各种应用场景（共享锁、leader选举）的抽象封装

```xml
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-framework</artifactId>
    <version>4.2.0</version>
</dependency>

<!--分布式锁、leader选举、队列...-->
<dependency>
    <groupId>org.apache.curator</groupId>
    <artifactId>curator-recipes</artifactId>
    <version>4.2.0</version>
</dependency>
```



### CRUD操作

```java
public static void main(String[] args) throws Exception {
    CuratorFramework curatorFramework= CuratorFrameworkFactory.builder()
        .connectString("192.168.216.128:2181,192.168.216.129:2181,192.168.216.130:2181")
        .sessionTimeoutMs(5000) // 会话超时时间，单位毫秒，默认60000ms
        .retryPolicy(new ExponentialBackoffRetry(1000,3)) // 重试策略,内建有四种重试策略,也可以自行实现RetryPolicy接口
        .onnectionTimeoutMs(4000) // 连接创建超时时间，单位毫秒，默认60000ms
        .build();
    curatorFramework.start(); //表示启动.
    //创建
    create(curatorFramework);
}
// 创建节点
private static String create(CuratorFramework curatorFramework) throws Exception {
    String path = curatorFramework.create()
        .creatingParentsIfNeeded() // 如果父节点不存在也会创建
        .withMode(CreateMode.PERSISTENT)
        .forPath("/fir1111","Hello Gupaao".getBytes());
    System.out.println("创建成功的节点： "+path);
    
    
    // 创建普通节点(默认是持久节点),内容为空
    curatorFramework.create().forPath("/t1");
    // 创建普通节点(默认是持久节点)
    curatorFramework.create().forPath("/t2", "123456".getBytes());
    // 创建永久顺序节点
    curatorFramework.create().withMode(CreateMode.PERSISTENT_SEQUENTIAL).forPath("/t3", "123456".getBytes());
    // 地柜创建，如果父节点不存在也会创建
    curatorFramework.create().creatingParentContainersIfNeeded()
        .withMode(CreateMode.PERSISTENT_SEQUENTIAL)
        .forPath("/t4/t41/t411", "123456".getBytes());
    
    return path;
}

// 获取节点数据
private static String get(CuratorFramework curatorFramework) throws Exception {
    // 读取数据不获取stat
    String rs = new String(curatorFramework.getData().forPath("/first_auth"));
    System.out.println(rs);
    return rs;
    
    // 读取数据且获取stat
    Stat stat = new Stat();
    byte[] forPath2 = curatorFramework.getData().storingStatIn(stat).forPath("/t4");
    System.out.println(new String(forPath2, "UTF-8"));
    System.out.println(stat);
    
    // 注册观察者，当节点变动时触发
    byte[] data = curatorFramework.getData().usingWatcher(new Watcher() {
        @Override
        public void process(WatchedEvent event) {
            System.out.println(event.getType());
        }
    }).forPath("/t4");
    System.out.println("/t4: " + new String(data));
    
}

// 删除节点数据
private static String get(CuratorFramework curatorFramework) throws Exception {
    // 删除子节点，只能删除叶子节点
    curatorFramework.delete().forPath("/t2");
    // 递归删除
    curatorFramework.delete().deletingChildrenIfNeeded().forPath("/t4/t41");
    // 指定版本进行删除
    curatorFramework.delete().withVersion(0).forPath("/t1");
    // 强制删除。guaranteed()接口是一个保障措施，只要客户端会话有效，那么Curator会在后台持续进行删除操作，直到删除节点成功。
    curatorFramework.delete().guaranteed().forPath("/t30000000002");
    return rs;
}

// 更新节点数据
private static String update(CuratorFramework curatorFramework) throws Exception {
    // 更新数据
    Stat forPath = curatorFramework.setData().forPath("/t4", "data".getBytes());

    // 更新数据, 返回stat数据
    Stat stat = new Stat();
    curatorFramework.getData().storingStatIn(stat).forPath("/t4", "data1".getBytes());
    
    // 更新一个节点的数据内容，强制指定版本进行更新
    Stat forPath2 = curatorFramework.setData().withVersion(stat.getVersion()).forPath("/t4", "data222".getBytes());
}

// 检查节点是否存在
Stat forPath = curatorFramework.checkExists().forPath("/t4");
if (forPath != null) {
    System.out.println("exists");
} else {
    System.out.println("not exists");
}

// 获取某个节点的所有子节点路径--这个获取的是子节点的名称且不带/
List<String> forPath = curatorFramework.getChildren().forPath("/");
System.out.println(forPath);


// 事务： 允许作为一个原子操作进行提交
// inTransaction( )方法开启一个ZooKeeper事务.可以复合create, setData, check, and/or delete 等操作然后调用commit()作为一个原子操作提交
curatorFramework.inTransaction()
    .check().forPath("/t4")
    .and()
    .create().withMode(CreateMode.EPHEMERAL).forPath("/t3", "data".getBytes())
    .and()
    .setData().forPath("/t3", "data2".getBytes())
    .and()
    .commit();


//异步访问 | 同步（future.get()）
private static String operatorWithAsync(CuratorFramework curatorFramework) throws Exception {
    CountDownLatch countDownLatch=new CountDownLatch(1);
    curatorFramework.create().creatingParentsIfNeeded().
        withMode(CreateMode.PERSISTENT).inBackground(new BackgroundCallback() {
        @Override
        public void processResult(CuratorFramework client, CuratorEvent event) throws Exception {
            System.out.println(Thread.currentThread().getName()+":"+event.getResultCode());
            countDownLatch.countDown();
        }
    }).forPath("/second","second".getBytes());
    System.out.println("before");
    countDownLatch.await(); //阻塞
    System.out.println("after");
    return "";
}

// ======================= 权限 =======================
// 添加授权 创建节点
private static String authOperation(CuratorFramework curatorFramework) throws Exception {
    List<ACL> acls = new ArrayList<>();
    // u1 拥有create 和 delete权限
    ACL acl = new ACL(ZooDefs.Perms.CREATE | ZooDefs.Perms.DELETE,new Id("digest", DigestAuthenticationProvider.generateDigest("u1:u1")));
    // u2 拥有 全部权限
    ACL acl1 = new ACL(ZooDefs.Perms.ALL,new Id("digest", DigestAuthenticationProvider.generateDigest("u2:u2")));
    acls.add(acl);
    acls.add(acl1);
    
    curatorFramework.create().creatingParentsIfNeeded()
        .withMode(CreateMode.PERSISTENT)
        .withACL(acls).forPath("/first_auth", "123".getBytes());
    return null;
}
// 授权访问节点
List<AuthInfo> list=new ArrayList<>();
AuthInfo authInfo=new AuthInfo("digest","u2:u2".getBytes()); // 账户
list.add(authInfo);
CuratorFramework curatorFramework= CuratorFrameworkFactory.builder()
        .connectString("192.168.216.128:2181,192.168.216.129:2181,192.168.216.130:2181")
        .sessionTimeoutMs(5000) // 会话超时时间，单位毫秒，默认60000ms
        .retryPolicy(new ExponentialBackoffRetry(1000,3)) // 重试策略,内建有四种重试策略,也可以自行实现RetryPolicy接口
        .onnectionTimeoutMs(4000) // 连接创建超时时间，单位毫秒，默认60000ms
	    .authorization(list) // 添加授权访问
        .build();


// ======================= 订阅事件 =======================
public static void main(String[] args) throws Exception {
        List<AuthInfo> list=new ArrayList<>();
        AuthInfo authInfo=new AuthInfo("digest","u2:u2".getBytes());
        list.add(authInfo);
        CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()
            .connectString("192.168.216.128:2181,192.168.216.129:2181,192.168.216.130:2181")
            .sessionTimeoutMs(5000)
            .retryPolicy(new ExponentialBackoffRetry(1000,3))
            .connectionTimeoutMs(4000)
            .authorization(list)
            .build();
        curatorFramework.start(); //表示启动.

        addNodeCacheListener(curatorFramework,"/first"); // 添加当前节点事件监听
        addPathChildCacheListener(curatorFramework,"/first"); // 添加当前节点子节点事件监听
        System.in.read();
    }

private static void addNodeCacheListener(CuratorFramework curatorFramework, String path) throws Exception {
    NodeCache nodeCache=new NodeCache(curatorFramework,path,false);
    NodeCacheListener nodeCacheListener=new NodeCacheListener() {
        @Override
        public void nodeChanged() throws Exception {
            System.out.println("Receive Node Changed");
            System.out.println(""+nodeCache.getCurrentData().getPath()+"->"+new String(nodeCache.getCurrentData().getData()));
        }
    };
    nodeCache.getListenable().addListener(nodeCacheListener);
    nodeCache.start();
}
private static void addPathChildCacheListener(CuratorFramework curatorFramework,String path) throws Exception {
    PathChildrenCache childrenCache=new PathChildrenCache(curatorFramework,path,true);
    PathChildrenCacheListener childrenCacheListener=new PathChildrenCacheListener() {
        @Override
        public void childEvent(CuratorFramework curatorFramework, PathChildrenCacheEvent pathChildrenCacheEvent) throws Exception {
            System.out.println("子节点事件变更的回调");
            ChildData childData=pathChildrenCacheEvent.getData();
            System.out.println(childData.getPath()+"-"+new String(childData.getData()));
        }
    };
    childrenCache.getListenable().addListener(childrenCacheListener);
    childrenCache.start(PathChildrenCache.StartMode.NORMAL);
}


// zookeeper原生 watcher之后，又重新注册监听
private ZooKeeper zooKeeper;
public void originApiTest() throws IOException, KeeperException, InterruptedException {
    ZooKeeper zooKeeper = new ZooKeeper("192.168.216.128:2181", 5000, new Watcher() {
        @Override
        public void process(WatchedEvent watchedEvent) {
            //表示连接成功之后，会产生的回调时间
        }
    });
    Stat stat = new Stat();
    zooKeeper.getData("/first", new DataWatchListener(), stat); // 针对当前节点进行监听

    /*  zooKeeper.exists();  //针对当前节点
        zooKeeper.getChildren();  //针对子节点的监听*/
}
class DataWatchListener implements Watcher {
    @Override
    public void process(WatchedEvent watchedEvent) {
        String path = watchedEvent.getPath();
        try {
            //再次注册监听
            zooKeeper.getData(path, this, new Stat());
        } catch (KeeperException e) {
            e.printStackTrace();
        } catch (InterruptedException e) {
            e.printStackTrace();
        }
    }
}

```

### 分布式锁应用

```java
public static void main(String[] args) {

    CuratorFramework curatorFramework = CuratorFrameworkFactory.builder()
        .connectString("192.168.216.128:2181,192.168.216.129:2181,192.168.216.130:2181")
        .sessionTimeoutMs(5000)
        .retryPolicy(new ExponentialBackoffRetry(1000,3))
        .connectionTimeoutMs(4000)
        .build();
    curatorFramework.start(); //表示启动.

    /**
    * locks 表示命名空间
    * 锁的获取逻辑是放在zookeeper
    * 当前锁是跨进程可见
    */
    InterProcessMutex lock=new InterProcessMutex(curatorFramework, "/locks");
    for(int i=0;i<10;i++){
        new Thread(()->{
            System.out.println(Thread.currentThread().getName()+"->尝试抢占锁");
            try {
                lock.acquire();// 抢占锁,没有抢到，则阻塞
                System.out.println(Thread.currentThread().getName()+"->获取锁成功");
            } catch (Exception e) {
                e.printStackTrace();
            }
            try {
                Thread.sleep(4000);
                lock.release(); //释放锁
                System.out.println(Thread.currentThread().getName()+"->释放锁成功");
            } catch (InterruptedException e) {
                e.printStackTrace();
            } catch (Exception e) {
                e.printStackTrace();
            }
        },"t-"+i).start();
    }

}

```

