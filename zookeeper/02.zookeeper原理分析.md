# zookeeper原理分析

## 利用zookeeper进行Master选举

> 分布式调度任务系统里，从可靠性角度出发，Master集群也是必不可少的。但往往，为了保证任务不会重复分配，分配任务的节点只能有一个，这种情况就需要从Master集群中选出一个Leader（老大）去任务池里取任务，本文就会介绍Curator基于Zookeeper封装的Leader选举工具类LeaderLatch与LeaderSelector的使用及原理分析

### Leader Latch

#### 基本原理

选择一个根路径，例如"/leader_select"，多个机器同时向该根路径下创建**临时顺序节点**，如"/leader_latch/node_3"，"/leader_latch/node_1"，"/leader_latch/node_2"，节点编号最小(这里为node_1)的zk客户端成为leader，没抢到Leader的节点都监听前一个节点的删除事件，在前一个节点删除后进行重新抢主

> org.apache.curator.framework.recipes.leader.LeaderLatch#LeaderLatch

```java
/**
 * LeaderLatch
 * @param client zk客户端实例
 * @param latchPath Leader选举根节点路径
 * @param id 客户端id，用来标记客户端，即客户端编号、名称
 * @param closeMode Latch关闭策略，SILENT-关闭时不触发监听器回调，NOTIFY_LEADER-关闭时触发监听器回调方法，默认不触发
 */
public LeaderLatch(CuratorFramework client, String latchPath, String id, CloseMode closeMode)
{
    this.client = Preconditions.checkNotNull(client, "client cannot be null").newWatcherRemoveCuratorFramework();
    this.latchPath = PathUtils.validatePath(latchPath);
    this.id = Preconditions.checkNotNull(id, "id cannot be null");
    this.closeMode = Preconditions.checkNotNull(closeMode, "closeMode cannot be null");
}


//调用start方法开始抢主
void start()
//调用close方法释放leader权限
void close()
//await方法阻塞线程，尝试获取leader权限，但不一定成功，超时失败
boolean await(long, java.util.concurrent.TimeUnit)
//判断是否拥有leader权限
boolean hasLeadership()
```

> org.apache.curator.framework.recipes.leader.LeaderLatchListener

```java
public interface LeaderLatchListener {
  	// 抢主成功时触发
	public void isLeader();
	// 抢主失败时触发
	public void notLeader();
}
```

具体使用

QuartzConfig.java

```java
@Configuration
public class QuartzConfig {

    @Bean
    public ZkSchedulerFactoryBean schedulerFactoryBean() throws Exception {
        ZkSchedulerFactoryBean schedulerFactoryBean = new ZkSchedulerFactoryBean();
        schedulerFactoryBean.setJobDetails(jobDetail());
        schedulerFactoryBean.setTriggers(trigger());
        return schedulerFactoryBean;
    }


    @Bean
    public JobDetail jobDetail() {
        return JobBuilder.newJob(QuartzJob.class).storeDurably().build();
    }

    @Bean
    public Trigger trigger() {
        SimpleScheduleBuilder simpleScheduleBuilder =
                SimpleScheduleBuilder.simpleSchedule().
                        withIntervalInSeconds(1).repeatForever();
        return TriggerBuilder.newTrigger().forJob(jobDetail()).
                withSchedule(simpleScheduleBuilder).build();
    }
}

```

QuartzJob.java

```java
public class QuartzJob extends QuartzJobBean {
    @Override
    protected void executeInternal(JobExecutionContext jobExecutionContext) throws JobExecutionException {
        System.out.println("[QuartzJob]-----:" + new SimpleDateFormat("yyyy-MM-dd HH:mm:ss").format(new Date()));
    }
}
```

ZkSchedulerFactoryBean.java

```java
public class ZkSchedulerFactoryBean extends SchedulerFactoryBean {
    private static final Logger LOG = LoggerFactory.getLogger(ZkSchedulerFactoryBean.class);

    private static CuratorFramework zkClient;
    private static String ZOOKEEPER_CONNECTION_STRING = "192.168.216.128:2181";
    private LeaderLatch leaderLatch; //leader选举的api
    private static final String LEADER_PATH = "/leader";


    public ZkSchedulerFactoryBean() throws Exception {
        this.setAutoStartup(false); //设置为非自动启动
        leaderLatch = new LeaderLatch(getClient(), LEADER_PATH);
        leaderLatch.addListener(new ZkJobLeaderLatchListener(getIp(), this));
        leaderLatch.start(); //表示当前节点参与到leader选举中来
    }

    @Override
    protected void startScheduler(Scheduler scheduler, int startupDelay) throws SchedulerException {
        if (this.isAutoStartup()) {//默认情况下，是true
            super.startScheduler(scheduler, startupDelay);
        }
    }

    @Override
    public void destroy() throws SchedulerException {
        if (leaderLatch != null) {
            try {
                leaderLatch.close(); // 释放leader权限 重新进行抢主
            } catch (IOException e) {
                e.printStackTrace();
            }
        }
        super.destroy();
    }

    //初始化连接
    private CuratorFramework getClient() {
        // 重试规则
        RetryPolicy retryPolicy = new ExponentialBackoffRetry(1000, 3);
        zkClient = CuratorFrameworkFactory.builder()
                .connectString(ZOOKEEPER_CONNECTION_STRING)
                .retryPolicy(retryPolicy)
                .build();
        zkClient.start();
        return zkClient;
    }

    private String getIp() {
        String host = null;
        try {
            host = InetAddress.getLocalHost().getHostAddress();
        } catch (UnknownHostException e) {
            e.printStackTrace();
        }
        return host;
    }

    class ZkJobLeaderLatchListener implements LeaderLatchListener {

        private String ip;
        private SchedulerFactoryBean schedulerFactoryBean;

        public ZkJobLeaderLatchListener(String ip, SchedulerFactoryBean schedulerFactoryBean) {
            this.ip = ip;
            this.schedulerFactoryBean = schedulerFactoryBean;
        }

        @Override
        public void isLeader() {
            LOG.info("ip:{} 成为leader，执行scheduler~", ip);
            schedulerFactoryBean.setAutoStartup(true);
            schedulerFactoryBean.start(); //启动（抢占到leader的节点去执行任务）
        }

        @Override
        public void notLeader() {
            LOG.info("ip:{} 不是leader，停止scheduler~", ip);
            schedulerFactoryBean.setAutoStartup(false);
            schedulerFactoryBean.stop(); //启动（抢占到leader的节点去执行任务）
        }
    }

}
```



### Leader Selector

#### 基本原理

利用Curator中InterProcessMutex分布式锁进行抢主，抢到锁的即为Leader

> org.apache.curator.framework.recipes.leader.LeaderSelector

```java
// 开始抢主
void start()
// 在抢到leader权限并释放后，自动加入抢主队列，重新抢主
void autoRequeue()
```

> org.apache.curator.framework.recipes.leader.LeaderSelectorListener

```java
// 抢主成功后的回调
public void takeLeadership(CuratorFramework client) throws Exception;
```



## Zookeeper的实现原理分析

### 数据一致性模型

- 弱一致性模型 
  - 2pc协议( 原子性 ) 
  - 过半提交 

zookeeper是一个顺序一致性模型。由于zookeeper设计出来是提供分布式锁服务，那么意味着它本身 需要实现顺序一致性（http://zookeeper.apache.org/doc/r3.5.5/zookeeperProgrammers.html#ch_zkGuarantees ）

### 顺序一致性

在讲顺序一致性之前，咱们思考一个问题，假如说zookeeper是一个最终一致性模型，那么他会发生什么情况 

ClientA/B/C假设只串行执行， clientA更新zookeeper上的一个值x。ClientB和clientC分别读取集群的不同副本，返回的x的值是不一样的。clientC的读取操作是发生在clientB之后，但是却读到了过期的 值。很明显，这是一种弱一致模型。如果用它来实现锁机制是有问题的。

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201130164716.jpg)

顺序一致性提供了更强的一致性保证，我们来观察下面这个图，从时间轴来看，B0发生在A0之前，读 取的值是0，B2发生在A0之后，读取到的x的值为1.而读操作B1/C0/C1和写操作A0在时间轴上有重叠， 因此他们可能读到旧的值为0，也可能读到新的值1. 但是在强顺序一致性模型中，如果B1得到的x的值 为1，那么C1看到的值也一定是1.

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201130164740.jpg)

需要注意的是：由于网络的延迟以及系统本身执行请求的不确定性，会导致请求发起的早的客户端不一 定会在服务端执行得早。最终以服务端执行的结果为准。

简单来说：顺序一致性是针对单个操作，单个数据对象。属于CAP中C这个范畴。一个数据被更新后， 能够立马被后续的读操作读到。

但是zookeeper的顺序一致性实现是缩水版的，在下面这个网页中，可以看到官网对于一致性这块做了 解释 https://zookeeper.apache.org/doc/r3.6.1/zookeeperProgrammers.html#ch_zkGuarantees  zookeeper不保证在每个实例中，两个不同的客户端具有相同的zookeeper数据视图，由于网络延迟等 因素，一个客户端可能会在另外一个客户端收到更改通知之前执行更新， 考虑到2个客户端A和B的场景，如果A把znode /a的值从0设置为1，然后告诉客户端B读取 /a， 则客户 端B可能会读取到旧的值0，具体取决于他连接到那个服务器，如果客户端A和B要读取必须要读取到相 同的值，那么client B在读取操作之前执行sync方法。 zooKeeper.sync();

## ZAB(Zookeeper Atomic Broadcast)

ZAB（Zookeeper Atomic Broadcast） 协议是为分布式协调服务 ZooKeeper 专门设计的一种支持崩溃恢复的原子广播协议。在 ZooKeeper 中，主要依赖 ZAB 协议来实现分布式数据一致性，基于该协议， ZooKeeper 实现了一种主备模式的系统架构来保持集群中各个副本之间的数据一致性。

- 崩溃恢复
  - 
- 原子广播
  - Observer(不参与投票和ack，只和leader保持数据同步)
  - zxid(64) 事务ID，高32位是epoch(leader选举次数)，低32位代表递增编号







