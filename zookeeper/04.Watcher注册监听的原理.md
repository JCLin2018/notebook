# Watcher注册监听的原理

**简单demo**

```java
public class TestZooKeeperWatch {

    public void test() throws KeeperException, InterruptedException, IOException {
        ZooKeeper zooKeeper = new ZooKeeper("localhost:2181", 5000, new Watcher() {
            @Override
            public void process(WatchedEvent watchedEvent) {
                // 表示连接成功后，会产生回调事件
                System.out.println(watchedEvent.getPath());
            }
        });

        Stat stat = new Stat();
        // 监听单个节点变化；需要重新注册监听
        zooKeeper.getData("/first", new DataWatchListener(), stat);
        // zookeeper3.6之后出现的功能
        zooKeeper.addWatch("/first", new DataWatchListener(), AddWatchMode.PERSISTENT); // 持久化监听，不需要重新注册监听
        zooKeeper.addWatch("/first", new DataWatchListener(), AddWatchMode.PERSISTENT_RECURSIVE); // 持久化递归监听，不需要重新注册监听

        zooKeeper.exists("first", new DataWatchListener());

        System.in.read();
    }


    static class DataWatchListener implements Watcher {
        @Override
        public void process(WatchedEvent watchedEvent) {
            System.out.println(watchedEvent.getPath());
        }
    }

}

```

## 客户端发送链接

### new ZooKeeper

`ZooKeeper zookeeper=new ZooKeeper("localhost:2181")` Zookeeper在初始化的时候， 会构建一个Watcher，我们可以先看看Zookeeper初始化做了什么事情。

```java
public ZooKeeper(
    String connectString,
    int sessionTimeout,
    Watcher watcher, // 用户传入的watcher
    boolean canBeReadOnly,
    HostProvider aHostProvider,
    ZKClientConfig clientConfig) throws IOException {
    
    LOG.info("Initiating client connection, connectString={} sessionTimeout={} watcher={}",
        connectString,
        sessionTimeout,
        watcher);

    if (clientConfig == null) {
        clientConfig = new ZKClientConfig();
    }
    this.clientConfig = clientConfig;
    watchManager = defaultWatchManager();
    watchManager.defaultWatcher = watcher;  // 在这里将watcher, 是用户传入的watcher
    ConnectStringParser connectStringParser = new ConnectStringParser(connectString);
    hostProvider = aHostProvider;
	// 初始化了ClientCnxn，并且调用cnxn.start()方法
    cnxn = createConnection(
        connectStringParser.getChrootPath(),
        hostProvider,
        sessionTimeout,
        this,
        watchManager,
        getClientCnxnSocket(),
        canBeReadOnly);
    cnxn.start();
}
```

> createConnection ，初始化一个ClientCnxn（是Zookeeper客户端和Zookeeper服务器端进行通信和事件通知处理的主要类）
>

```java
protected ClientCnxn createConnection(
    String chrootPath,
    HostProvider hostProvider,
    int sessionTimeout,
    ZooKeeper zooKeeper,
    ClientWatchManager watcher,
    ClientCnxnSocket clientCnxnSocket,
    boolean canBeReadOnly) throws IOException {
    
    return new ClientCnxn(
        chrootPath,
        hostProvider,
        sessionTimeout,
        this,
        watchManager,
        clientCnxnSocket,
        canBeReadOnly);
}
```

### ClientCnxn初始化

```java
public ClientCnxn(
    String chrootPath,
    HostProvider hostProvider,
    int sessionTimeout,
    ZooKeeper zooKeeper,
    ClientWatchManager watcher,
    ClientCnxnSocket clientCnxnSocket,
    long sessionId,
    byte[] sessionPasswd,
    boolean canBeReadOnly) {
    
    this.zooKeeper = zooKeeper;
    this.watcher = watcher;
    this.sessionId = sessionId;
    this.sessionPasswd = sessionPasswd;
    this.sessionTimeout = sessionTimeout;
    this.hostProvider = hostProvider;
    this.chrootPath = chrootPath;
	
    // 连接超时时间
    connectTimeout = sessionTimeout / hostProvider.size();
    readTimeout = sessionTimeout * 2 / 3;
    readOnly = canBeReadOnly;
	// 初始化一个sendThread （发送处理线程）
    sendThread = new SendThread(clientCnxnSocket);
    // 初始化一个EventThread （事件处理线程）
    eventThread = new EventThread();
    this.clientConfig = zooKeeper.getClientConfig();
    initRequestTimeout();
    
}
```

## zookeeper.exists 注册watcher监听

exists是用来判断一个节点是否存在，同时，还会针对这个节点注册一个watcher事件。

```java
zooKeeper.exists("/first", new DataWatchListener());
```



```java
public Stat exists(final String path, Watcher watcher) throws KeeperException, InterruptedException {
    final String clientPath = path;
    PathUtils.validatePath(clientPath);

    // the watch contains the un-chroot path
    WatchRegistration wcb = null;
    if (watcher != null) {
        wcb = new ExistsWatchRegistration(watcher, clientPath);
    }

    final String serverPath = prependChroot(clientPath);

    RequestHeader h = new RequestHeader();// 构建请求头，
    h.setType(ZooDefs.OpCode.exists);// 表示当前请求的操作类型是exists
    ExistsRequest request = new ExistsRequest(); // 构建发送请求和响应的response
    request.setPath(serverPath);
    request.setWatch(watcher != null);
    SetDataResponse response = new SetDataResponse();
    // 通过submitRequest发送请求
    ReplyHeader r = cnxn.submitRequest(h, request, response, wcb);
    if (r.getErr() != 0) {
        if (r.getErr() == KeeperException.Code.NONODE.intValue()) {
            return null;
        }
        throw KeeperException.create(KeeperException.Code.get(r.getErr()), clientPath);
    }
	// 返回stat元数据
    return response.getStat().getCzxid() == -1 ? null : response.getStat();
}
```

### ClientCnxn.submitRequest

这里面的处理逻辑比较简单 

- 调用queuePacket，把请求数据添加到队列 
- 通过packet.wait使得当前线程一直阻塞，直到请求完成

```java
public ReplyHeader submitRequest(
    RequestHeader h,
    Record request,
    Record response,
    WatchRegistration watchRegistration,
    WatchDeregistration watchDeregistration) throws InterruptedException {
    ReplyHeader r = new ReplyHeader();
    // 把请求数据添加到队列
    Packet packet = queuePacket(
        h,
        r,
        request,
        response,
        null,
        null,
        null,
        null,
        watchRegistration,
        watchDeregistration);
    
    synchronized (packet) { // 等到请求执行完成
        if (requestTimeout > 0) { // 有配置超时时间，不无限等待
            // Wait for request completion with timeout
            waitForPacketFinish(r, packet);
        } else {
            // Wait for request completion infinitely
            while (!packet.finished) {
                packet.wait();
            }
        }
    }
    if (r.getErr() == Code.REQUESTTIMEOUT.intValue()) {
        sendThread.cleanAndNotifyState();
    }
    return r;
}
```

### queuePacket

我们仔细分析这段代码，似乎就只是把待发送的数据包添加到outgoingQueue中，没有做其他的操作了？ 那到底发送的流程是在哪里呢？

按照zookeeper整个设计的思想，我们猜想这里又用到了异步线程。所以在这个方法的最后这个代码中可以看到一个sendThread。似乎找到了一些线索

```java
public Packet queuePacket(
    RequestHeader h,
    ReplyHeader r,
    Record request,
    Record response,
    AsyncCallback cb,
    String clientPath,
    String serverPath,
    Object ctx,
    WatchRegistration watchRegistration,
    WatchDeregistration watchDeregistration) {
    Packet packet = null;

    // Note that we do not generate the Xid for the packet yet. It is
    // generated later at send-time, by an implementation of ClientCnxnSocket::doIO(),
    // where the packet is actually sent.
    // 构建一个Packet对象
    packet = new Packet(h, r, request, response, watchRegistration);
    packet.cb = cb;
    packet.ctx = ctx;
    packet.clientPath = clientPath;
    packet.serverPath = serverPath;
    packet.watchDeregistration = watchDeregistration;
    // The synchronized block here is for two purpose:
    // 1. synchronize with the final cleanup() in SendThread.run() to avoid race
    // 2. synchronized against each packet. So if a closeSession packet is added,
    // later packet will be notified.
    
    // 1.与SendThread.run（）中的最终cleanup（）同步以避免竞争
	// 2.针对每个数据包进行同步。 因此，如果添加了closeSession数据包，则将通知以后的数据包
    synchronized (state) {
        if (!state.isAlive() || closing) {
            conLossPacket(packet); // 客户端关闭时，处理
        } else {
            // 如果客户端要求关闭会话，则标记为关闭
            if (h.getType() == OpCode.closeSession) {
                closing = true;
            }
            // 添加到outgoingQueue，这里很显然又是一个生产者消费者模式
            outgoingQueue.add(packet);
        }
    }
    // 唤醒阻塞在selector.select上的线程
    sendThread.getClientCnxnSocket().packetAdded();
    return packet;
}
```

```java
// org.apache.zookeeper.ClientCnxn.SendThread#getClientCnxnSocket
ClientCnxnSocket getClientCnxnSocket() {
    return clientCnxnSocket;
}
// org.apache.zookeeper.ClientCnxnSocketNIO#packetAdded
@Override
void packetAdded() {
    wakeupCnxn();
}

private synchronized void wakeupCnxn() {
    //  NIO中的Selector封装了底层的系统调用，其中wakeup用于唤醒阻塞在select方法上的线程，它的实现很简单，在linux上就是创建一个管道并加入poll的fd集合，wakeup就是往管道里写一个字节，那么阻塞的poll方法有数据可读就立即返回
    selector.wakeup(); 
}
```



### SendThread.run

> org.apache.zookeeper.ClientCnxn.SendThread#run

在Zookeeper这个对象初始化的时候，启动了一个SendThread，这个线程会从outgoingQueue中获取任务，然后发送到服务端处理。

```java
@Override
public void run() {
    clientCnxnSocket.introduce(this, sessionId, outgoingQueue);
    clientCnxnSocket.updateNow();
    clientCnxnSocket.updateLastSendAndHeard();
    int to;
    long lastPingRwServer = Time.currentElapsedTime();
    final int MAX_SEND_PING_INTERVAL = 10000; //10 seconds
    InetSocketAddress serverAddress = null;
    while (state.isAlive()) {  // 如果是存活状态
        try {
            if (!clientCnxnSocket.isConnected()) { // 如果不是连接状态，则需要进行连接的建立
                // don't re-establish connection if we are closing
                if (closing) {
                    break;
                }
                if (rwServerAddress != null) {
                    serverAddress = rwServerAddress;
                    rwServerAddress = null;
                } else {
                    serverAddress = hostProvider.next(1000);
                }
                startConnect(serverAddress);// 开启连接
                clientCnxnSocket.updateLastSendAndHeard(); // 更新最近一次发送和心跳的时间
            }

            if (state.isConnected()) { // 如果连接是正常状态
                // determine whether we need to send an AuthFailed event.
                if (zooKeeperSaslClient != null) { // 是否是ssl连接
                    boolean sendAuthEvent = false;
                    if (zooKeeperSaslClient.getSaslState() == ZooKeeperSaslClient.SaslState.INITIAL) {
                        try {
                            zooKeeperSaslClient.initialize(ClientCnxn.this);
                        } catch (SaslException e) {
                            LOG.error("SASL authentication with Zookeeper Quorum member failed.", e);
                            state = States.AUTH_FAILED;
                            sendAuthEvent = true;
                        }
                    }
                    KeeperState authState = zooKeeperSaslClient.getKeeperState();
                    if (authState != null) {
                        if (authState == KeeperState.AuthFailed) {
                            // An authentication error occurred during authentication with the Zookeeper Server.
                            state = States.AUTH_FAILED;
                            sendAuthEvent = true;
                        } else {
                            if (authState == KeeperState.SaslAuthenticated) {
                                sendAuthEvent = true;
                            }
                        }
                    }

                    if (sendAuthEvent) {
                        eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None, authState, null));
                        if (state == States.AUTH_FAILED) {
                            eventThread.queueEventOfDeath();
                        }
                    }
                }
                to = readTimeout - clientCnxnSocket.getIdleRecv();
            } else {
                to = connectTimeout - clientCnxnSocket.getIdleRecv();
            }

            if (to <= 0) { // 会话是否超时
                String warnInfo = String.format(
                    "Client session timed out, have not heard from server in %dms for session id 0x%s",
                    clientCnxnSocket.getIdleRecv(),
                    Long.toHexString(sessionId));
                LOG.warn(warnInfo);
                throw new SessionTimeoutException(warnInfo);
            }
            if (state.isConnected()) { // 如果连接是正常状态
                //1000(1 second) is to prevent race condition missing to send the second ping
                //also make sure not to send too many pings when readTimeout is small
                int timeToNextPing = readTimeout / 2
                    - clientCnxnSocket.getIdleSend()
                    - ((clientCnxnSocket.getIdleSend() > 1000) ? 1000 : 0);
                // send a ping request either time is due or no packet sent out within MAX_SEND_PING_INTERVAL
                // 发送ping请求
                if (timeToNextPing <= 0 || clientCnxnSocket.getIdleSend() > MAX_SEND_PING_INTERVAL) {
                    sendPing(); // 发送ping请求，还是得存放到 outgoingQueue 队列中
                    clientCnxnSocket.updateLastSend(); // 更新最后发送时间
                } else {
                    if (timeToNextPing < to) {
                        to = timeToNextPing;
                    }
                }
            }

            // If we are in read-only mode, seek for read/write server
            // 是否是只读请求连接状态
            if (state == States.CONNECTEDREADONLY) {
                long now = Time.currentElapsedTime();
                int idlePingRwServer = (int) (now - lastPingRwServer);
                if (idlePingRwServer >= pingRwTimeout) {
                    lastPingRwServer = now;
                    idlePingRwServer = 0;
                    pingRwTimeout = Math.min(2 * pingRwTimeout, maxPingRwTimeout);
                    pingRwServer();
                }
                to = Math.min(to, pingRwTimeout - idlePingRwServer);
            }
			// 这里就是核心的处理逻辑，真正进行网络传输;
			// pendingQueue 表示已经发送出去的数据需要等待server返回的packet队列
			// outgoingQueue 是等待发送出去的packet队列
            clientCnxnSocket.doTransport(to, pendingQueue, ClientCnxn.this);
        } catch (Throwable e) {
            if (closing) {
                // closing so this is expected
                LOG.warn(
                    "An exception was thrown while closing send thread for session 0x{}.",
                    Long.toHexString(getSessionId()),
                    e);
                break;
            } else {
                LOG.warn(
                    "Session 0x{} for sever {}, Closing socket connection. "
                    + "Attempting reconnect except it is a SessionExpiredException.",
                    Long.toHexString(getSessionId()),
                    serverAddress,
                    e);

                // At this point, there might still be new packets appended to outgoingQueue.
                // they will be handled in next connection or cleared up if closed.
                cleanAndNotifyState();
            }
        }
    }

    synchronized (state) {
        // When it comes to this point, it guarantees that later queued
        // packet to outgoingQueue will be notified of death.
        cleanup();
    }
    clientCnxnSocket.close();
    if (state.isAlive()) {
        eventThread.queueEvent(new WatchedEvent(Event.EventType.None, Event.KeeperState.Disconnected, null));
    }
    eventThread.queueEvent(new WatchedEvent(Event.EventType.None, Event.KeeperState.Closed, null));
    ZooTrace.logTraceMessage(
        LOG,
        ZooTrace.getTextTraceLevel(),
        "SendThread exited loop for session: 0x" + Long.toHexString(getSessionId()));
}
```

### ClientCnxnSocketNIO.doTransport

> org.apache.zookeeper.ClientCnxnSocketNIO#doTransport

调用协议层进行数据传输。

```java
@Override
void doTransport(
    int waitTimeOut,
    Queue<Packet> pendingQueue, // 表示已经发送出去的数据需要等待server返回的packet队列
    ClientCnxn cnxn) throws IOException, InterruptedException {
    
    selector.select(waitTimeOut); // 这里会阻塞，当有数据来时，就会唤醒，一种通过 selector.wakeup() 唤醒，一种是通过接受事件唤醒
    Set<SelectionKey> selected;
    synchronized (this) {
        selected = selector.selectedKeys(); // 获取通道key
    }
    // Everything below and until we get back to the select is
    // non blocking, so time is effectively a constant. That is
    // Why we just have to do this once, here
    updateNow();
    for (SelectionKey k : selected) {
        SocketChannel sc = ((SocketChannel) k.channel());
        if ((k.readyOps() & SelectionKey.OP_CONNECT) != 0) { // 打开连接事件
            if (sc.finishConnect()) { // 判断连接是否完成
                updateLastSendAndHeard(); // 更新最后发送时间
                updateSocketAddresses(); // 更新socket 本地地址和远程地址
                sendThread.primeConnection(); // 会话、监听、身份验证处理
            }
        } else if ((k.readyOps() & (SelectionKey.OP_READ | SelectionKey.OP_WRITE)) != 0) {
            doIO(pendingQueue, cnxn); // 如果是读写事件，则调用doIO进行传输
        }
    }
    if (sendThread.getZkState().isConnected()) {
        if (findSendablePacket(outgoingQueue, sendThread.tunnelAuthInProgress()) != null) {
            enableWrite();
        }
    }
    selected.clear();
}
```

### ClientCnxnSocketNIO.doIO

> org.apache.zookeeper.ClientCnxnSocketNIO#doIO

```java
void doIO(Queue<Packet> pendingQueue, ClientCnxn cnxn) throws InterruptedException, IOException {
    SocketChannel sock = (SocketChannel) sockKey.channel();
    if (sock == null) {
        throw new IOException("Socket is null!");
    }
    if (sockKey.isReadable()) { // 如果是都请求
        int rc = sock.read(incomingBuffer); // 读取数据
        if (rc < 0) {
            throw new EndOfStreamException("Unable to read additional data from server sessionid 0x"
                                           + Long.toHexString(sessionId)
                                           + ", likely server has closed socket");
        }
        if (!incomingBuffer.hasRemaining()) {
            incomingBuffer.flip(); // 反转
            if (incomingBuffer == lenBuffer) {
                recvCount.getAndIncrement();
                readLength();
            } else if (!initialized) {
                readConnectResult();
                enableRead();
                if (findSendablePacket(outgoingQueue, sendThread.tunnelAuthInProgress()) != null) {
                    // Since SASL authentication has completed (if client is configured to do so),
                    // outgoing packets waiting in the outgoingQueue can now be sent.
                    enableWrite();
                }
                lenBuffer.clear();
                incomingBuffer = lenBuffer;
                updateLastHeard();
                initialized = true;
            } else {
                sendThread.readResponse(incomingBuffer);
                lenBuffer.clear();
                incomingBuffer = lenBuffer;
                updateLastHeard();
            }
        }
    }
    // 如果是写请求
    if (sockKey.isWritable()) {
        // 找到可以发送的packet
        Packet p = findSendablePacket(outgoingQueue, sendThread.tunnelAuthInProgress());

        if (p != null) {
            updateLastSend(); // 更新最近发送时间
            // 如果Packet的byteBuffer没有创建，那么就创建
            if (p.bb == null) {
                if ((p.requestHeader != null)
                    && (p.requestHeader.getType() != OpCode.ping)
                    && (p.requestHeader.getType() != OpCode.auth)) {
                    p.requestHeader.setXid(cnxn.getXid());
                }
                p.createBB(); // 创建 byteBuffer
            }
            sock.write(p.bb); // 写出去
            if (!p.bb.hasRemaining()) {
                sentCount.getAndIncrement();
                outgoingQueue.removeFirstOccurrence(p); // 从待发送队列中移除
                if (p.requestHeader != null // 判断数据包的请求，ping以及auth不加入待回复队列
                    && p.requestHeader.getType() != OpCode.ping
                    && p.requestHeader.getType() != OpCode.auth) {
                    synchronized (pendingQueue) { // 添加到pendingQueue待回复队列
                        pendingQueue.add(p);
                    }
                }
            }
        }
        if (outgoingQueue.isEmpty()) {
            // No more packets to send: turn off write interest flag.
            // Will be turned on later by a later call to enableWrite(),
            // from within ZooKeeperSaslClient (if client is configured
            // to attempt SASL authentication), or in either doIO() or
            // in doTransport() if not.
            disableWrite();
        } else if (!initialized && p != null && !p.bb.hasRemaining()) {
            // On initial connection, write the complete connect request
            // packet, but then disable further writes until after
            // receiving a successful connection response.  If the
            // session is expired, then the server sends the expiration
            // response and immediately closes its end of the socket.  If
            // the client is simultaneously writing on its end, then the
            // TCP stack may choose to abort with RST, in which case the
            // client would never receive the session expired event.  See
            // http://docs.oracle.com/javase/6/docs/technotes/guides/net/articles/connection_release.html
            disableWrite();
        } else {
            // Just in case
            enableWrite();
        }
    }
}
```

**findSendablePacket**

```java
private Packet findSendablePacket(LinkedBlockingDeque<Packet> outgoingQueue, boolean tunneledAuthInProgres) {
    if (outgoingQueue.isEmpty()) {
        return null;
    }
    // If we've already starting sending the first packet, we better finish
    if (outgoingQueue.getFirst().bb != null || !tunneledAuthInProgres) {
        return outgoingQueue.getFirst(); // 获取第一个数据包
    }
    // Since client's authentication with server is in progress,
    // send only the null-header packet queued by primeConnection().
    // This packet must be sent so that the SASL authentication process
    // can proceed, but all other packets should wait until
    // SASL authentication completes.
    Iterator<Packet> iter = outgoingQueue.iterator();
    while (iter.hasNext()) {
        Packet p = iter.next();
        if (p.requestHeader == null) {
            // 我们找到包装了。将其移动到队列的开头
            iter.remove();
            outgoingQueue.addFirst(p);
            return p;
        } else {
            // Non-priming packet: defer it until later, leaving it in the queue
            // until authentication completes.
            LOG.debug("Deferring non-priming packet {} until SASL authentication completes.", p);
        }
    }
    return null;
}
```

### 总结

到目前为止，我们已经分析了客户端请求的发送流程，我们来画一个简单的流程图梳理一下

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201203172345.jpg)

## 服务端接收数据请求

服务端收到的数据包应该在哪里呢？zookeeper启动的时候，通过下面的代码构建了一个

ServerCnxnFactory cnxnFactory = ServerCnxnFactory.createFactory();

并且，在 `org.apache.zookeeper.server.quorum.QuorumPeerMain#runFromConfig -> QuorumPeer.start() -> startServerCnxnFactory() -> cnxnFactory.start();` 中，启动了一个 acceptThread线程，这个线程从名字上看，应该是用来处理客户端的来请求

> org.apache.zookeeper.server.NIOServerCnxnFactory#start

```java
@Override
public void start() {
    stopped = false;
    if (workerPool == null) {
        workerPool = new WorkerService("NIOWorker", numWorkerThreads, false);
    }
    for (SelectorThread thread : selectorThreads) {
        if (thread.getState() == Thread.State.NEW) {
            thread.start();
        }
    }
    // ensure thread is started once and only once
    if (acceptThread.getState() == Thread.State.NEW) {
        acceptThread.start(); // 这里进入接收请求
    }
    if (expirerThread.getState() == Thread.State.NEW) {
        expirerThread.start();
    }
}
```

### AcceptThread.run

在run方法中，调用了select()方法

```java
public void run() {
    try {
        while (!stopped && !acceptSocket.socket().isClosed()) {
            try {
                select(); // 多路复用器
            } catch (RuntimeException e) {
                LOG.warn("Ignoring unexpected runtime exception", e);
            } catch (Exception e) {
                LOG.warn("Ignoring unexpected exception", e);
            }
        }
    } finally {
        closeSelector();
        // This will wake up the selector threads, and tell the
        // worker thread pool to begin shutdown.
        if (!reconfiguring) {
            NIOServerCnxnFactory.this.stop();
        }
        LOG.info("accept thread exitted run method");
    }
}
```

select方法中，会通过复路器Selector，去进行select操作，获取就绪的连接。其中select这个方法中主 要做的事情是

- 遍历所有的就绪连接，进行连接的判断 
- 调用doAccept方法进行处理

```java
private void select() {
    try {
        selector.select(); // 阻塞，等到有事件触发时，才会唤醒

        Iterator<SelectionKey> selectedKeys = selector.selectedKeys().iterator();
        while (!stopped && selectedKeys.hasNext()) {
            SelectionKey key = selectedKeys.next();
            selectedKeys.remove();

            if (!key.isValid()) {
                continue;
            }
            if (key.isAcceptable()) { // 是否 连接就绪
                if (!doAccept()) { // 这里用来处理客户端连接事件
                    // If unable to pull a new connection off the accept
                    // queue, pause accepting to give us time to free
                    // up file descriptors and so the accept thread
                    // doesn't spin in a tight loop.
                    pauseAccept(10);
                }
            } else {
                LOG.warn("Unexpected ops in accept select {}", key.readyOps());
            }
        }
    } catch (IOException e) {
        LOG.warn("Ignoring IOException while selecting", e);
    }
}
```

### doAccept

真正读取客户端的请求

```java
private boolean doAccept() { // 这里用来处理客户端连接事件
    boolean accepted = false;
    SocketChannel sc = null;
    try {
        sc = acceptSocket.accept(); // 获得客户端连接
        accepted = true;
        if (limitTotalNumberOfCnxns()) {
            throw new IOException("Too many connections max allowed is " + maxCnxns);
        }
        InetAddress ia = sc.socket().getInetAddress();
        int cnxncount = getClientCnxnCount(ia);
		// 是否超过最大连接
        if (maxClientCnxns > 0 && cnxncount >= maxClientCnxns) {
            throw new IOException("Too many connections from " + ia + " - max is " + maxClientCnxns);
        }

        LOG.debug("Accepted socket connection from {}", sc.socket().getRemoteSocketAddress());

        sc.configureBlocking(false);// 设置非阻塞

        // Round-robin assign this connection to a selector thread
        // 轮询，将当前连接分配给选择器线程
        if (!selectorIterator.hasNext()) {
            selectorIterator = selectorThreads.iterator();
        }
        SelectorThread selectorThread = selectorIterator.next();
        if (!selectorThread.addAcceptedConnection(sc)) {// 把当前连接再丢给SelectorThread来处理。
            throw new IOException("Unable to add connection to selector queue"
                                  + (stopped ? " (shutdown in progress)" : ""));
        }
        acceptErrorLogger.flush();
    } catch (IOException e) {
        // accept, maxClientCnxns, configureBlocking
        ServerMetrics.getMetrics().CONNECTION_REJECTED.add(1);
        acceptErrorLogger.rateLimitLog("Error accepting new connection: " + e.getMessage());
        fastCloseSock(sc);
    }
    return accepted;
}
```

把当前客户端连接丢到acceptedQueue这个阻塞队列中。

```java
public boolean addAcceptedConnection(SocketChannel accepted) {
    if (stopped || !acceptedQueue.offer(accepted)) { // 添加到接收队列,后续会为该连接注册读写事件
        return false;
    }
    wakeupSelector(); // 唤醒阻塞在selector.select上的线程
    return true;
}
```

### SelectorThread.run

> org.apache.zookeeper.server.NIOServerCnxnFactory.SelectorThread#run

由于在doAccept方法中，已经把客户端的连接交给了SelectorThread，所以我们去这个线程的run方法 中看看处理逻辑

```java
public void run() { // 这里用来处理读写请求事件
    try {
        while (!stopped) {
            try {
                select(); // 处理多路复用
                processAcceptedConnections();  //处理连接请求
                processInterestOpsUpdateRequests();  //注册一个更新请求
            } catch (RuntimeException e) {
                LOG.warn("Ignoring unexpected runtime exception", e);
            } catch (Exception e) {
                LOG.warn("Ignoring unexpected exception", e);
            }
        }

        // Close connections still pending on the selector. Any others
        // with in-flight work, let drain out of the work queue.
        for (SelectionKey key : selector.keys()) {
            NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
            if (cnxn.isSelectable()) {
                cnxn.close(ServerCnxn.DisconnectReason.SERVER_SHUTDOWN);
            }
            cleanupSelectionKey(key);
        }
        SocketChannel accepted;
        while ((accepted = acceptedQueue.poll()) != null) {
            fastCloseSock(accepted);
        }
        updateQueue.clear();
    } finally {
        closeSelector();
        // This will wake up the accept thread and the other selector
        // threads, and tell the worker thread pool to begin shutdown.
        NIOServerCnxnFactory.this.stop();
        LOG.info("selector thread exitted run method");
    }
}
```

从selector中获取就绪的连接，针对读写事件，调用handleIO方法进行处理。

```java
private void select() {
    try {
        selector.select(); // 阻塞，接受到事件时触发

        Set<SelectionKey> selected = selector.selectedKeys(); // 获取通道key
        ArrayList<SelectionKey> selectedList = new ArrayList<SelectionKey>(selected);
        Collections.shuffle(selectedList); // 随机打乱原来的顺序
        Iterator<SelectionKey> selectedKeys = selectedList.iterator();
        while (!stopped && selectedKeys.hasNext()) {
            SelectionKey key = selectedKeys.next();
            selected.remove(key);

            if (!key.isValid()) {
                cleanupSelectionKey(key);
                continue;
            }
            if (key.isReadable() || key.isWritable()) { // 是读写请求
                handleIO(key); // 处理请求
            } else {
                LOG.warn("Unexpected ops in select {}", key.readyOps());
            }
        }
    } catch (IOException e) {
        LOG.warn("Ignoring IOException while selecting", e);
    }
}
```

### handleIO

- 构建一个IOWorkRequest 
- 把这个请求丢给workerPool来处理

```java
private void handleIO(SelectionKey key) {
    IOWorkRequest workRequest = new IOWorkRequest(this, key);
    NIOServerCnxn cnxn = (NIOServerCnxn) key.attachment();
	
    // Stop selecting this key while processing on its connection
    cnxn.disableSelectable();
    key.interestOps(0); // 代表取消上面的四个监听（SelectionKey.OP_READ、OP_CONNECT...），代表不监听任何东西
    touchCnxn(cnxn);
    workerPool.schedule(workRequest); // 放到线程池处理
}
```

### ZookeeperServer.processPacket

通过N个异步化处理过程，最终进入到 `ZookeeperServer.processPacket`

调用链路： `WorkerService.schedule -> ScheduledWorkRequest.run -> IOWorkRequest.doWork -> NIOServerCnxn.doIO -> readPayload -> readRequest -> ZooKeeperServer.processPacket`

**这个方法根据数据包的类型来处理不同的数据包，对于读写请求，我们主要关注下面这块代码即可**

```java
public void processPacket(ServerCnxn cnxn, ByteBuffer incomingBuffer) throws IOException {
    // We have the request, now process and setup for next
    InputStream bais = new ByteBufferInputStream(incomingBuffer);
    BinaryInputArchive bia = BinaryInputArchive.getArchive(bais);
    RequestHeader h = new RequestHeader();
    h.deserialize(bia, "header"); // 反序列化

    // Need to increase the outstanding request count first, otherwise
    // there might be a race condition that it enabled recv after
    // processing request and then disabled when check throttling.
    //
    // Be aware that we're actually checking the global outstanding
    // request before this request.
    //
    // It's fine if the IOException thrown before we decrease the count
    // in cnxn, since it will close the cnxn anyway.
    cnxn.incrOutstandingAndCheckThrottle(h);

    // Through the magic of byte buffers, txn will not be
    // pointing
    // to the start of the txn
    incomingBuffer = incomingBuffer.slice();
    if (h.getType() == OpCode.auth) { // 授权请求
        LOG.info("got auth packet {}", cnxn.getRemoteSocketAddress());
        AuthPacket authPacket = new AuthPacket();
        ByteBufferInputStream.byteBuffer2Record(incomingBuffer, authPacket);
        String scheme = authPacket.getScheme();
        ServerAuthenticationProvider ap = ProviderRegistry.getServerProvider(scheme);
        Code authReturn = KeeperException.Code.AUTHFAILED;
        if (ap != null) {
            try {
                // handleAuthentication may close the connection, to allow the client to choose
                // a different server to connect to.
                authReturn = ap.handleAuthentication(
                    new ServerAuthenticationProvider.ServerObjs(this, cnxn),
                    authPacket.getAuth());
            } catch (RuntimeException e) {
                LOG.warn("Caught runtime exception from AuthenticationProvider: {}", scheme, e);
                authReturn = KeeperException.Code.AUTHFAILED;
            }
        }
        if (authReturn == KeeperException.Code.OK) {
            LOG.debug("Authentication succeeded for scheme: {}", scheme);
            LOG.info("auth success {}", cnxn.getRemoteSocketAddress());
            ReplyHeader rh = new ReplyHeader(h.getXid(), 0, KeeperException.Code.OK.intValue());
            cnxn.sendResponse(rh, null, null);
        } else {
            if (ap == null) {
                LOG.warn(
                    "No authentication provider for scheme: {} has {}",
                    scheme,
                    ProviderRegistry.listProviders());
            } else {
                LOG.warn("Authentication failed for scheme: {}", scheme);
            }
            // send a response...
            ReplyHeader rh = new ReplyHeader(h.getXid(), 0, KeeperException.Code.AUTHFAILED.intValue());
            cnxn.sendResponse(rh, null, null);
            // ... and close connection
            cnxn.sendBuffer(ServerCnxnFactory.closeConn);
            cnxn.disableRecv();
        }
        return;
    } else if (h.getType() == OpCode.sasl) { // 安全请求
        processSasl(incomingBuffer, cnxn, h);
    } else { // 业务请求
        if (shouldRequireClientSaslAuth() && !hasCnxSASLAuthenticated(cnxn)) {
            ReplyHeader replyHeader = new ReplyHeader(h.getXid(), 0, Code.SESSIONCLOSEDREQUIRESASLAUTH.intValue());
            cnxn.sendResponse(replyHeader, null, "response");
            cnxn.sendCloseSession();
            cnxn.disableRecv();
        } else {
            Request si = new Request(cnxn, cnxn.getSessionId(), h.getXid(), h.getType(), incomingBuffer, cnxn.getAuthInfo());
            int length = incomingBuffer.limit();
            if (isLargeRequest(length)) {
                // checkRequestSize will throw IOException if request is rejected
                checkRequestSizeWhenMessageReceived(length);
                si.setLargeRequestSize(length);
            }
            si.setOwner(ServerCnxn.me);
            submitRequest(si); // 提交请求
        }
    }
}
```

### ZookeeperServer.submitRequest

将请求添加到 RequestThrottler（限流器）中去处理，它是一个线程，而 sumitReuqest 方法实际就是把任务添加到阻塞队列。

```java
public void submitRequest(Request si) {
    enqueueRequest(si);
}

public void enqueueRequest(Request si) {
    if (requestThrottler == null) {
        synchronized (this) {
            try {
                // Since all requests are passed to the request
                // processor it should wait for setting up the request
                // processor chain. The state will be updated to RUNNING
                // after the setup.
                while (state == State.INITIAL) {
                    wait(1000);
                }
            } catch (InterruptedException e) {
                LOG.warn("Unexpected interruption", e);
            }
            if (requestThrottler == null) {
                throw new RuntimeException("Not started");
            }
        }
    }
    requestThrottler.submitRequest(si);
}
```

```java
public void submitRequest(Request request) {
    if (stopping) {
        LOG.debug("Shutdown in progress. Request cannot be processed");
        dropRequest(request);
    } else { // 服务未停止时
        submittedRequests.add(request); // 添加到队列中
    }
}
```



### RequestThrottler.run

在RequestThrottler的run 方法中，会从阻塞队列中取出任务进行处理。

```java
public static final Request requestOfDeath = new Request(null, 0, 0, 0, null, null);


@Override
public void run() {
    try {
        while (true) {
            if (killed) {
                break;
            }
			// 从阻塞队列中获取任务
            Request request = submittedRequests.take();
            if (Request.requestOfDeath == request) { // 如果是空，跳过
                break;
            }

            if (request.mustDrop()) {
                continue;
            }

            // 当maxRequests = 0时，将禁用节流
            if (maxRequests > 0) {
                while (!killed) {
                    if (dropStaleRequests && request.isStale()) {
                        // Note: this will close the connection
                        // 注意:这将关闭连接
                        dropRequest(request);
                        ServerMetrics.getMetrics().STALE_REQUESTS_DROPPED.add(1);
                        request = null;
                        break;
                    }
                    // 只要没达到最大限制，直接通过(跳出while循环)
                    if (zks.getInProcess() < maxRequests) {
                        break;
                    }
                    // 否则会等待一段时间继续再处理 this.wait(stallTime)
                    throttleSleep(stallTime);
                }
            }

            if (killed) {
                break;
            }

            // 已丢弃的过时请求将为空
            // 如果请求不为空，则处理请求
            if (request != null) {
                if (request.isStale()) {
                    ServerMetrics.getMetrics().STALE_REQUESTS.add(1);
                }
                // 验证通过后，提交给 zkServer 处理
                zks.submitRequestNow(request);
            }
        }
    } catch (InterruptedException e) {
        LOG.error("Unexpected interruption", e);
    }
    int dropped = drainQueue();
    LOG.info("RequestThrottler shutdown. Dropped {} requests", dropped);
}
```

### ZookeeperServer.submitRequestNow

提交请求，这里面涉及到一个firstProcessor，这个是一个责任链模式。

`firstProcessor` 的初始化是在 `ZookeeperServer` 的 `setupRequestProcessor` 中完成的，代码如下

```java
public void submitRequestNow(Request si) {
    if (firstProcessor == null) {
        // 如果firstProcessor，可能是服务端还没有完成初始化，则等待
        synchronized (this) {
            try {
                while (state == State.INITIAL) {
                    wait(1000);
                }
            } catch (InterruptedException e) {
                LOG.warn("Unexpected interruption", e);
            }
            if (firstProcessor == null || state != State.RUNNING) {
                throw new RuntimeException("Not started");
            }
        }
    }
    
    
    try {
        touch(si.cnxn);
        boolean validpacket = Request.isValid(si.type);
        if (validpacket) { // 验证请求的类型是否合法
            setLocalSessionFlag(si);
            // 特殊请求处理(异步请求链路)
            firstProcessor.processRequest(si);
            if (si.cnxn != null) {
                incInProcess();
            }
        } else {
            LOG.warn("Received packet at server of unknown type {}", si.type);
            // Update request accounting/throttling limits
            requestFinished(si);
            new UnimplementedRequestProcessor().processRequest(si);
        }
    } catch (MissingSessionException e) {
        LOG.debug("Dropping request.", e);
        // Update request accounting/throttling limits
        requestFinished(si);
    } catch (RequestProcessorException e) {
        LOG.error("Unable to process request", e);
        // Update request accounting/throttling limits
        requestFinished(si);
    }
}
```

#### firstProcessor请求链组成

> org.apache.zookeeper.server.ZooKeeperServer#setupRequestProcessors

```java
protected void setupRequestProcessors() {
    RequestProcessor finalProcessor = new FinalRequestProcessor(this);
    RequestProcessor syncProcessor = new SyncRequestProcessor(this, finalProcessor);
    ((SyncRequestProcessor) syncProcessor).start();
    firstProcessor = new PrepRequestProcessor(this, syncProcessor);
    ((PrepRequestProcessor) firstProcessor).start();
}
```

从上面我们可以看到firstProcessor的实例是一个PrepRequestProcessor，而这个构造方法中又传递了 一个Processor构成了一个调用链。

`RequestProcessor syncProcessor = new SyncRequestProcessor(this, finalProcessor);` 而 syncProcessor 的构造方法传递的又是一个Processor，对应的是FinalRequestProcessor

> PrepRequestProcessor（SyncRequestProcessor（FinalRequestProcessor））

### ZookeeperServer.submitRequestNow

```java
public void submitRequestNow(Request si) {
    if (firstProcessor == null) {
        // 如果firstProcessor，可能是服务端还没有完成初始化，则等待
        synchronized (this) {
            try {
                // Since all requests are passed to the request
                // processor it should wait for setting up the request
                // processor chain. The state will be updated to RUNNING
                // after the setup.
                while (state == State.INITIAL) {
                    wait(1000);
                }
            } catch (InterruptedException e) {
                LOG.warn("Unexpected interruption", e);
            }
            if (firstProcessor == null || state != State.RUNNING) {
                throw new RuntimeException("Not started");
            }
        }
    }
    try {
        touch(si.cnxn);
        boolean validpacket = Request.isValid(si.type);
        if (validpacket) { // 验证请求的类型是否合法
            setLocalSessionFlag(si);
            // 特殊 请求处理
            firstProcessor.processRequest(si);
            if (si.cnxn != null) {
                incInProcess();
            }
        } else {
            LOG.warn("Received packet at server of unknown type {}", si.type);
            // Update request accounting/throttling limits
            requestFinished(si);
            new UnimplementedRequestProcessor().processRequest(si);
        }
    } catch (MissingSessionException e) {
        LOG.debug("Dropping request.", e);
        // Update request accounting/throttling limits
        requestFinished(si);
    } catch (RequestProcessorException e) {
        LOG.error("Unable to process request", e);
        // Update request accounting/throttling limits
        requestFinished(si);
    }
}
```

### PredRequestProcessor.processRequest

通过上面了解到调用链关系以后，我们继续再看firstProcessor.processRequest(si)； 会调用到 PrepRequestProcessor.

在这个处理器中，又把请求对象提交到了阻塞队列中。

```java
public void processRequest(Request request) {
    request.prepQueueStartTime = Time.currentElapsedTime();
    submittedRequests.add(request); // 添加一个阻塞队列
    ServerMetrics.getMetrics().PREP_PROCESSOR_QUEUED.add(1);
}
```

PredRequestProcessor是一个线程，在构建处理器链的时候，就已经启动了这个线程，所以直接进入到run方法中。

PreRequestProcessor一般是放在处理链的起始部分的，它对请求做一些预处理

1. 检查Session
2. 检查要操作的节点及其父节点是否存在
3. 检查客户端是否有权限

> org.apache.zookeeper.server.PrepRequestProcessor#run

```java
@Override
public void run() {
    try {
        while (true) {
            ServerMetrics.getMetrics().PREP_PROCESSOR_QUEUE_SIZE.add(submittedRequests.size());
            Request request = submittedRequests.take();  // 从阻塞队列中获取请求
            ServerMetrics.getMetrics().PREP_PROCESSOR_QUEUE_TIME
                .add(Time.currentElapsedTime() - request.prepQueueStartTime);
            long traceMask = ZooTrace.CLIENT_REQUEST_TRACE_MASK;
            if (request.type == OpCode.ping) {  // 判断请求类型
                traceMask = ZooTrace.CLIENT_PING_TRACE_MASK;
            }
            if (LOG.isTraceEnabled()) {
                ZooTrace.logRequest(LOG, traceMask, 'P', request, "");
            }
            if (Request.requestOfDeath == request) {
                break;
            }

            request.prepStartTime = Time.currentElapsedTime();
            pRequest(request); // 调用pRequest方法
        }
    } catch (Exception e) {
        handleException(this.getName(), e);
    }
    LOG.info("PrepRequestProcessor exited loop!");
}
```

pRequest方法比较长，主要逻辑就是根据不同的请求类型实现不同的操作。

而对于 exists 请求，没有特别的逻辑处理。

```java
protected void pRequest(Request request) throws RequestProcessorException {
    // LOG.info("Prep>>> cxid = " + request.cxid + " type = " +
    // request.type + " id = 0x" + Long.toHexString(request.sessionId));
    request.setHdr(null);
    request.setTxn(null);

    try {
        switch (request.type) {
            case OpCode.createContainer:
            case OpCode.create:
            case OpCode.create2:
                CreateRequest create2Request = new CreateRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, create2Request, true);
                break;
            case OpCode.createTTL:
                CreateTTLRequest createTtlRequest = new CreateTTLRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, createTtlRequest, true);
                break;
            case OpCode.deleteContainer:
            case OpCode.delete:
                DeleteRequest deleteRequest = new DeleteRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, deleteRequest, true);
                break;
            case OpCode.setData:
                SetDataRequest setDataRequest = new SetDataRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, setDataRequest, true);
                break;
            case OpCode.reconfig:
                ReconfigRequest reconfigRequest = new ReconfigRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, reconfigRequest);
                pRequest2Txn(request.type, zks.getNextZxid(), request, reconfigRequest, true);
                break;
            case OpCode.setACL:
                SetACLRequest setAclRequest = new SetACLRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, setAclRequest, true);
                break;
            case OpCode.check:
                CheckVersionRequest checkRequest = new CheckVersionRequest();
                pRequest2Txn(request.type, zks.getNextZxid(), request, checkRequest, true);
                break;
            case OpCode.multi:
                MultiOperationRecord multiRequest = new MultiOperationRecord();
                try {
                    ByteBufferInputStream.byteBuffer2Record(request.request, multiRequest);
                } catch (IOException e) {
                    request.setHdr(new TxnHeader(request.sessionId, request.cxid, zks.getNextZxid(), Time.currentWallTime(), OpCode.multi));
                    throw e;
                }
                List<Txn> txns = new ArrayList<Txn>();
                //Each op in a multi-op must have the same zxid!
                long zxid = zks.getNextZxid();
                KeeperException ke = null;

                //Store off current pending change records in case we need to rollback
                Map<String, ChangeRecord> pendingChanges = getPendingChanges(multiRequest);
                request.setHdr(new TxnHeader(request.sessionId, request.cxid, zxid,
                                             Time.currentWallTime(), request.type));

                for (Op op : multiRequest) {
                    Record subrequest = op.toRequestRecord();
                    int type;
                    Record txn;

                    /* If we've already failed one of the ops, don't bother
                     * trying the rest as we know it's going to fail and it
                     * would be confusing in the logfiles.
                     */
                    if (ke != null) {
                        type = OpCode.error;
                        txn = new ErrorTxn(Code.RUNTIMEINCONSISTENCY.intValue());
                    } else {
                        /* Prep the request and convert to a Txn */
                        try {
                            pRequest2Txn(op.getType(), zxid, request, subrequest, false);
                            type = op.getType();
                            txn = request.getTxn();
                        } catch (KeeperException e) {
                            ke = e;
                            type = OpCode.error;
                            txn = new ErrorTxn(e.code().intValue());

                            if (e.code().intValue() > Code.APIERROR.intValue()) {
                                LOG.info("Got user-level KeeperException when processing {} aborting"
                                         + " remaining multi ops. Error Path:{} Error:{}",
                                         request.toString(),
                                         e.getPath(),
                                         e.getMessage());
                            }

                            request.setException(e);

                            /* Rollback change records from failed multi-op */
                            rollbackPendingChanges(zxid, pendingChanges);
                        }
                    }

                    // TODO: I don't want to have to serialize it here and then
                    //       immediately deserialize in next processor. But I'm
                    //       not sure how else to get the txn stored into our list.
                    try (ByteArrayOutputStream baos = new ByteArrayOutputStream()) {
                        BinaryOutputArchive boa = BinaryOutputArchive.getArchive(baos);
                        txn.serialize(boa, "request");
                        ByteBuffer bb = ByteBuffer.wrap(baos.toByteArray());
                        txns.add(new Txn(type, bb.array()));
                    }
                }

                request.setTxn(new MultiTxn(txns));
                if (digestEnabled) {
                    setTxnDigest(request);
                }

                break;

                //create/close session don't require request record
            case OpCode.createSession:
            case OpCode.closeSession:
                if (!request.isLocalSession()) {
                    pRequest2Txn(request.type, zks.getNextZxid(), request, null, true);
                }
                break;

                //All the rest don't need to create a Txn - just verify session
            case OpCode.sync:
            case OpCode.exists: // 判断是否存在节点
            case OpCode.getData:
            case OpCode.getACL:
            case OpCode.getChildren:
            case OpCode.getAllChildrenNumber:
            case OpCode.getChildren2:
            case OpCode.ping:
            case OpCode.setWatches:
            case OpCode.setWatches2:
            case OpCode.checkWatches:
            case OpCode.removeWatches:
            case OpCode.getEphemerals:
            case OpCode.multiRead:
            case OpCode.addWatch:
                zks.sessionTracker.checkSession(request.sessionId, request.getOwner());
                break;
            default:
                LOG.warn("unknown type {}", request.type);
                break;
        }
    } catch (KeeperException e) {
        if (request.getHdr() != null) {
            request.getHdr().setType(OpCode.error);
            request.setTxn(new ErrorTxn(e.code().intValue()));
        }

        if (e.code().intValue() > Code.APIERROR.intValue()) {
            LOG.info(
                "Got user-level KeeperException when processing {} Error Path:{} Error:{}",
                request.toString(),
                e.getPath(),
                e.getMessage());
        }
        request.setException(e);
    } catch (Exception e) {
        // log at error level as we are returning a marshalling
        // error to the user
        LOG.error("Failed to process {}", request, e);

        StringBuilder sb = new StringBuilder();
        ByteBuffer bb = request.request;
        if (bb != null) {
            bb.rewind();
            while (bb.hasRemaining()) {
                sb.append(Integer.toHexString(bb.get() & 0xff));
            }
        } else {
            sb.append("request buffer is null");
        }

        LOG.error("Dumping request buffer: 0x{}", sb.toString());
        if (request.getHdr() != null) {
            request.getHdr().setType(OpCode.error);
            request.setTxn(new ErrorTxn(Code.MARSHALLINGERROR.intValue()));
        }
    }
    request.zxid = zks.getZxid();
    ServerMetrics.getMetrics().PREP_PROCESS_TIME.add(Time.currentElapsedTime() - request.prepStartTime);
    nextProcessor.processRequest(request);
}
```

### SyncRequestProcessor.processRequest

这个processor负责把写request持久化到本地磁盘，为了提高写磁盘的效率，这里使用的是缓冲写， 但是会周期性（1000个request）的调用flush操作，flush之后request已经确保写到磁盘了

同时他还要维护本机的txnlog和snapshot，这里的基本逻辑是：

**每隔 snapCount / 2 个request会重新生成一个snapshot并滚动一次txnlog，同时为了避免所有的 zookeeper server在同一个时间生成snapshot和滚动日志，这里会再加上一个随机数，snapCount 的默认值是10w个request**

> org.apache.zookeeper.server.SyncRequestProcessor#processRequest

```java
public void processRequest(final Request request) {
    Objects.requireNonNull(request, "Request cannot be null");

    request.syncQueueStartTime = Time.currentElapsedTime();
    queuedRequests.add(request);
    ServerMetrics.getMetrics().SYNC_PROCESSOR_QUEUED.add(1);
}
```

```java
// 判断是否需要快照
private boolean shouldSnapshot() { 
    int logCount = zks.getZKDatabase().getTxnCount();
    long logSize = zks.getZKDatabase().getTxnSize();
    // snapCount 默认是100000;  randRoll 是一个随机数
    return (logCount > (snapCount / 2 + randRoll))
        || (snapSizeInBytes > 0 && logSize > (snapSizeInBytes / 2 + randSize));
}

@Override
public void run() {
    try {
        // we do this in an attempt to ensure that not all of the servers
        // in the ensemble take a snapshot at the same time
        resetSnapshotStats();
        lastFlushTime = Time.currentElapsedTime();
        while (true) {
            ServerMetrics.getMetrics().SYNC_PROCESSOR_QUEUE_SIZE.add(queuedRequests.size());

            long pollTime = Math.min(zks.getMaxWriteQueuePollTime(), getRemainingDelay());
            Request si = queuedRequests.poll(pollTime, TimeUnit.MILLISECONDS);
            if (si == null) {
                /* We timed out looking for more writes to batch, go ahead and flush immediately */
                flush();
                si = queuedRequests.take();
            }

            if (si == REQUEST_OF_DEATH) {
                break;
            }

            long startProcessTime = Time.currentElapsedTime();
            ServerMetrics.getMetrics().SYNC_PROCESSOR_QUEUE_TIME.add(startProcessTime - si.syncQueueStartTime);

            // track the number of records written to the log
            // 将请求写入到事务日志中，并跟踪写入日志的记录数量
            if (zks.getZKDatabase().append(si)) {
                if (shouldSnapshot()) {  // 判断是否要生成快照
                    resetSnapshotStats();
                    // roll the log
                    zks.getZKDatabase().rollLog(); // 滚动日志
                    // take a snapshot
                    if (!snapThreadMutex.tryAcquire()) {
                        LOG.warn("Too busy to snap, skipping");
                    } else {
                        new ZooKeeperThread("Snapshot Thread") {
                            public void run() {
                                try {
                                    zks.takeSnapshot(); // 生成快照
                                } catch (Exception e) {
                                    LOG.warn("Unexpected exception", e);
                                } finally {
                                    snapThreadMutex.release();
                                }
                            }
                        }.start();
                    }
                }
            } else if (toFlush.isEmpty()) {
                // optimization for read heavy workloads
                // iff this is a read, and there are no pending
                // flushes (writes), then just pass this to the next
                // processor
                if (nextProcessor != null) {
                    nextProcessor.processRequest(si); // 执行下一个责任链
                    if (nextProcessor instanceof Flushable) {
                        ((Flushable) nextProcessor).flush();
                    }
                }
                continue;
            }
            toFlush.add(si);
            if (shouldFlush()) {
                flush();
            }
            ServerMetrics.getMetrics().SYNC_PROCESS_TIME.add(Time.currentElapsedTime() - startProcessTime);
        }
    } catch (Throwable t) {
        handleException(this.getName(), t);
    }
    LOG.info("SyncRequestProcessor exited!");
}
```

### FinalRequestProcessor.processRequest

这个是最终的一个处理器，主要负责把已经commit的写操作应用到本机，对于读操作则从本机中读取数据并返回给client

> org.apache.zookeeper.server.FinalRequestProcessor#processRequest

```java
public void processRequest(Request request) {
    LOG.debug("Processing request:: {}", request);

    // request.addRQRec(">final");
    long traceMask = ZooTrace.CLIENT_REQUEST_TRACE_MASK;
    if (request.type == OpCode.ping) {
        traceMask = ZooTrace.SERVER_PING_TRACE_MASK;
    }
    if (LOG.isTraceEnabled()) {
        ZooTrace.logRequest(LOG, traceMask, 'E', request, "");
    }

    ProcessTxnResult rc = zks.processTxn(request);

    // ZOOKEEPER-558:
    // In some cases the server does not close the connection (e.g., closeconn buffer
    // was not being queued — ZOOKEEPER-558) properly. This happens, for example,
    // when the client closes the connection. The server should still close the session, though.
    // Calling closeSession() after losing the cnxn, results in the client close session response being dropped.
    if (request.type == OpCode.closeSession && connClosedByClient(request)) {
        // We need to check if we can close the session id.
        // Sometimes the corresponding ServerCnxnFactory could be null because
        // we are just playing diffs from the leader.
        if (closeSession(zks.serverCnxnFactory, request.sessionId)
            || closeSession(zks.secureServerCnxnFactory, request.sessionId)) {
            return;
        }
    }

    if (request.getHdr() != null) {
        /*
             * Request header is created only by the leader, so this must be
             * a quorum request. Since we're comparing timestamps across hosts,
             * this metric may be incorrect. However, it's still a very useful
             * metric to track in the happy case. If there is clock drift,
             * the latency can go negative. Note: headers use wall time, not
             * CLOCK_MONOTONIC.
             */
        long propagationLatency = Time.currentWallTime() - request.getHdr().getTime();
        if (propagationLatency >= 0) {
            ServerMetrics.getMetrics().PROPAGATION_LATENCY.add(propagationLatency);
        }
    }

    if (request.cnxn == null) {
        return;
    }
    ServerCnxn cnxn = request.cnxn;

    long lastZxid = zks.getZKDatabase().getDataTreeLastProcessedZxid();

    String lastOp = "NA";
    // Notify ZooKeeperServer that the request has finished so that it can
    // update any request accounting/throttling limits
    zks.decInProcess();
    zks.requestFinished(request);
    Code err = Code.OK;
    Record rsp = null;
    String path = null;
    try {
        if (request.getHdr() != null && request.getHdr().getType() == OpCode.error) {
            AuditHelper.addAuditLog(request, rc, true);
            /*
                 * When local session upgrading is disabled, leader will
                 * reject the ephemeral node creation due to session expire.
                 * However, if this is the follower that issue the request,
                 * it will have the correct error code, so we should use that
                 * and report to user
                 */
            if (request.getException() != null) {
                throw request.getException();
            } else {
                throw KeeperException.create(KeeperException.Code.get(((ErrorTxn) request.getTxn()).getErr()));
            }
        }

        KeeperException ke = request.getException();
        if (ke instanceof SessionMovedException) {
            throw ke;
        }
        if (ke != null && request.type != OpCode.multi) {
            throw ke;
        }

        LOG.debug("{}", request);

        if (request.isStale()) {
            ServerMetrics.getMetrics().STALE_REPLIES.add(1);
        }
        AuditHelper.addAuditLog(request, rc);
        switch (request.type) {
            case OpCode.ping: {
                lastOp = "PING";
                updateStats(request, lastOp, lastZxid);

                cnxn.sendResponse(new ReplyHeader(ClientCnxn.PING_XID, lastZxid, 0), null, "response");
                return;
            }
            case OpCode.createSession: {
                lastOp = "SESS";
                updateStats(request, lastOp, lastZxid);

                zks.finishSessionInit(request.cnxn, true);
                return;
            }
            case OpCode.multi: {
                lastOp = "MULT";
                rsp = new MultiResponse();

                for (ProcessTxnResult subTxnResult : rc.multiResult) {

                    OpResult subResult;

                    switch (subTxnResult.type) {
                        case OpCode.check:
                            subResult = new CheckResult();
                            break;
                        case OpCode.create:
                            subResult = new CreateResult(subTxnResult.path);
                            break;
                        case OpCode.create2:
                        case OpCode.createTTL:
                        case OpCode.createContainer:
                            subResult = new CreateResult(subTxnResult.path, subTxnResult.stat);
                            break;
                        case OpCode.delete:
                        case OpCode.deleteContainer:
                            subResult = new DeleteResult();
                            break;
                        case OpCode.setData:
                            subResult = new SetDataResult(subTxnResult.stat);
                            break;
                        case OpCode.error:
                            subResult = new ErrorResult(subTxnResult.err);
                            if (subTxnResult.err == Code.SESSIONMOVED.intValue()) {
                                throw new SessionMovedException();
                            }
                            break;
                        default:
                            throw new IOException("Invalid type of op");
                    }

                    ((MultiResponse) rsp).add(subResult);
                }

                break;
            }
            case OpCode.multiRead: {
                lastOp = "MLTR";
                MultiOperationRecord multiReadRecord = new MultiOperationRecord();
                ByteBufferInputStream.byteBuffer2Record(request.request, multiReadRecord);
                rsp = new MultiResponse();
                OpResult subResult;
                for (Op readOp : multiReadRecord) {
                    try {
                        Record rec;
                        switch (readOp.getType()) {
                            case OpCode.getChildren:
                                rec = handleGetChildrenRequest(readOp.toRequestRecord(), cnxn, request.authInfo);
                                subResult = new GetChildrenResult(((GetChildrenResponse) rec).getChildren());
                                break;
                            case OpCode.getData:
                                rec = handleGetDataRequest(readOp.toRequestRecord(), cnxn, request.authInfo);
                                GetDataResponse gdr = (GetDataResponse) rec;
                                subResult = new GetDataResult(gdr.getData(), gdr.getStat());
                                break;
                            default:
                                throw new IOException("Invalid type of readOp");
                        }
                    } catch (KeeperException e) {
                        subResult = new ErrorResult(e.code().intValue());
                    }
                    ((MultiResponse) rsp).add(subResult);
                }
                break;
            }
            case OpCode.create: {
                lastOp = "CREA";
                rsp = new CreateResponse(rc.path);
                err = Code.get(rc.err);
                requestPathMetricsCollector.registerRequest(request.type, rc.path);
                break;
            }
            case OpCode.create2:
            case OpCode.createTTL:
            case OpCode.createContainer: {
                lastOp = "CREA";
                rsp = new Create2Response(rc.path, rc.stat);
                err = Code.get(rc.err);
                requestPathMetricsCollector.registerRequest(request.type, rc.path);
                break;
            }
            case OpCode.delete:
            case OpCode.deleteContainer: {
                lastOp = "DELE";
                err = Code.get(rc.err);
                requestPathMetricsCollector.registerRequest(request.type, rc.path);
                break;
            }
            case OpCode.setData: {
                lastOp = "SETD";
                rsp = new SetDataResponse(rc.stat);
                err = Code.get(rc.err);
                requestPathMetricsCollector.registerRequest(request.type, rc.path);
                break;
            }
            case OpCode.reconfig: {
                lastOp = "RECO";
                rsp = new GetDataResponse(
                    ((QuorumZooKeeperServer) zks).self.getQuorumVerifier().toString().getBytes(),
                    rc.stat);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.setACL: {
                lastOp = "SETA";
                rsp = new SetACLResponse(rc.stat);
                err = Code.get(rc.err);
                requestPathMetricsCollector.registerRequest(request.type, rc.path);
                break;
            }
            case OpCode.closeSession: {
                lastOp = "CLOS";
                err = Code.get(rc.err);
                break;
            }
            case OpCode.sync: {
                lastOp = "SYNC";
                SyncRequest syncRequest = new SyncRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, syncRequest);
                rsp = new SyncResponse(syncRequest.getPath());
                requestPathMetricsCollector.registerRequest(request.type, syncRequest.getPath());
                break;
            }
            case OpCode.check: {
                lastOp = "CHEC";
                rsp = new SetDataResponse(rc.stat);
                err = Code.get(rc.err);
                break;
            }
            case OpCode.exists: { // 进入到exists请求
                lastOp = "EXIS";
                // TODO we need to figure out the security requirement for this!
                // 构建一个Exists请求
                ExistsRequest existsRequest = new ExistsRequest();
                // 反序列化 (将ByteBuffer反序列化成为ExitsRequest.这个就是我们在客户端发起请求的时候传递过来的Request对象
                ByteBufferInputStream.byteBuffer2Record(request.request, existsRequest);
                path = existsRequest.getPath(); // 得到请求的路径
                if (path.indexOf('\0') != -1) {
                    throw new KeeperException.BadArgumentsException();
                }
                // 终于找到一个很关键的代码，判断请求的getWatch是否存在，如果存在，则传递cnxn（servercnxn）
			   // 对于exists请求，需要监听data变化事件，添加watcher
                Stat stat = zks.getZKDatabase().statNode(path, existsRequest.getWatch() ? cnxn : null);
                rsp = new ExistsResponse(stat); // 返回元数据
                requestPathMetricsCollector.registerRequest(request.type, path);
                break;
            }
            case OpCode.getData: {
                lastOp = "GETD";
                GetDataRequest getDataRequest = new GetDataRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, getDataRequest);
                path = getDataRequest.getPath();
                rsp = handleGetDataRequest(getDataRequest, cnxn, request.authInfo);
                requestPathMetricsCollector.registerRequest(request.type, path);
                break;
            }
            case OpCode.setWatches: {
                lastOp = "SETW";
                SetWatches setWatches = new SetWatches();
                // TODO we really should not need this
                request.request.rewind();
                ByteBufferInputStream.byteBuffer2Record(request.request, setWatches);
                long relativeZxid = setWatches.getRelativeZxid();
                zks.getZKDatabase()
                    .setWatches(
                    relativeZxid,
                    setWatches.getDataWatches(),
                    setWatches.getExistWatches(),
                    setWatches.getChildWatches(),
                    Collections.emptyList(),
                    Collections.emptyList(),
                    cnxn);
                break;
            }
            case OpCode.setWatches2: {
                lastOp = "STW2";
                SetWatches2 setWatches = new SetWatches2();
                // TODO we really should not need this
                request.request.rewind();
                ByteBufferInputStream.byteBuffer2Record(request.request, setWatches);
                long relativeZxid = setWatches.getRelativeZxid();
                zks.getZKDatabase().setWatches(relativeZxid,
                                               setWatches.getDataWatches(),
                                               setWatches.getExistWatches(),
                                               setWatches.getChildWatches(),
                                               setWatches.getPersistentWatches(),
                                               setWatches.getPersistentRecursiveWatches(),
                                               cnxn);
                break;
            }
            case OpCode.addWatch: {
                lastOp = "ADDW";
                AddWatchRequest addWatcherRequest = new AddWatchRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request,
                                                        addWatcherRequest);
                zks.getZKDatabase().addWatch(addWatcherRequest.getPath(), cnxn, addWatcherRequest.getMode());
                rsp = new ErrorResponse(0);
                break;
            }
            case OpCode.getACL: {
                lastOp = "GETA";
                GetACLRequest getACLRequest = new GetACLRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, getACLRequest);
                path = getACLRequest.getPath();
                DataNode n = zks.getZKDatabase().getNode(path);
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                zks.checkACL(
                    request.cnxn,
                    zks.getZKDatabase().aclForNode(n),
                    ZooDefs.Perms.READ | ZooDefs.Perms.ADMIN, request.authInfo, path,
                    null);

                Stat stat = new Stat();
                List<ACL> acl = zks.getZKDatabase().getACL(path, stat);
                requestPathMetricsCollector.registerRequest(request.type, getACLRequest.getPath());

                try {
                    zks.checkACL(
                        request.cnxn,
                        zks.getZKDatabase().aclForNode(n),
                        ZooDefs.Perms.ADMIN,
                        request.authInfo,
                        path,
                        null);
                    rsp = new GetACLResponse(acl, stat);
                } catch (KeeperException.NoAuthException e) {
                    List<ACL> acl1 = new ArrayList<ACL>(acl.size());
                    for (ACL a : acl) {
                        if ("digest".equals(a.getId().getScheme())) {
                            Id id = a.getId();
                            Id id1 = new Id(id.getScheme(), id.getId().replaceAll(":.*", ":x"));
                            acl1.add(new ACL(a.getPerms(), id1));
                        } else {
                            acl1.add(a);
                        }
                    }
                    rsp = new GetACLResponse(acl1, stat);
                }
                break;
            }
            case OpCode.getChildren: {
                lastOp = "GETC";
                GetChildrenRequest getChildrenRequest = new GetChildrenRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, getChildrenRequest);
                path = getChildrenRequest.getPath();
                rsp = handleGetChildrenRequest(getChildrenRequest, cnxn, request.authInfo);
                requestPathMetricsCollector.registerRequest(request.type, path);
                break;
            }
            case OpCode.getAllChildrenNumber: {
                lastOp = "GETACN";
                GetAllChildrenNumberRequest getAllChildrenNumberRequest = new GetAllChildrenNumberRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, getAllChildrenNumberRequest);
                path = getAllChildrenNumberRequest.getPath();
                DataNode n = zks.getZKDatabase().getNode(path);
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                zks.checkACL(
                    request.cnxn,
                    zks.getZKDatabase().aclForNode(n),
                    ZooDefs.Perms.READ,
                    request.authInfo,
                    path,
                    null);
                int number = zks.getZKDatabase().getAllChildrenNumber(path);
                rsp = new GetAllChildrenNumberResponse(number);
                break;
            }
            case OpCode.getChildren2: {
                lastOp = "GETC";
                GetChildren2Request getChildren2Request = new GetChildren2Request();
                ByteBufferInputStream.byteBuffer2Record(request.request, getChildren2Request);
                Stat stat = new Stat();
                path = getChildren2Request.getPath();
                DataNode n = zks.getZKDatabase().getNode(path);
                if (n == null) {
                    throw new KeeperException.NoNodeException();
                }
                zks.checkACL(
                    request.cnxn,
                    zks.getZKDatabase().aclForNode(n),
                    ZooDefs.Perms.READ,
                    request.authInfo, path,
                    null);
                List<String> children = zks.getZKDatabase()
                    .getChildren(path, stat, getChildren2Request.getWatch() ? cnxn : null);
                rsp = new GetChildren2Response(children, stat);
                requestPathMetricsCollector.registerRequest(request.type, path);
                break;
            }
            case OpCode.checkWatches: {
                lastOp = "CHKW";
                CheckWatchesRequest checkWatches = new CheckWatchesRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, checkWatches);
                WatcherType type = WatcherType.fromInt(checkWatches.getType());
                path = checkWatches.getPath();
                boolean containsWatcher = zks.getZKDatabase().containsWatcher(path, type, cnxn);
                if (!containsWatcher) {
                    String msg = String.format(Locale.ENGLISH, "%s (type: %s)", path, type);
                    throw new KeeperException.NoWatcherException(msg);
                }
                requestPathMetricsCollector.registerRequest(request.type, checkWatches.getPath());
                break;
            }
            case OpCode.removeWatches: {
                lastOp = "REMW";
                RemoveWatchesRequest removeWatches = new RemoveWatchesRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, removeWatches);
                WatcherType type = WatcherType.fromInt(removeWatches.getType());
                path = removeWatches.getPath();
                boolean removed = zks.getZKDatabase().removeWatch(path, type, cnxn);
                if (!removed) {
                    String msg = String.format(Locale.ENGLISH, "%s (type: %s)", path, type);
                    throw new KeeperException.NoWatcherException(msg);
                }
                requestPathMetricsCollector.registerRequest(request.type, removeWatches.getPath());
                break;
            }
            case OpCode.getEphemerals: {
                lastOp = "GETE";
                GetEphemeralsRequest getEphemerals = new GetEphemeralsRequest();
                ByteBufferInputStream.byteBuffer2Record(request.request, getEphemerals);
                String prefixPath = getEphemerals.getPrefixPath();
                Set<String> allEphems = zks.getZKDatabase().getDataTree().getEphemerals(request.sessionId);
                List<String> ephemerals = new ArrayList<>();
                if (StringUtils.isBlank(prefixPath) || "/".equals(prefixPath.trim())) {
                    ephemerals.addAll(allEphems);
                } else {
                    for (String p : allEphems) {
                        if (p.startsWith(prefixPath)) {
                            ephemerals.add(p);
                        }
                    }
                }
                rsp = new GetEphemeralsResponse(ephemerals);
                break;
            }
        }
    } catch (SessionMovedException e) {
        // session moved is a connection level error, we need to tear
        // down the connection otw ZOOKEEPER-710 might happen
        // ie client on slow follower starts to renew session, fails
        // before this completes, then tries the fast follower (leader)
        // and is successful, however the initial renew is then
        // successfully fwd/processed by the leader and as a result
        // the client and leader disagree on where the client is most
        // recently attached (and therefore invalid SESSION MOVED generated)
        cnxn.sendCloseSession();
        return;
    } catch (KeeperException e) {
        err = e.code();
    } catch (Exception e) {
        // log at error level as we are returning a marshalling
        // error to the user
        LOG.error("Failed to process {}", request, e);
        StringBuilder sb = new StringBuilder();
        ByteBuffer bb = request.request;
        bb.rewind();
        while (bb.hasRemaining()) {
            sb.append(Integer.toHexString(bb.get() & 0xff));
        }
        LOG.error("Dumping request buffer: 0x{}", sb.toString());
        err = Code.MARSHALLINGERROR;
    }

    ReplyHeader hdr = new ReplyHeader(request.cxid, lastZxid, err.intValue());

    updateStats(request, lastOp, lastZxid);

    try {
        if (path == null || rsp == null) {
            cnxn.sendResponse(hdr, rsp, "response");
        } else {
            int opCode = request.type;
            Stat stat = null;
            // Serialized read and get children responses could be cached by the connection
            // object. Cache entries are identified by their path and last modified zxid,
            // so these values are passed along with the response.
            switch (opCode) {
                case OpCode.getData : {
                    GetDataResponse getDataResponse = (GetDataResponse) rsp;
                    stat = getDataResponse.getStat();
                    cnxn.sendResponse(hdr, rsp, "response", path, stat, opCode);
                    break;
                }
                case OpCode.getChildren2 : {
                    GetChildren2Response getChildren2Response = (GetChildren2Response) rsp;
                    stat = getChildren2Response.getStat();
                    cnxn.sendResponse(hdr, rsp, "response", path, stat, opCode);
                    break;
                }
                default:
                    cnxn.sendResponse(hdr, rsp, "response");
            }
        }

        if (request.type == OpCode.closeSession) {
            cnxn.sendCloseSession();
        }
    } catch (IOException e) {
        LOG.error("FIXMSG", e);
    }
}
```

### statNode的处理逻辑

按照前面我们讲过的原理，statNode应该会做两个事情

- 获取指定节点的元数据
- 保存针对该节点的事件监听

> 注意，在这个方法中，将ServerCnxn向上转型为Watcher了。

> org.apache.zookeeper.server.DataTree#statNode

```java
public Stat statNode(String path, Watcher watcher) throws KeeperException.NoNodeException {
    Stat stat = new Stat();
    DataNode n = nodes.get(path); // 根据path获取节点数据
    if (watcher != null) {        // 如果watcher不为空，则将当前的watcher和path进行绑定
        dataWatches.addWatch(path, watcher);
    }
    if (n == null) {
        throw new KeeperException.NoNodeException();
    }
    synchronized (n) {
        n.copyStat(stat);          // copy属性设置到stat中
    }
    updateReadStat(path, 0L);
    return stat;
}
```

### WatchManager.addWatch

通过WatchManager来保存指定节点的事件监听，WatchManager维护了两个集合。

```java
public static final WatcherMode DEFAULT_WATCHER_MODE = WatcherMode.STANDARD;

public class WatchManager implements IWatchManager {
    private final Map<String, Set<Watcher>> watchTable = new HashMap<>(); // 一个Path对应多个Watcher
    private final Map<Watcher, Set<String>> watch2Paths = new HashMap<>();// 一个Watcher对应多个Path
    
    @Override
    public boolean addWatch(String path, Watcher watcher) {
        return addWatch(path, watcher, WatcherMode.DEFAULT_WATCHER_MODE); // 默认模式：WatcherMode.STANDARD
    }

    @Override
    public synchronized boolean addWatch(String path, Watcher watcher, WatcherMode watcherMode) {
        if (isDeadWatcher(watcher)) { // 判断这个连接是否已经断开，如果是，则直接忽略
            LOG.debug("Ignoring addWatch with closed cnxn");
            return false;
        }
        // 存储指定path对应的watcher，一个path可以存在多个客户端进行watcher，所以保存了一个set集合
        Set<Watcher> list = watchTable.get(path); // 判断watcherTable中是否存在当前路径对应的watcher
        if (list == null) {
            // 如果节点上的watcher很少，就不要浪费内存，只添加4个长度，后续进行扩容
            // don't waste memory if there are few watches on a node
            // rehash when the 4th entry is added, doubling size thereafter
            // seems like a good compromise
            list = new HashSet<>(4);
            watchTable.put(path, list);
        }
        list.add(watcher); // 添加到 watchTable 中

        // watcher到节点的映射关系表
        Set<String> paths = watch2Paths.get(watcher);
        if (paths == null) { // 如果为空，则初始化并保存
            // cnxns typically have many watches, so use default cap here
            paths = new HashSet<>();
            watch2Paths.put(watcher, paths); // 添加到 watch2Paths 中
        }

        // 设置watch的模式
        // watch 有三种类型，一种是PERSISTENT、一种是PERSISTENT_RECURSIVE、STANDARD，前者是持久化订阅，后者是持久化递归订阅，所谓递归订阅就是针对监听的节点的子节点的变化都会触发监听,
        watcherModeManager.setWatcherMode(watcher, path, watcherMode);

        return paths.add(path); // 将path保存到集合
    }
```

watchTable表示从节点路径到watcher集合的映射 

而watch2Paths则表示从watcher到所有节点路径集合的映射。

### 返回处理结果

在FinalRequestProcessor的processRequest方法中，将处理结果rsp返回给客户端。

```java
public void processRequest(Request request) {
	// 省略代码...
    try {
        if (path == null || rsp == null) {
            cnxn.sendResponse(hdr, rsp, "response"); // 发送响应信息
        } else {
            int opCode = request.type;
            Stat stat = null;
            // Serialized read and get children responses could be cached by the connection
            // object. Cache entries are identified by their path and last modified zxid,
            // so these values are passed along with the response.
            switch (opCode) {
                case OpCode.getData : {
                    GetDataResponse getDataResponse = (GetDataResponse) rsp;
                    stat = getDataResponse.getStat();
                    cnxn.sendResponse(hdr, rsp, "response", path, stat, opCode);
                    break;
                }
                case OpCode.getChildren2 : {
                    GetChildren2Response getChildren2Response = (GetChildren2Response) rsp;
                    stat = getChildren2Response.getStat();
                    cnxn.sendResponse(hdr, rsp, "response", path, stat, opCode);
                    break;
                }
                default:
                    cnxn.sendResponse(hdr, rsp, "response");
            }
        }

        if (request.type == OpCode.closeSession) {
            cnxn.sendCloseSession();
        }
    } catch (IOException e) {
        LOG.error("FIXMSG", e);
    }
}
```

## 客户端收到请求后的处理

客户端接收请求的处理是在 `ClientCnxnSocketNIO` 的doIO中，之前客户端发起请求是写，现在客户端收到请求，则是一个读操作，也就是当客户端收到服务端的数据时会触发一下代码的执行。其中很关键的是`sendThread.readResponse(incomingBuffer);` 来接收服务端的请求。

```java
void doIO(Queue<Packet> pendingQueue, ClientCnxn cnxn) throws InterruptedException, IOException {
    SocketChannel sock = (SocketChannel) sockKey.channel();
    if (sock == null) {
        throw new IOException("Socket is null!");
    }
    if (sockKey.isReadable()) {
        int rc = sock.read(incomingBuffer);
        if (rc < 0) {
            throw new EndOfStreamException("Unable to read additional data from server sessionid 0x"
                                           + Long.toHexString(sessionId)
                                           + ", likely server has closed socket");
        }
        if (!incomingBuffer.hasRemaining()) {
            incomingBuffer.flip();
            if (incomingBuffer == lenBuffer) {
                recvCount.getAndIncrement();
                readLength();
            } else if (!initialized) {
                readConnectResult();
                enableRead();
                if (findSendablePacket(outgoingQueue, sendThread.tunnelAuthInProgress()) != null) {
                    // Since SASL authentication has completed (if client is configured to do so),
                    // outgoing packets waiting in the outgoingQueue can now be sent.
                    enableWrite();
                }
                lenBuffer.clear();
                incomingBuffer = lenBuffer;
                updateLastHeard();
                initialized = true;
            } else {
                // ========================= this ===============================
                sendThread.readResponse(incomingBuffer);
                lenBuffer.clear();
                incomingBuffer = lenBuffer;
                updateLastHeard();
                // ========================= this ===============================
            }
        }
    }
```

### SendThread.readResponse

这个方法里面主要的流程如下：

1. 首先读取header，如果其xid == -2，表明是一个ping的response，return 
2. 如果xid是 -4 ，表明是一个AuthPacket的response return
3. 如果xid是 -1，表明是一个notification,此时要继续读取并构造一个enent，通过 EventThread.queueEvent发送，return 
4. 其它情况下：从pendingQueue拿出一个Packet，校验后更新packet信息

> 对于exists请求，返回的xid=1，则进入到其他情况来处理

> org.apache.zookeeper.ClientCnxn.SendThread#readResponse

```java
void readResponse(ByteBuffer incomingBuffer) throws IOException {
    ByteBufferInputStream bbis = new ByteBufferInputStream(incomingBuffer);
    BinaryInputArchive bbia = BinaryInputArchive.getArchive(bbis);
    ReplyHeader replyHdr = new ReplyHeader();

    replyHdr.deserialize(bbia, "header");
    switch (replyHdr.getXid()) {  //判断返回的信息类型。
        case PING_XID:
            LOG.debug("Got ping response for session id: 0x{} after {}ms.",
                      Long.toHexString(sessionId),
                      ((System.nanoTime() - lastPingSentNs) / 1000000));
            return;
        case AUTHPACKET_XID:
            LOG.debug("Got auth session id: 0x{}", Long.toHexString(sessionId));
            if (replyHdr.getErr() == KeeperException.Code.AUTHFAILED.intValue()) {
                state = States.AUTH_FAILED;
                eventThread.queueEvent(new WatchedEvent(Watcher.Event.EventType.None,
                                                        Watcher.Event.KeeperState.AuthFailed, null));
                eventThread.queueEventOfDeath();
            }
            return;
        case NOTIFICATION_XID:
            LOG.debug("Got notification session id: 0x{}",
                      Long.toHexString(sessionId));
            WatcherEvent event = new WatcherEvent();
            event.deserialize(bbia, "response");

            // convert from a server path to a client path
            if (chrootPath != null) {
                String serverPath = event.getPath();
                if (serverPath.compareTo(chrootPath) == 0) {
                    event.setPath("/");
                } else if (serverPath.length() > chrootPath.length()) {
                    event.setPath(serverPath.substring(chrootPath.length()));
                } else {
                    LOG.warn("Got server path {} which is too short for chroot path {}.",
                             event.getPath(), chrootPath);
                }
            }

            WatchedEvent we = new WatchedEvent(event);
            LOG.debug("Got {} for session id 0x{}", we, Long.toHexString(sessionId));
            eventThread.queueEvent(we);
            return;
        default:
            break;
    }

    // If SASL authentication is currently in progress, construct and
    // send a response packet immediately, rather than queuing a
    // response as with other packets.
    if (tunnelAuthInProgress()) {
        GetSASLRequest request = new GetSASLRequest();
        request.deserialize(bbia, "token");
        zooKeeperSaslClient.respondToServer(request.getToken(), ClientCnxn.this);
        return;
    }

    Packet packet;
    synchronized (pendingQueue) { // pendingQueue中存储的是客户端传递过去的数据包packet
        if (pendingQueue.size() == 0) {
            throw new IOException("Nothing in the queue, but got " + replyHdr.getXid());
        }
        packet = pendingQueue.remove();  // 表示这个请求包已经处理完成，直接移除
    }
    /*
    * Since requests are processed in order, we better get a response
    * to the first request!
    */
    try {
        // 确保是同一个id
        if (packet.requestHeader.getXid() != replyHdr.getXid()) {
            packet.replyHeader.setErr(KeeperException.Code.CONNECTIONLOSS.intValue());
            throw new IOException("Xid out of order. Got Xid " + replyHdr.getXid()
                                  + " with err " + replyHdr.getErr()
                                  + " expected Xid " + packet.requestHeader.getXid()
                                  + " for a packet with details: " + packet);
        }
		// 把服务端返回的头信息设置到packet中
        packet.replyHeader.setXid(replyHdr.getXid());
        packet.replyHeader.setErr(replyHdr.getErr());
        packet.replyHeader.setZxid(replyHdr.getZxid());
        if (replyHdr.getZxid() > 0) {
            lastZxid = replyHdr.getZxid();
        }
        // 反序列化返回的消息体
        if (packet.response != null && replyHdr.getErr() == 0) {
            packet.response.deserialize(bbia, "response");
        }

        LOG.debug("Reading reply session id: 0x{}, packet:: {}", Long.toHexString(sessionId), packet);
    } finally {
        finishPacket(packet);  // 调用finishPacket完成消息的处理
    }
}
```

### finishPacket

通过前面客户端和服务端的交互，可以确定服务端已经成功保存了watcher这个事件，那么受到服务端 的确认之后，客户端会把这个watcher保存到本地的事件中。

> 所以，finishPacket主要功能是把从 Packet 中取出对应的 Watcher 并注册到 ZKWatchManager 中去

> org.apache.zookeeper.ClientCnxn#finishPacket

```java
// @VisibleForTesting
protected void finishPacket(Packet p) {
    int err = p.replyHeader.getErr();
    if (p.watchRegistration != null) {
        // 将事件注册到zkwatchemanager中
		// watchRegistration，熟悉吗？在组装请求的时候，我们初始化了这个对象
		// 把watchRegistration 子类里面的 Watcher 实例放到 ZKWatchManager 的 existsWatches 中存储起来。
        p.watchRegistration.register(err);
    }
    // Add all the removed watch events to the event queue, so that the
    // clients will be notified with 'Data/Child WatchRemoved' event type.
    
    // 将所有已删除的监听时间添加到事件队列,这样客户端可以收到 `data/child`事件已删除的类型通知
    if (p.watchDeregistration != null) {
        Map<EventType, Set<Watcher>> materializedWatchers = null;
        try {
            materializedWatchers = p.watchDeregistration.unregister(err);
            for (Entry<EventType, Set<Watcher>> entry : materializedWatchers.entrySet()) {
                Set<Watcher> watchers = entry.getValue();
                if (watchers.size() > 0) {
                    queueEvent(p.watchDeregistration.getClientPath(), err, watchers, entry.getKey());
                    // ignore connectionloss when removing from local
                    // session
                    p.replyHeader.setErr(Code.OK.intValue());
                }
            }
        } catch (KeeperException.NoWatcherException nwe) {
            p.replyHeader.setErr(nwe.code().intValue());
        } catch (KeeperException ke) {
            p.replyHeader.setErr(ke.code().intValue());
        }
    }

    // cb就是AsnycCallback，如果为null，表明是同步调用的接口，不需要异步回掉，因此，直接notifyAll即可。
	// 这里唤醒的就是在客户端调用exists方法中，wait()的逻辑，这样表示服务处理完成
    if (p.cb == null) {
        synchronized (p) {
            p.finished = true;
            p.notifyAll();
        }
    } else {
        p.finished = true;
        eventThread.queuePacket(p);
    }
}
```

### watchRegistration.register

把path对应的watcher本地回调保存到一个集合中。

```java
/**
* Register a watcher for a particular path.
*/
public abstract class WatchRegistration {

    private Watcher watcher;
    private String clientPath;
    public WatchRegistration(Watcher watcher, String clientPath) {
        this.watcher = watcher;
        this.clientPath = clientPath;
    }

    protected abstract Map<String, Set<Watcher>> getWatches(int rc);

    /**
    * Register the watcher with the set of watches on path.
    * @param rc the result code of the operation that attempted to
    * add the watch on the path.
    */
    public void register(int rc) {
        if (shouldAddWatch(rc)) { // 根据返回的code来决定是否需要添加watch
            Map<String, Set<Watcher>> watches = getWatches(rc);
            synchronized (watches) { // 初始化watches集合
                Set<Watcher> watchers = watches.get(clientPath);
                if (watchers == null) {
                    watchers = new HashSet<Watcher>();
                    watches.put(clientPath, watchers);
                }
                watchers.add(watcher);  // 把watcher保存到watches集合，此时的watcher对应的就是在exists方法中传入的匿名内部类。
            }
        }
    }
    /**
    * Determine whether the watch should be added based on return code.
    * @param rc the result code of the operation that attempted to add the
    * watch on the node
    * @return true if the watch should be added, otw false
    */
    protected boolean shouldAddWatch(int rc) {
        return rc == 0;
    }

}
```

### ZkWatchManager

ZkWatchManager是客户端这边用来保存本地节点对应的watcher回调的管理类，提供了三种不同的事 件管理机制。

```java
class DataWatchRegistration extends WatchRegistration {
    @Override
    protected Map<String, Set<Watcher>> getWatches(int rc) {
        return watchManager.dataWatches;
    }
}

class AddWatchRegistration extends WatchRegistration {
    @Override
    protected Map<String, Set<Watcher>> getWatches(int rc) {
        switch (mode) {
            case PERSISTENT:
                return watchManager.persistentWatches;
            case PERSISTENT_RECURSIVE:
                return watchManager.persistentRecursiveWatches;
        }
        throw new IllegalArgumentException("Mode not supported: " + mode);
    }
}

class ChildWatchRegistration extends WatchRegistration {
    @Override
    protected Map<String, Set<Watcher>> getWatches(int rc) {
        return watchManager.childWatches;
    }
}

class ExistsWatchRegistration extends WatchRegistration {
    @Override
    protected Map<String, Set<Watcher>> getWatches(int rc) {
        return rc == 0 ? watchManager.dataWatches : watchManager.existWatches;
    }
}

static class ZKWatchManager implements ClientWatchManager {

    private final Map<String, Set<Watcher>> dataWatches = new HashMap<String, Set<Watcher>>();
    private final Map<String, Set<Watcher>> existWatches = new HashMap<String, Set<Watcher>>();
    private final Map<String, Set<Watcher>> childWatches = new HashMap<String, Set<Watcher>>();
    private final Map<String, Set<Watcher>> persistentWatches = new HashMap<String, Set<Watcher>>();
    private final Map<String, Set<Watcher>> persistentRecursiveWatches = new HashMap<String, Set<Watcher>>();
    private boolean disableAutoWatchReset;
```

总的来说，当使用ZooKeeper 构造方法或者使用 getData、exists 和 getChildren 三个接口来向 ZooKeeper 服务器注册 Watcher 的时候，首先将此消息传递给服务端，传递成功后，服务端会通知客 户端，然后客户端将该路径和Watcher对应关系存储起来备用。