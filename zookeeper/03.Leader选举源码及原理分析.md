# Leader选举源码及原理分析

## Leader选举原理

> leader选举存在与两个阶段中，一个是服务器启动时的leader选举。 另一个是运行过程中leader节点宕机导致的leader选举

**Leader选举重要参数**

1. 服务器ID（myid）

   比如有三台服务器，编号分别是1,2,3。 

   编号越大在选择算法中的权重越大。

2. zxid（事务id） 

   高32位是代表逻辑时钟epoch，低32位是代表递增编号

   值越大说明数据越新，在选举算法中的权重也越大

3. 逻辑时钟（epoch-logicalclock）

   或者叫投票的次数，同一轮投票过程中的逻辑时钟值是相同的。每投完一次票这个数据就会增加，然后与接收到的其它服务器返回的投票信息中的数值相比，根据不同的值做出不同的判断

4. 选举状态

   LOOKING：竞选状态。 

   FOLLOWING：随从状态，同步leader状态，参与投票。

   OBSERVING：观察状态,同步leader状态，不参与投票。

   LEADING：领导者状态。

### Leader选举流程

#### 服务器启动时的leader选举

> 每个节点启动的时候状态都是LOOKING，处于观望状态，接下来就开始进行选主流程

若进行Leader选举，则至少需要两台机器，这里选取3台机器组成的服务器集群为例。在集群初始化阶段，当有一台服务器Server1启动时，其单独无法进行和完成Leader选举，当第二台服务器Server2启动时，此时两台机器可以相互通信，每台机器都试图找到Leader，于是进入Leader选举过程。选举过程如下 ：

(1) 每个Server发出一个投票。由于是初始情况，Server1和Server2都会将自己作为Leader服务器来进行投票，每次投票会包含所推举的服务器的myid和zxid、epoch，使用(epoch, zxid, myid)来表示，此时Server1的投票为(1, 0)，Server2的投票为(2, 0)，然后各自将这个投票发给集群中其他机器。 

(2) 接受来自各个服务器的投票。集群的每个服务器收到投票后，首先判断该投票的有效性，如检查是否是本轮投票（epoch）、是否来自LOOKING状态的服务器。

(3) 处理投票。针对每一个投票，服务器都需要将别人的投票和自己的投票进行PK，PK规则如下 

​	i. 优先比较epoch

​	ii. 其次检查zxid。zxid比较大的服务器优先作为Leader

​	iii. 如果zxid相同，那么就比较myid。myid较大的服务器作为Leader服务器。

对于Server1而言，它的投票是(1, 0)，接收Server2的投票为(2, 0)，首先会比较两者的zxid，均为0， 再比较myid，此时Server2的myid最大，于是更新自己的投票为(2, 0)，然后重新投票，对于Server2而言，其无须更新自己的投票，只是再次向集群中所有机器发出上一次投票信息即可。

(4) 统计投票。每次投票后，服务器都会统计投票信息，判断是否已经有过半机器接受到相同的投票信息，对于Server1、Server2而言，都统计出集群中已经有两台机器接受了(2, 0)的投票信息，此时便认为已经选出了Leader。

(5) 改变服务器状态。一旦确定了Leader，每个服务器就会更新自己的状态，如果是Follower，那么就变更为FOLLOWING，如果是Leader，就变更为LEADING。

#### 运行过程中的leader选举

当集群中的leader服务器出现宕机或者不可用的情况时，那么整个集群将无法对外提供服务，而是进入新一轮的Leader选举，服务器运行期间的Leader选举和启动时期的Leader选举基本过程是一致的。 

(1) 变更状态。Leader挂后，余下的非Observer服务器都会将自己的服务器状态变更为LOOKING，然后开始进入Leader选举过程。

(2) 每个Server会发出一个投票。在运行期间，每个服务器上的zxid可能不同，此时假定Server1的zxid 为123，Server3的zxid为122；在第一轮投票中，Server1和Server3都会投自己，产生投票(1, 123)， (3, 122)，然后各自将投票发送给集群中所有机器。接收来自各个服务器的投票。与启动时过程相同。

(3) 处理投票。与启动时过程相同，此时，Server1将会成为Leader。

(4) 统计投票。与启动时过程相同。

(5) 改变服务器的状态。与启动时过程相同





启动zookeer server 

- 集群的方式 : QuorumPeerMain 
- 单机的方式 : ZookeeperServerMain

具体流程

- zoo.cfg，加载配置
- 监听2181，（NIO / Netty）-> NIO (TCP协议的课程中有讲到) 
- 初始化一些2888、3888，选举/数据同步的监听，BIO的方式 
- 选举算法的初始化以及选举的执行（leader）
- 本地文件的加载和恢复
- 数据同步

## 源码分析

QuorumPeerMain.main -> 

```java
protected void initializeAndRun(String[] args) throws ConfigException, IOException, AdminServerException {
    QuorumPeerConfig config = new QuorumPeerConfig();
    if (args.length == 1) {
        // 启动时传入 zoo.cfg 文件地址作为参数
        config.parse(args[0]);
    }

    // Start and schedule the the purge task
    DatadirCleanupManager purgeMgr = new DatadirCleanupManager(
        config.getDataDir(),
        config.getDataLogDir(),
        config.getSnapRetainCount(),
        config.getPurgeInterval());
    purgeMgr.start();
	// 如果有传入 zoo.cfg 文件地址作为参数 而且 配置是集群启动
    if (args.length == 1 && config.isDistributed()) {
        runFromConfig(config);
    } else {
        LOG.warn("Either no config or no quorum defined in config, running in standalone mode");
        // there is only server in the quorum -- run as standalone
        // 单机启动
        ZooKeeperServerMain.main(args);
    }
}


public void runFromConfig(QuorumPeerConfig config) throws IOException, AdminServerException {
    try {
        ManagedUtil.registerLog4jMBeans();
    } catch (JMException e) {
        LOG.warn("Unable to register log4j JMX control", e);
    }

    LOG.info("Starting quorum peer, myid=" + config.getServerId());
    MetricsProvider metricsProvider;
    try {
        metricsProvider = MetricsProviderBootstrap.startMetricsProvider(
            config.getMetricsProviderClassName(),
            config.getMetricsProviderConfiguration());
    } catch (MetricsProviderLifeCycleException error) {
        throw new IOException("Cannot boot MetricsProvider " + config.getMetricsProviderClassName(), error);
    }
    
    // 上面都是处理监控埋点
    try {
        ServerMetrics.metricsProviderInitialized(metricsProvider);
        ServerCnxnFactory cnxnFactory = null;
        ServerCnxnFactory secureCnxnFactory = null;

        if (config.getClientPortAddress() != null) { // 不安全协议地址
            cnxnFactory = ServerCnxnFactory.createFactory(); // 创建 NIOServerCnxnFactory 对象,后面用到
            cnxnFactory.configure(config.getClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), false);
        }

        if (config.getSecureClientPortAddress() != null) { // 安全协议地址
            secureCnxnFactory = ServerCnxnFactory.createFactory(); // 创建 NIOServerCnxnFactory 对象，后面用到
            secureCnxnFactory.configure(config.getSecureClientPortAddress(), config.getMaxClientCnxns(), config.getClientPortListenBacklog(), true);
        }

        quorumPeer = getQuorumPeer(); // 创建配置类
        quorumPeer.setTxnFactory(new FileTxnSnapLog(config.getDataLogDir(), config.getDataDir()));
        quorumPeer.enableLocalSessions(config.areLocalSessionsEnabled());
        quorumPeer.enableLocalSessionsUpgrading(config.isLocalSessionsUpgradingEnabled());
        //quorumPeer.setQuorumPeers(config.getAllMembers());
        quorumPeer.setElectionType(config.getElectionAlg());
        quorumPeer.setMyid(config.getServerId());
        quorumPeer.setTickTime(config.getTickTime());
        quorumPeer.setMinSessionTimeout(config.getMinSessionTimeout());
        quorumPeer.setMaxSessionTimeout(config.getMaxSessionTimeout());
        quorumPeer.setInitLimit(config.getInitLimit());
        quorumPeer.setSyncLimit(config.getSyncLimit());
        quorumPeer.setConnectToLearnerMasterLimit(config.getConnectToLearnerMasterLimit());
        quorumPeer.setObserverMasterPort(config.getObserverMasterPort());
        quorumPeer.setConfigFileName(config.getConfigFilename());
        quorumPeer.setClientPortListenBacklog(config.getClientPortListenBacklog());
        quorumPeer.setZKDatabase(new ZKDatabase(quorumPeer.getTxnFactory()));
        quorumPeer.setQuorumVerifier(config.getQuorumVerifier(), false);
        if (config.getLastSeenQuorumVerifier() != null) {
            quorumPeer.setLastSeenQuorumVerifier(config.getLastSeenQuorumVerifier(), false);
        }
        quorumPeer.initConfigInZKDatabase();
        quorumPeer.setCnxnFactory(cnxnFactory);
        quorumPeer.setSecureCnxnFactory(secureCnxnFactory);
        quorumPeer.setSslQuorum(config.isSslQuorum());
        quorumPeer.setUsePortUnification(config.shouldUsePortUnification());
        quorumPeer.setLearnerType(config.getPeerType());
        quorumPeer.setSyncEnabled(config.getSyncEnabled());
        quorumPeer.setQuorumListenOnAllIPs(config.getQuorumListenOnAllIPs());
        if (config.sslQuorumReloadCertFiles) {
            quorumPeer.getX509Util().enableCertFileReloading();
        }
        quorumPeer.setMultiAddressEnabled(config.isMultiAddressEnabled());
        quorumPeer.setMultiAddressReachabilityCheckEnabled(config.isMultiAddressReachabilityCheckEnabled());
        quorumPeer.setMultiAddressReachabilityCheckTimeoutMs(config.getMultiAddressReachabilityCheckTimeoutMs());

        // sets quorum sasl authentication configurations
        quorumPeer.setQuorumSaslEnabled(config.quorumEnableSasl);
        if (quorumPeer.isQuorumSaslAuthEnabled()) {
            quorumPeer.setQuorumServerSaslRequired(config.quorumServerRequireSasl);
            quorumPeer.setQuorumLearnerSaslRequired(config.quorumLearnerRequireSasl);
            quorumPeer.setQuorumServicePrincipal(config.quorumServicePrincipal);
            quorumPeer.setQuorumServerLoginContext(config.quorumServerLoginContext);
            quorumPeer.setQuorumLearnerLoginContext(config.quorumLearnerLoginContext);
        }
        quorumPeer.setQuorumCnxnThreadsSize(config.quorumCnxnThreadsSize);
        quorumPeer.initialize();

        if (config.jvmPauseMonitorToRun) {
            quorumPeer.setJvmPauseMonitor(new JvmPauseMonitor(config));
        }

        quorumPeer.start(); // quorumPeer 继承了Thread类，这里启动线程
        ZKAuditProvider.addZKStartStopAuditLog();
        // 作用主线程挂起
        quorumPeer.join();
    } catch (InterruptedException e) {
        // warn, but generally this is ok
        LOG.warn("Quorum Peer interrupted", e);
    } finally {
        if (metricsProvider != null) {
            try {
                metricsProvider.stop();
            } catch (Throwable error) {
                LOG.warn("Error while stopping metrics", error);
            }
        }
    }
}
```

```java

public static ServerCnxnFactory createFactory() throws IOException {
    // 读取系统配置zookeeper.serverCnxnFactory
    String serverCnxnFactoryName = System.getProperty(ZOOKEEPER_SERVER_CNXN_FACTORY);
    if (serverCnxnFactoryName == null) {
        // 一般都没有配置，默认初始化 NIOServerCnxnFactory 类
        serverCnxnFactoryName = NIOServerCnxnFactory.class.getName();
    }
    try {
        ServerCnxnFactory serverCnxnFactory = (ServerCnxnFactory) Class.forName(serverCnxnFactoryName)
            .getDeclaredConstructor()
            .newInstance();
        LOG.info("Using {} as server connection factory", serverCnxnFactoryName);
        return serverCnxnFactory;
    } catch (Exception e) {
        IOException ioe = new IOException("Couldn't instantiate " + serverCnxnFactoryName, e);
        throw ioe;
    }
}
```

> org.apache.zookeeper.server.NIOServerCnxnFactory#configure
>
> ```java
> @Override
> public void configure(InetSocketAddress addr, int maxcc, int backlog, boolean secure) throws IOException {
>     if (secure) {
>         throw new UnsupportedOperationException("SSL isn't supported in NIOServerCnxn");
>     }
>     configureSaslLogin();
> 
>     maxClientCnxns = maxcc;
>     initMaxCnxns();
>     // 会话超时时间
>     sessionlessCnxnTimeout = Integer.getInteger(ZOOKEEPER_NIO_SESSIONLESS_CNXN_TIMEOUT, 10000);
>     // We also use the sessionlessCnxnTimeout as expiring interval for
>     // cnxnExpiryQueue. These don't need to be the same, but the expiring
>     // interval passed into the ExpiryQueue() constructor below should be
>     // less than or equal to the timeout.
>     cnxnExpiryQueue = new ExpiryQueue<NIOServerCnxn>(sessionlessCnxnTimeout);
>     expirerThread = new ConnectionExpirerThread();
> 	
>     // 获得核心数
>     int numCores = Runtime.getRuntime().availableProcessors();
>     // 32 cores sweet spot seems to be 4 selector threads
>     numSelectorThreads = Integer.getInteger(
>         ZOOKEEPER_NIO_NUM_SELECTOR_THREADS,
>         Math.max((int) Math.sqrt((float) numCores / 2), 1));
>     if (numSelectorThreads < 1) {
>         throw new IOException("numSelectorThreads must be at least 1");
>     }
> 	// selector工作线程数
>     numWorkerThreads = Integer.getInteger(ZOOKEEPER_NIO_NUM_WORKER_THREADS, 2 * numCores);
>     workerShutdownTimeoutMS = Long.getLong(ZOOKEEPER_NIO_SHUTDOWN_TIMEOUT, 5000);
> 
>     String logMsg = "Configuring NIO connection handler with "
>         + (sessionlessCnxnTimeout / 1000) + "s sessionless connection timeout, "
>         + numSelectorThreads + " selector thread(s), "
>         + (numWorkerThreads > 0 ? numWorkerThreads : "no") + " worker threads, and "
>         + (directBufferBytes == 0 ? "gathered writes." : ("" + (directBufferBytes / 1024) + " kB direct buffers."));
>     LOG.info(logMsg);
>     for (int i = 0; i < numSelectorThreads; ++i) {
>         selectorThreads.add(new SelectorThread(i)); // 创建selector工作线程
>     }
> 
>     listenBacklog = backlog;
>     // 打开一个ServerSocketChannel实例
>     this.ss = ServerSocketChannel.open();
>     ss.socket().setReuseAddress(true);
>     LOG.info("binding to port {}", addr);
>     if (listenBacklog == -1) {
>         ss.socket().bind(addr); // 绑定监听端口
>     } else {
>         ss.socket().bind(addr, listenBacklog);
>     }
>     ss.configureBlocking(false); // 配置非阻塞
>     // 接收线程
>     // acceptThread 用于处理接收客户端请求
>     // selectorThreads 用来处理selector的读写请求
>     acceptThread = new AcceptThread(ss, addr, selectorThreads);
> }
> ```
>
> org.apache.zookeeper.server.NIOServerCnxnFactory.AcceptThread#AcceptThread
>
> ```java
> public AcceptThread(ServerSocketChannel ss, InetSocketAddress addr, Set<SelectorThread> selectorThreads) throws IOException {
>     super("NIOServerCxnFactory.AcceptThread:" + addr);
>     this.acceptSocket = ss;
>     // NIO 注册 接收事件处理
>     this.acceptKey = acceptSocket.register(selector, SelectionKey.OP_ACCEPT);
>     this.selectorThreads = Collections.unmodifiableList(new ArrayList<SelectorThread>(selectorThreads));
>     selectorIterator = this.selectorThreads.iterator();
> }
> ```
>
> 

> org.apache.zookeeper.server.quorum.QuorumPeer#start

```java
@Override
public synchronized void start() {
    // 校验myid是否配置正确
    if (!getView().containsKey(myid)) {
        throw new RuntimeException("My id " + myid + " not in the peer list");
    }
    // 加载 myid、zxid、epoch
    loadDataBase();
    // 这里来启动2181的服务监听. ServerSocketChannel
    startServerCnxnFactory();
    try {
        adminServer.start();
    } catch (AdminServerException e) {
        LOG.warn("Problem starting AdminServer", e);
        System.out.println(e);
    }
    // 开启leader选举
    startLeaderElection();
    startJvmPauseMonitor();
    super.start();
}
```

### loadDataBase (加载myid、zxid、epoch)

```java
private void loadDataBase() {
    try {
        // 加载数据
        zkDb.loadDataBase();

        // load the epochs
        long lastProcessedZxid = zkDb.getDataTree().lastProcessedZxid;
        long epochOfZxid = ZxidUtils.getEpochFromZxid(lastProcessedZxid);
        try {
            // 获取当前 epoch
            currentEpoch = readLongFromFile(CURRENT_EPOCH_FILENAME);
        } catch (FileNotFoundException e) {
            // pick a reasonable epoch number
            // this should only happen once when moving to a
            // new code version
            currentEpoch = epochOfZxid;
            LOG.info(
                "{} not found! Creating with a reasonable default of {}. "
                + "This should only happen when you are upgrading your installation",
                CURRENT_EPOCH_FILENAME,
                currentEpoch);
            writeLongToFile(CURRENT_EPOCH_FILENAME, currentEpoch);
        }
        if (epochOfZxid > currentEpoch) {
            throw new IOException("The current epoch, "
                                  + ZxidUtils.zxidToString(currentEpoch)
                                  + ", is older than the last zxid, "
                                  + lastProcessedZxid);
        }
        try {
            acceptedEpoch = readLongFromFile(ACCEPTED_EPOCH_FILENAME);
        } catch (FileNotFoundException e) {
            // pick a reasonable epoch number
            // this should only happen once when moving to a
            // new code version
            acceptedEpoch = epochOfZxid;
            LOG.info(
                "{} not found! Creating with a reasonable default of {}. "
                + "This should only happen when you are upgrading your installation",
                ACCEPTED_EPOCH_FILENAME,
                acceptedEpoch);
            writeLongToFile(ACCEPTED_EPOCH_FILENAME, acceptedEpoch);
        }
        if (acceptedEpoch < currentEpoch) {
            throw new IOException("The accepted epoch, "
                                  + ZxidUtils.zxidToString(acceptedEpoch)
                                  + " is less than the current epoch, "
                                  + ZxidUtils.zxidToString(currentEpoch));
        }
    } catch (IOException ie) {
        LOG.error("Unable to load database on disk", ie);
        throw new RuntimeException("Unable to run quorum server ", ie);
    }
}
```

### startServerCnxnFactory（启动2181的服务监听）

> org.apache.zookeeper.server.quorum.QuorumPeer#startServerCnxnFactory

```java
private void startServerCnxnFactory() {
    if (cnxnFactory != null) { // 不加密
        // 默认采用不加密; cnxnFactory对象是在上面main方法创建，默认创建NIOServerCnxnFactory类
        cnxnFactory.start(); 
    }
    if (secureCnxnFactory != null) { // 加密 ssl
        secureCnxnFactory.start();
    }
}
```

> org.apache.zookeeper.server.NIOServerCnxnFactory#start

```java
@Override
public void start() {
    stopped = false;
    if (workerPool == null) {
        // 创建工作线程池
        workerPool = new WorkerService("NIOWorker", numWorkerThreads, false);
    }
    for (SelectorThread thread : selectorThreads) {
        if (thread.getState() == Thread.State.NEW) { // 一开始启动默认都是 Thread.State.NEW 状态
            thread.start();
        }
    }
    // ensure thread is started once and only once
    // 确保线程只启动一次
    if (acceptThread.getState() == Thread.State.NEW) {
        acceptThread.start(); // 接收线程
    }
    if (expirerThread.getState() == Thread.State.NEW) {
        expirerThread.start(); // 连接终止线程
    }
}
```

### startLeaderElection（leader选举）

**选举流程：**

![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201202162956.jpg)

```java
public synchronized void startLeaderElection() {
    try {
        if (getPeerState() == ServerState.LOOKING) { // 刚刚启动状态 looking
            // 当前自己的票据是currentVote
            currentVote = new Vote(myid, getLastLoggedZxid(), getCurrentEpoch());
        }
    } catch (IOException e) {
        RuntimeException re = new RuntimeException(e.getMessage());
        re.setStackTrace(e.getStackTrace());
        throw re;
    }
    // 创建选举算法
    this.electionAlg = createElectionAlgorithm(electionType);
}

@SuppressWarnings("deprecation")
protected Election createElectionAlgorithm(int electionAlgorithm) {
    Election le = null;

    //TODO: use a factory rather than a switch
    switch (electionAlgorithm) {
        case 1:
            throw new UnsupportedOperationException("Election Algorithm 1 is not supported.");
        case 2:
            throw new UnsupportedOperationException("Election Algorithm 2 is not supported.");
        case 3:
            // cnxn (和网络通信有关的一个类,ServerCnxn， ClientCnxn)
            // QuorumCnxManager 管理集群选举和投票相关的操作
            QuorumCnxManager qcm = createCnxnManager();
            QuorumCnxManager oldQcm = qcmRef.getAndSet(qcm);
            if (oldQcm != null) { // 判断是否已经开启了选举
                LOG.warn("Clobbering already-set QuorumCnxManager (restarting leader election?)");
                oldQcm.halt(); // 终止掉当前的选举
            }
            // 监听集群中的票据
            QuorumCnxManager.Listener listener = qcm.listener;
            if (listener != null) {
                listener.start();
                // 创建选举算法
                FastLeaderElection fle = new FastLeaderElection(this, qcm);
                fle.start(); // 启动leader选举
                le = fle;
            } else {
                LOG.error("Null listener when initializing cnx manager");
            }
            break;
        default:
            assert false;
    }
    return le;
}
```

#### (1)QuorumCnxManager.Listener (监听集群中的票据)

> QuorumCnxManager.Listener 它继承了Thread，就是一个线程
>
> ![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/zookeeper/20201202134329.jpg)

```java

public class Listener extends ZooKeeperThread {

    public Listener() {
        // 在线程启动期间，线程名称将被覆盖到特定的选举地址
        super("ListenerThread");

        socketException = new AtomicBoolean(false);

        // 在尝试绑定到选举端口时的最大重试计数
        final Integer maxRetry = Integer.getInteger(ELECTION_PORT_BIND_RETRY, DEFAULT_PORT_BIND_MAX_RETRY);
        if (maxRetry >= 0) {
            LOG.info("Election port bind maximum retries is {}", maxRetry == 0 ? "infinite" : maxRetry);
            portBindMaxRetry = maxRetry;
        } else {
            LOG.info(
                "'{}' contains invalid value: {}(must be >= 0). Use default value of {} instead.",
                ELECTION_PORT_BIND_RETRY,
                maxRetry,
                DEFAULT_PORT_BIND_MAX_RETRY);
            portBindMaxRetry = DEFAULT_PORT_BIND_MAX_RETRY;
        }
    }
```

listener.start() 方法后，执行线程run方法

```java
@Override
public void run() {
    if (!shutdown) {
        LOG.debug("Listener thread started, myId: {}", self.getId());
        Set<InetSocketAddress> addresses; // zookeeper选举端口
		// 是否监听全部端口
        if (self.getQuorumListenOnAllIPs()) {
            addresses = self.getElectionAddress().getWildcardAddresses();
        } else {
            addresses = self.getElectionAddress().getAllAddresses();
        }

        CountDownLatch latch = new CountDownLatch(addresses.size());
        // 遍历逐个地址，逐个地址生成ListenerHandler，组合成集合
        listenerHandlers = addresses.stream().map(address ->
			new ListenerHandler(address, 
                                self.shouldUsePortUnification(),
                                self.isSslQuorum(), 
                                latch))
            .collect(Collectors.toList());

        ExecutorService executor = Executors.newFixedThreadPool(addresses.size());
        // 遍历对象加入到 线程池；从这里可以看出ListenerHandler是一个线程
        listenerHandlers.forEach(executor::submit);

        try {
            latch.await(); // 等待所有监听线程启动完
        } catch (InterruptedException ie) {
            LOG.error("Interrupted while sleeping. Ignoring exception", ie);
        } finally {
            // Clean up for shutdown.
            for (ListenerHandler handler : listenerHandlers) {
                try {
                    handler.close();
                } catch (IOException ie) {
                    // Don't log an error for shutdown.
                    LOG.debug("Error closing server socket", ie);
                }
            }
        }
    }
```

##### ListenerHandler

```java
class ListenerHandler implements Runnable, Closeable {
    /**
    * Sleeps on acceptConnections().
    */
    @Override
    public void run() {
        try {
            Thread.currentThread().setName("ListenerHandler-" + address);
            acceptConnections(); // 接收选举端口连接
            try {
                close();
            } catch (IOException e) {
                LOG.warn("Exception when shutting down listener: ", e);
            }
        } catch (Exception e) {
            // Output of unexpected exception, should never happen
            LOG.error("Unexpected error ", e);
        } finally {
            latch.countDown();
        }
    }
    
    
    /**
    * Sleeps on accept().  接收选举端口连接
    */
    private void acceptConnections() {
        int numRetries = 0;
        Socket client = null;

        while ((!shutdown) && (portBindMaxRetry == 0 || numRetries < portBindMaxRetry)) {
            try {
                serverSocket = createNewServerSocket();
                LOG.info("{} is accepting connections now, my election bind port: {}", QuorumCnxManager.this.mySid, address.toString());
                while (!shutdown) {
                    try {
                        // 接收请求
                        client = serverSocket.accept();
                        setSockOpts(client);
                        LOG.info("Received connection request from {}", client.getRemoteSocketAddress());
                        // Receive and handle the connection request
                        // asynchronously if the quorum sasl authentication is
                        // enabled. This is required because sasl server
                        // authentication process may take few seconds to finish,
                        // this may delay next peer connection requests.
                        if (quorumSaslAuthEnabled) {
                            receiveConnectionAsync(client); // 异步接收连接
                        } else {
                            receiveConnection(client); // 同步接收连接
                        }
                        numRetries = 0;
                    } catch (SocketTimeoutException e) {
                        LOG.warn("The socket is listening for the election accepted "
                                 + "and it timed out unexpectedly, but will retry."
                                 + "see ZOOKEEPER-2836");
                    }
                }
            } catch (IOException e) {
                if (shutdown) {
                    break;
                }

                LOG.error("Exception while listening", e);

                if (e instanceof SocketException) {
                    socketException.set(true);
                }

                numRetries++; // 记录重试次数
                try {
                    close();
                    Thread.sleep(1000);
                } catch (IOException ie) {
                    LOG.error("Error closing server socket", ie);
                } catch (InterruptedException ie) {
                    LOG.error("Interrupted while sleeping. Ignoring exception", ie);
                }
                closeSocket(client);
            }
        }
        if (!shutdown) {
            LOG.error(
                "Leaving listener thread for address {} after {} errors. Use {} property to increase retry count.",
                formatInetAddr(address),
                numRetries,
                ELECTION_PORT_BIND_RETRY);
        }
    }
}
```

```java
// 接收请求
public void receiveConnection(final Socket sock) {
    DataInputStream din = null;
    try {
        din = new DataInputStream(new BufferedInputStream(sock.getInputStream()));

        LOG.debug("Sync handling of connection request received from: {}", sock.getRemoteSocketAddress());
        handleConnection(sock, din);
    } catch (IOException e) {
        LOG.error("Exception handling connection, addr: {}, closing server connection", sock.getRemoteSocketAddress());
        LOG.debug("Exception details: ", e);
        closeSocket(sock);
    }
}
```

```java
/**
 * 处理请求
 * @param sock
 * @param din 接收的数据流
 */
private void handleConnection(Socket sock, DataInputStream din) throws IOException {
    Long sid = null, protocolVersion = null;
    MultipleAddresses electionAddr = null;

    try {
        protocolVersion = din.readLong();
        if (protocolVersion >= 0) { // 这是一个服务器id，而不是协议版本
            sid = protocolVersion;
        } else {
            try {
                InitialMessage init = InitialMessage.parse(protocolVersion, din);
                sid = init.sid;
                if (!init.electionAddr.isEmpty()) {
                    electionAddr = new MultipleAddresses(init.electionAddr,
                                                         Duration.ofMillis(self.getMultiAddressReachabilityCheckTimeoutMs()));
                }
                LOG.debug("Initial message parsed by {}: {}", self.getId(), init.toString());
            } catch (InitialMessage.InitialMessageException ex) {
                LOG.error("Initial message parsing error!", ex);
                closeSocket(sock);
                return;
            }
        }

        if (sid == QuorumPeer.OBSERVER_ID) {
            /*
            * Choose identifier at random. We need a value to identify
            * the connection.
            */
            sid = observerCounter.getAndDecrement();
            LOG.info("Setting arbitrary identifier to observer: {}", sid);
        }
    } catch (IOException e) {
        LOG.warn("Exception reading or writing challenge", e);
        closeSocket(sock);
        return;
    }

    // do authenticating learner
    authServer.authenticate(sock, din);
    //If wins the challenge, then close the new connection.
    if (sid < self.getId()) {
        /*
             * This replica might still believe that the connection to sid is
             * up, so we have to shut down the workers before trying to open a
             * new connection.
             */
        SendWorker sw = senderWorkerMap.get(sid);
        if (sw != null) {
            sw.finish();
        }

        /*
             * Now we start a new connection
             */
        LOG.debug("Create new connection to server: {}", sid);
        closeSocket(sock);

        if (electionAddr != null) {
            connectOne(sid, electionAddr);
        } else {
            connectOne(sid); //连接到指定的sid对应的服务
        }

    } else if (sid == self.getId()) {
        // we saw this case in ZOOKEEPER-2164
        LOG.warn("We got a connection request from a server with our own ID. "
                 + "This should be either a configuration error, or a bug.");
    } else { // Otherwise start worker threads to receive data.
        SendWorker sw = new SendWorker(sock, sid); // leader选举端口 发送处理
        RecvWorker rw = new RecvWorker(sock, din, sid, sw); // leader选举端口 接收处理
        sw.setRecv(rw);

        SendWorker vsw = senderWorkerMap.get(sid);

        if (vsw != null) {
            vsw.finish();
        }

        senderWorkerMap.put(sid, sw);

        queueSendMap.putIfAbsent(sid, new CircularBlockingQueue<>(SEND_CAPACITY));
		 // 启动 发送||接收线程
        sw.start();
        rw.start();
    }
}
```



#### (2)FastLeaderElection (创建选举算法)

```java
public FastLeaderElection(QuorumPeer self, QuorumCnxManager manager) {
    this.stop = false;
    this.manager = manager;
    starter(self, manager);
}

private void starter(QuorumPeer self, QuorumCnxManager manager) {
    this.self = self;
    proposedLeader = -1;
    proposedZxid = -1;
	// 构建发送队列
    sendqueue = new LinkedBlockingQueue<ToSend>();
    // 构建接收队列
    recvqueue = new LinkedBlockingQueue<Notification>();
    this.messenger = new Messenger(manager);
}


Messenger(QuorumCnxManager manager) {
	
    this.ws = new WorkerSender(manager);

    this.wsThread = new Thread(this.ws, "WorkerSender[myid=" + self.getId() + "]");
    this.wsThread.setDaemon(true);

    this.wr = new WorkerReceiver(manager);

    this.wrThread = new Thread(this.wr, "WorkerReceiver[myid=" + self.getId() + "]");
    this.wrThread.setDaemon(true);
}

```

#### FastLeaderElection.start

```java
public void start() {
    this.messenger.start();
}
```

#### messenger.start

> org.apache.zookeeper.server.quorum.FastLeaderElection.Messenger#start

```java
void start() {
    //启动两个线程
    /**
    * * WorkerSender      ->发送票据
    * * WorkerReceiver    ->接收票据
    */
    this.wsThread.start();
    this.wrThread.start();
}
```

#### WorkerSender.run (发送票据)

```java
class WorkerSender extends ZooKeeperThread {

    volatile boolean stop;
    QuorumCnxManager manager;

    WorkerSender(QuorumCnxManager manager) {
        super("WorkerSender");
        this.stop = false;
        this.manager = manager;
    }

    public void run() {
        while (!stop) {
            try {
                // 阻塞式从阻塞队列获取需要发送的任务
                ToSend m = sendqueue.poll(3000, TimeUnit.MILLISECONDS);
                if (m == null) {
                    continue;
                }

                process(m);
            } catch (InterruptedException e) {
                break;
            }
        }
        LOG.info("WorkerSender is down");
    }

    /**
    * Called by run() once there is a new message to send.
    *
    * @param m     message to send
    */
    void process(ToSend m) {
        // 构建消息
        ByteBuffer requestBuffer = buildMsg(m.state.ordinal(), 
                                            m.leader, 
                                            m.zxid, 
                                            m.electionEpoch, 
                                            m.peerEpoch, 
                                            m.configData);
		// 发送消息
        manager.toSend(m.sid, requestBuffer);

    }

}
```

> org.apache.zookeeper.server.quorum.QuorumCnxManager#toSend

```java
public void toSend(Long sid, ByteBuffer b) {
    /*
    * If sending message to myself, then simply enqueue it (loopback).
    */
    if (this.mySid == sid) { //如果toSend这个消息发送到自己的节点
        b.position(0);
        addToRecvQueue(new Message(b.duplicate(), sid)); //直接添加到接收队列
        /*
        * Otherwise send to the corresponding thread to send.
        */
    } else {
        /*
        * Start a new connection if doesn't have one already.
        */
        BlockingQueue<ByteBuffer> bq = queueSendMap.computeIfAbsent(sid, serverId -> 
                                         new CircularBlockingQueue<>(SEND_CAPACITY));
        addToSendQueue(bq, b); // 添加到发送队列  queueSendMap 根据sid区分发送队列
        connectOne(sid);
    }
}
```

#### SendWorker.run (发送票据，在queueSendMap队列拿数据，然后进行发送)

```java
@Override
public void run() {
    threadCnt.incrementAndGet();
    try {
        /**
        * If there is nothing in the queue to send, then we
        * send the lastMessage to ensure that the last message
        * was received by the peer. The message could be dropped
        * in case self or the peer shutdown their connection
        * (and exit the thread) prior to reading/processing
        * the last message. Duplicate messages are handled correctly
        * by the peer.
        *
        * If the send queue is non-empty, then we have a recent
        * message than that stored in lastMessage. To avoid sending
        * stale message, we should send the message in the send queue.
        */
        // 在queueSendMap 根据sid获取指定发送消息队列
        BlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);
        if (bq == null || isSendQueueEmpty(bq)) {
            ByteBuffer b = lastMessageSent.get(sid);
            if (b != null) {
                LOG.debug("Attempting to send lastMessage to sid={}", sid);
                send(b);
            }
        }
    } catch (IOException e) {
        LOG.error("Failed to send last message. Shutting down thread.", e);
        this.finish();
    }
    LOG.debug("SendWorker thread started towards {}. myId: {}", sid, QuorumCnxManager.this.mySid);

    try {
        while (running && !shutdown && sock != null) {

            ByteBuffer b = null;
            try {
                // 在queueSendMap 根据sid获取指定发送消息队列
                BlockingQueue<ByteBuffer> bq = queueSendMap.get(sid);
                if (bq != null) { 
                    // 指定时间内拿出一个消息对象
                    b = pollSendQueue(bq, 1000, TimeUnit.MILLISECONDS);
                } else {
                    LOG.error("No queue of incoming messages for server {}", sid);
                    break;
                }

                if (b != null) {
                    lastMessageSent.put(sid, b);
                    send(b); // 发送消息
                }
            } catch (InterruptedException e) {
                LOG.warn("Interrupted while waiting for message on queue", e);
            }
        }
    } catch (Exception e) {
        LOG.warn("Exception when using channel: for id {} my id = {}",
            sid ,
            QuorumCnxManager.this.mySid,
            e);
    }
    this.finish();

    LOG.warn("Send worker leaving thread id {} my id = {}", sid, self.getId());
}


synchronized void send(ByteBuffer b) throws IOException {
    byte[] msgBytes = new byte[b.capacity()];
    try {
        b.position(0);
        b.get(msgBytes);
    } catch (BufferUnderflowException be) {
        LOG.error("BufferUnderflowException ", be);
        return;
    }
    // socket 发送
    dout.writeInt(b.capacity());
    dout.write(b.array());
    dout.flush();
}
```



#### RecvWorker.run (接收票据，接收到指定消息存储到 recvQueue 队列)

```java
// 接收票据
class RecvWorker extends ZooKeeperThread {

    Long sid;
    Socket sock;
    volatile boolean running = true;
    final DataInputStream din;
    final SendWorker sw;

    RecvWorker(Socket sock, DataInputStream din, Long sid, SendWorker sw) {
        super("RecvWorker:" + sid);
        this.sid = sid;
        this.sock = sock;
        this.sw = sw;
        this.din = din;
        try {
            // OK to wait until socket disconnects while reading.
            sock.setSoTimeout(0);
        } catch (IOException e) {
            LOG.error("Error while accessing socket for {}", sid, e);
            closeSocket(sock);
            running = false;
        }
    }

    /**
    * Shuts down this worker
    *
    * @return boolean  Value of variable running
    */
    synchronized boolean finish() {
        LOG.debug("RecvWorker.finish called. sid: {}. myId: {}", sid, QuorumCnxManager.this.mySid);
        if (!running) {
            /*
            * Avoids running finish() twice.
            * 避免跑两次finish()。
            */
            return running;
        }
        running = false;

        this.interrupt();
        threadCnt.decrementAndGet();
        return running;
    }

    @Override
    public void run() {
        threadCnt.incrementAndGet();
        try {
            LOG.debug("RecvWorker thread towards {} started. myId: {}", sid, QuorumCnxManager.this.mySid);
            while (running && !shutdown && sock != null) {
                /**
                * Reads the first int to determine the length of the
                * message
                */
                int length = din.readInt();
                if (length <= 0 || length > PACKETMAXSIZE) {
                    throw new IOException("Received packet with invalid packet: " + length);
                }
                /**
                * 分配一个新的ByteBuffer来接收消息
                */
                final byte[] msgArray = new byte[length];
                din.readFully(msgArray, 0, length); // 将消息读取到msgArray数组中
                addToRecvQueue(new Message(ByteBuffer.wrap(msgArray), sid)); // 存储recvQueue队列
            }
        } catch (Exception e) {
            LOG.warn(
                "Connection broken for id {}, my id = {}",
                sid,
                QuorumCnxManager.this.mySid,
                e);
        } finally {
            LOG.warn("Interrupting SendWorker thread from RecvWorker. sid: {}. myId: {}", sid, QuorumCnxManager.this.mySid);
            sw.finish();
            closeSocket(sock);
        }
    }

}
```



#### WorkerReceiver.run (接收票据，从 recvQueue 队列获取票据)

```java
public void run() {

    Message response;
    while (!stop) {
        // Sleeps on receive
        try {
            // 从recvQueue 队列中读取消息
            response = manager.pollRecvQueue(3000, TimeUnit.MILLISECONDS);
            if (response == null) {
                continue;
            }
			// 获取消息长度
            final int capacity = response.buffer.capacity();

            // The current protocol and two previous generations all send at least 28 bytes
            if (capacity < 28) {
                LOG.error("Got a short response from server {}: {}", response.sid, capacity);
                continue;
            }

            // this is the backwardCompatibility mode in place before ZK-107
            // It is for a version of the protocol in which we didn't send peer epoch
            // With peer epoch and version the message became 40 bytes
            boolean backCompatibility28 = (capacity == 28);

            // this is the backwardCompatibility mode for no version information
            boolean backCompatibility40 = (capacity == 40);

            response.buffer.clear();

            // Instantiate Notification and set its attributes
            Notification n = new Notification();

            int rstate = response.buffer.getInt(); // 远程服务器状态
            long rleader = response.buffer.getLong(); // 远程服务器 leader 状态
            long rzxid = response.buffer.getLong(); // 远程服务器的 zxid
            long relectionEpoch = response.buffer.getLong(); // 远程服务器的 epoch
            long rpeerepoch;

            int version = 0x0;
            QuorumVerifier rqv = null;

            try {
                if (!backCompatibility28) {
                    rpeerepoch = response.buffer.getLong();
                    if (!backCompatibility40) {
                        /*
                        * Version added in 3.4.6
                        */
                        version = response.buffer.getInt();
                    } else {
                        LOG.info("Backward compatibility mode (36 bits), server id: {}", response.sid);
                    }
                } else {
                    LOG.info("Backward compatibility mode (28 bits), server id: {}", response.sid);
                    rpeerepoch = ZxidUtils.getEpochFromZxid(rzxid);
                }

                // check if we have a version that includes config. If so extract config info from message.
                if (version > 0x1) {
                    int configLength = response.buffer.getInt();

                    // we want to avoid errors caused by the allocation of a byte array with negative length
                    // (causing NegativeArraySizeException) or huge length (causing e.g. OutOfMemoryError)
                    if (configLength < 0 || configLength > capacity) {
                        throw new IOException(String.format("Invalid configLength in notification message! sid=%d, capacity=%d, version=%d, configLength=%d",
                                                            response.sid, capacity, version, configLength));
                    }

                    byte[] b = new byte[configLength];
                    response.buffer.get(b);

                    synchronized (self) {
                        try {
                            rqv = self.configFromString(new String(b));
                            QuorumVerifier curQV = self.getQuorumVerifier();
                            if (rqv.getVersion() > curQV.getVersion()) {
                                LOG.info("{} Received version: {} my version: {}",
                                         self.getId(),
                                         Long.toHexString(rqv.getVersion()),
                                         Long.toHexString(self.getQuorumVerifier().getVersion()));
                                if (self.getPeerState() == ServerState.LOOKING) {
                                    LOG.debug("Invoking processReconfig(), state: {}", self.getServerState());
                                    self.processReconfig(rqv, null, null, false);
                                    if (!rqv.equals(curQV)) {
                                        LOG.info("restarting leader election");
                                        self.shuttingDownLE = true;
                                        self.getElectionAlg().shutdown();

                                        break;
                                    }
                                } else {
                                    LOG.debug("Skip processReconfig(), state: {}", self.getServerState());
                                }
                            }
                        } catch (IOException | ConfigException e) {
                            LOG.error("Something went wrong while processing config received from {}", response.sid);
                        }
                    }
                } else {
                    LOG.info("Backward compatibility mode (before reconfig), server id: {}", response.sid);
                }
            } catch (BufferUnderflowException | IOException e) {
                LOG.warn("Skipping the processing of a partial / malformed response message sent by sid={} (message length: {})",
                         response.sid, capacity, e);
                continue;
            }
            /*
            * 如果它来自一个没有投票的服务器(比如一个观察者或者一个没有投票的关注者)，请立即响应
            */
            if (!validVoter(response.sid)) {
                Vote current = self.getCurrentVote();
                QuorumVerifier qv = self.getQuorumVerifier();
                ToSend notmsg = new ToSend(
                    ToSend.mType.notification,
                    current.getId(),
                    current.getZxid(),
                    logicalclock.get(),
                    self.getPeerState(),
                    response.sid,
                    current.getPeerEpoch(),
                    qv.toString().getBytes());

                sendqueue.offer(notmsg);
            } else {
                // Receive new message
                LOG.debug("Receive new notification message. My id = {}", self.getId());

                // State of peer that sent this message
                QuorumPeer.ServerState ackstate = QuorumPeer.ServerState.LOOKING;
                switch (rstate) {
                    case 0:
                        ackstate = QuorumPeer.ServerState.LOOKING;
                        break;
                    case 1:
                        ackstate = QuorumPeer.ServerState.FOLLOWING;
                        break;
                    case 2:
                        ackstate = QuorumPeer.ServerState.LEADING;
                        break;
                    case 3:
                        ackstate = QuorumPeer.ServerState.OBSERVING;
                        break;
                    default:
                        continue;
                }

                n.leader = rleader;
                n.zxid = rzxid;
                n.electionEpoch = relectionEpoch;
                n.state = ackstate;
                n.sid = response.sid;
                n.peerEpoch = rpeerepoch;
                n.version = version;
                n.qv = rqv;
                /*
                             * Print notification info
                             */
                LOG.info(
                    "Notification: my state:{}; n.sid:{}, n.state:{}, n.leader:{}, n.round:0x{}, "
                    + "n.peerEpoch:0x{}, n.zxid:0x{}, message format version:0x{}, n.config version:0x{}",
                    self.getPeerState(),
                    n.sid,
                    n.state,
                    n.leader,
                    Long.toHexString(n.electionEpoch),
                    Long.toHexString(n.peerEpoch),
                    Long.toHexString(n.zxid),
                    Long.toHexString(n.version),
                    (n.qv != null ? (Long.toHexString(n.qv.getVersion())) : "0"));

                /*
                             * If this server is looking, then send proposed leader
                             */

                if (self.getPeerState() == QuorumPeer.ServerState.LOOKING) {
                    recvqueue.offer(n);

                    /*
                                 * Send a notification back if the peer that sent this
                                 * message is also looking and its logical clock is
                                 * lagging behind.
                                 */
                    if ((ackstate == QuorumPeer.ServerState.LOOKING)
                        && (n.electionEpoch < logicalclock.get())) {
                        Vote v = getVote();
                        QuorumVerifier qv = self.getQuorumVerifier();
                        ToSend notmsg = new ToSend(
                            ToSend.mType.notification,
                            v.getId(),
                            v.getZxid(),
                            logicalclock.get(),
                            self.getPeerState(),
                            response.sid,
                            v.getPeerEpoch(),
                            qv.toString().getBytes());
                        sendqueue.offer(notmsg);
                    }
                } else {
                    /*
                                 * If this server is not looking, but the one that sent the ack
                                 * is looking, then send back what it believes to be the leader.
                                 */
                    Vote current = self.getCurrentVote();
                    if (ackstate == QuorumPeer.ServerState.LOOKING) {
                        if (self.leader != null) {
                            if (leadingVoteSet != null) {
                                self.leader.setLeadingVoteSet(leadingVoteSet);
                                leadingVoteSet = null;
                            }
                            self.leader.reportLookingSid(response.sid);
                        }


                        LOG.debug(
                            "Sending new notification. My id ={} recipient={} zxid=0x{} leader={} config version = {}",
                            self.getId(),
                            response.sid,
                            Long.toHexString(current.getZxid()),
                            current.getId(),
                            Long.toHexString(self.getQuorumVerifier().getVersion()));

                        QuorumVerifier qv = self.getQuorumVerifier();
                        ToSend notmsg = new ToSend(
                            ToSend.mType.notification,
                            current.getId(),
                            current.getZxid(),
                            current.getElectionEpoch(),
                            self.getPeerState(),
                            response.sid,
                            current.getPeerEpoch(),
                            qv.toString().getBytes());
                        sendqueue.offer(notmsg);
                    }
                }
            }
        } catch (InterruptedException e) {
            LOG.warn("Interrupted Exception while waiting for new message", e);
        }
    }
    LOG.info("WorkerReceiver is down");
}
```

