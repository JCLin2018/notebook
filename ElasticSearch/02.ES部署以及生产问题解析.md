# ES的部署以及集群配置

## 系统环境准备

首先，centOs上去，确认网卡

```sh
vi /etc/sysconfig/network-scripts/ifcfg-eth0
```

> ONBOOT=yes
> ONBOOT是指明在系统启动时是否激活网卡，只有在激活状态的网卡才能去连接网络，进行网络通讯 其中eth0是设备名,可能是en33

重启生效

```sh
service network restart
```

`ping www.baidu.com` 接收到返回结果则成功。

### 1.确认系统内存

要是在生产环境下确定你的内存在16G以上 ，因为我们的ES7.X默认需要这么大的内存.

> 参考书籍:[Elasticsearch: 权威指南]节选 https://www.elastic.co/guide/cn/elasticsearch/guide/current/hardware.html
>
> 上文是基于ES2.x版本,但是7.X版本与2.X版本内存要求没有明显变化.

### 2.确定防火墙

> 如果服务器涉及到一些端口开放，特别是要自动连接无限制端口，那么我们就得清除Linux的防火墙。
>
> 但是绝大多数的Linux服务器，默认的防火墙是有OUTPUT策略。 输入命令:
>
> ```sh
> iptables -L Chain OUTPUT (policy ACCEPT)
> ```
>
> 如果有这个策略在，那么就应该先改掉这个。如果不改掉，就清空防火墙，那么你服务器就打不开了，需要重新配置网络才行。
>
> 把默认策略改成ACCEPT。`iptables -P INPUT ACCEPT`
>
> 如果不确定就再看看现在的服务器配置，里面是否有这个信息: 
>
> `Chain FORWARD (policy ACCEPT 0 packets, 0 bytes)` 
>
> 如果有上面这句，那么就没问题了，然后再去清除防火墙。 
>
> 输入命令:
> `/sbin/iptables -F`
> 或 `iptables -F` 删除防火墙规则 
>
> 现在防火墙就关闭清空了，端口任意连接了。

```sh
# 查询防火墙规则列表
iptables -L
# 把默认策略改成ACCEPT 
iptables -P INPUT ACCEPT
# 删除防火墙规则
iptables -F
# 下面有详细描述
iptables -I INPUT -s 192.168.0.0 -j ACCEPT
```

> 删除防火墙规则
>
> iptables设置句，-A INPUT只在INPUT链中插入一条规则，-s匹配源地址，这里的0/0指可以是任 何地址，-i指定网络接口 -d匹配目的地址 -p匹配协议类型，-j指定要采取的操作，这里ACCEPT表 示允许连接， 
>
> 这句语句的意思就是配置网络接口eth0允许来自任何地址的目的地址是192.168.0.0的TCP数据包联机。

### 3.SEliunx

首先,SELinux策略是白名单原则，所以你需要非常清楚你的各项操作都需要哪些访问权限，这个好像数量有点多了,不是专业的运维这东西确实有点麻烦.

- getenforce命令

  > 这个是查看当前SELinux的运行模式的指令
  >
  > SEliunx有三种模式. 分别是:
  >
  > - **Enforcing**: 强制模式。代表SELinux在运行中，且已经开始限制domain/type之间的验证关系
  > - **Permissive**: 宽容模式。代表SELinux在运行中，不过不会限制domain/type之间的验证关系，即使验证不正确，进程仍可以对文件进行操作。不过如果验证不正确会发出警告
  > - **Disabled**: 关闭模式。SELinux并没有实际运行我们这里需要关闭.

SEliunx模式快捷转换

```sh
# 转换为Permissive宽容模式 
setenforce 0
# 转换为Enforcing强制模式 
setenforce 1
```

- **注意事项**: setenforce无法设置SELinux为Disabled模式

演示:

如何关闭SEliunx 

> 我们既然要永久关闭selinux，只能通过修改配置文件。
>
> 因为selinux开机直接被内核整合，所以selinux没有提供服务接口，也就是说你在 `/etc/init.d` 里是找不到selinux的服务的。
>
> selinux的配置文件是**/etc/selinux/config**
>
> `vim /etc/selinux/config` 打开selinux配置文件 
>
> 打开后按i插入
> 修改参数部分
> SELINUX=参数 
> 参数可选(enforcing、permissive、disabled) 
> 输入disabled保存 
> 直接查看运行状态或者重启reboot

```sh
#打开selinux配置文件
vim /etc/selinux/config 
修改配置文件，SELINUX=参数 
参数可选(enforcing、permissive、disabled) 
输入disabled保存
重启命令，生产环境禁止使用
reboot
```

### 4.时区

确认时区:

```sh
date
```

如果时区是EST或者其他，要修改为CST(中国时区) 

`cp /usr/share/zoneinfo/Asia/Shanghai /etc/localtime`

### 5.检查原有的是否自带openJDK

java -version判断centOS是否自带JDK 

自带JDK则需要删除

```sh
rpm -qa | grep java # 这些都是自带的java文件
# 表示删除你所找到的openJDK，请注意。上面JDK版本与你自带的JDK版本对应
rpm -e --nodeps java-1.8.0-openjdk-1.8.0.111-2.6.7.8.el7.x86_64
```

 执行 rpm -qa | grep java 无文件，表示删除干净。

### 6.然后安装你需要安装的JDK版本

解压

```sh
tar  -zxvf  java-1.8.0-openjdk-1.8.0.111-2.6.7.8.el7.x86_64
```

配置环境变量JAVA_HOME

```sh
vi /etc/profile
# 在/etc/profile下添加如下内容:
export JAVA_HOME=/jdk/jdk1.8.0_261
export PATH=$JAVA_HOME/bin:$PATH
export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar
```

使环境变量生效

```sh
source    /etc/profile
```

查看环境变量

```sh
# 查看我们配置的环境变量是否生效
echo $PATH
```

测试JDK是否生效

```sh
java -version
```

### 7.配置操作系统

> 注意:这里的话，需要使用root权限，不然所有的用户都需要单独设置一遍。

- 进程线程数与文件句柄数

  找到limits.conf文件

  ```sh
  vi /etc/security/limits.conf
  ```

  进入文件，在文件末尾增加

  ![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/Elasticsearch/20201220142725.png)

  ```sh
  *        soft   nproc   131072
  *        hard   nproc   131072
  *        soft   nofile  131072
  *        hard   nofile  131072
  ```

  > 设置限制数量，第一列表示用户，* 表示所有用户
  > soft xxx: 代表警告的设定，可以超过这个设定值，但是超过后会有警告。 
  > hard xxx: 代表严格的设定，不允许超过这个设定的值。
  >
  > nproc: 是操作系统级别对每个用户创建的进程数的限制
  > nofile: 是每个进程可以打开的文件数的限制
  >
  > soft nproc: 单个用户可用的最大进程数量(超过会警告); 
  > hard nproc: 单个用户可用的最大进程数量(超过会报错); 
  >
  > soft nofile: 可打开的文件描述符的最大数(超过会警告); 
  > hard nofile: 可打开的文件描述符的最大数(超过会报错);
  >
  > 举例:soft 设为1024，hard设为2048 ，则当你使用数在1~1024之间时可以随便使用， 1024~2048时会出现警告信息，大于2048时，就会报错。
  >
  > 注:
  >
  > 1. 一般soft的值会比hard小，也可相等。
  > 2. /etc/security/limits.d/ 里面配置会覆盖 /etc/security/limits.conf 的配置
  > 3. 只有root用户才有权限修改/etc/security/limits.conf
  > 4. 如果limits.conf没有做设定，则默认值是1024

### 8.虚拟内存

```sh
vi /etc/sysctl.conf
```

进行设置

```sh
#在文件末尾进行设置
vm.swappiness=1
vm.max_map_count=262144
```

验证：

```sh
sysctl vm.max_map_count
```

如果未生效，使用sysctl -p重新加载配置文件，看文件是否有误，不指定默认加载sysctl.conf

```sh
[root@localhost /]# sysctl -p
vm.max_map_count = 262144
```

> es使用hybrid max mapfs / niofs目录来存储index数据，操作系统的默认mmap count限制是很低的，可能会导致内存耗尽的异常。
>
> 需要提升max map count的限制:sysctl -w vm.max_map_count=262144 
>
> 如果要永久性设置这个值，要修改/etc/sysctl.conf，将vm.max_map_count的值修改一下，重启过后，用sysctl vm.max_map_count来验证一下数值是否修改成功
>
> es同时会用NioFS和MMapFS来处理不同的文件，我们需要设置最大的map count，这样我们才能有足够的虚拟内存来给mmapped文件使用，可以用sysctl来设置:sysctl -w vm.max_map_count=262144。还可以在/etc/sysctl.conf中，对vm.max_map_count来设置。

**虚拟内存另一个问题**

> 通常来说，es进程会在一个节点上单独运行，那么es进程的内存使用是由jvm option控制的。 
>
> swap(交换分区)主要是在内存不够用的时候，将部分内存上的数据交换到swap空间上，以便让系统不会因内存不够用而导致oom或者更致命的情况出现。

>我们思考一下如果频繁的将es进程的内存swap到磁盘上，绝对会是一个服务器的性能杀手。想象 一下，内存中的操作都是要求快速完成的，如果需要将内存页的数据从磁盘swap回main memory的话，性能会有多差。如果内存被swap到了磁盘，那么100微秒的操作会瞬间变成10毫秒，那么如果是大量的这种内存操作呢?这会导致性能急剧下降。
>
>可以使用下面的命令临时性禁止swap:swapoff -a
>
>要永久性的禁止swap，需要修改/etc/fstab文件，然后将所有包含swap的行都注释掉

关闭虚拟内存配置

```sh
# 禁用内存和硬盘交换 0:完全禁用 1:在紧急情况下才会swap
# vm.swappiness=1
# 设置虚拟内存大小
vm.max_map_count = 262144
```

> 另外一个方法就是通过sysctl，将vm.swappiness设置为1，这可以尽量减少linux内核swap的倾向，在正常情况下，就不会进行swap，但是在紧急情况下，还是会进行swap操作。 也就是 `vm.swappiness=1` ，如果你希望完全不会swap。那我们还是需要配置swappiness值为0，那么内存在free和file-backed使用的页面总量小于高水位标记(high water mark也就是我们设置的 值)之前，不会发生交换。

## 开始部署ES

首先，在你的运行环境下创建一个安装目录

1.`mkdir /es` ，创建一个名为es的文件夹

2.下载安装包并且上传到指定目录

- 访问elasticSearch官网地址 https://www.elastic.co/ 
- 下载指定版本的安装包:elasticsearch-7.9.1.tar.gz 

3.解压安装包至es包下
 tar -zxvf elasticsearch-7.9.1.tar.gz

> 如果解压报错,请注意,可能是文件类型不对,可以file elasticsearch-7.9.1.tar.gz 查看一下文件类型再或者不是gzip格式,可以不加z .直接tar -xvf elasticsearch-7.9.1.tar.gz
>
> 安装包目录:
>
> - bin: 存放es的一些可执行脚本，比如用于启动进程的elasticsearch命令，以及用于安装插件的 elasticsearch-plugin插件 
> - conf: 用于存放es的配置文件，比如elasticsearch.yml，这里面存放了很多我们相关的生产上会使用到的配置，比如预防我们集群脑裂，过度移动导致网络带宽被消耗等问题。还存放了我们 log4j的一些配置。
> - data: 用于存放es的数据文件，就是每个索引的shard的数据文件 
> - logs: 用于存放es的日志文件
> - plugins: 用于存放es的插件
> - script: 用于存放一些脚本文件
>
> 在这里我们可以测试启动一下ES,看我们的ES会出什么样的错误.

4.重新指定JDK

```sh
vi bin/elasticsearch
```

文件内容

```sh
#配置ES自带的jdk
export JAVA_HOME=/es/elasticsearch-7.9.1/jdk 
export PATH=$JAVA_HOME/bin:$PATH 
#添加jdk判断,注意，要带小引号。
if [ -x "$JAVA_HOME/bin/java" ]; then 
JAVA="/es/elasticsearch-7.9.1/jdk/bin/java" 
else
JAVA=`which java`
fi
```

5.创建用户

> 在启动elasticsearch之前，**ES在启动的时候是不允许使用root账户的**，所以我们要新建一个用户 es。

`useradd es`

6.更换权限

`chown -R es:es /es/elasticsearch/elasticsearch-7.9.1`

> 注意,需要在root权限下更换权限,而不是其他权限下.sudo相当于以系统管理员身份运行,而不需要 root密码.

7.切换用户
` su es`

8.启动
`./bin/elasticsearch-7.9.1 `

测试是否成功运行 

有可能因为没有logs文件夹报错。 

这个时候`mkdir logs`就可以启动了

```sh
jps 或者 
测试9200端口,能否获得响应
curl 127.0.0.1:9200 或者 netstat -ntplu
```

这个可以看到访问端口的占用情况。

9.绑定IP

我们这个时候去访问测试一下,会发现我们是测试不通的,因为我们这里没有做一件非常关键的事情,我们没有将ES的运行进程ip绑定到我们的这台机器上.

vi /es/elasticsearch-7.9.1/config/elasticsearch.yml

> elasticsearch的config文件夹里面有两个配置文件:elasticsearch.yml和logging.yml，第一个是es的基本配置文件，第二个是日志配置文件，es也是使用log4j来记录日志的，所以logging.yml里 的设置按普通log4j配置文件来设置就行了。

配置几个参数:

```sh
# 设置绑定的ip地址，可以是ipv4或ipv6的，默认为0.0.0.0
network.host: 192.168.88.118
# 是集群发现，以前版本的参数是discovery.zen.ping.unicast.hosts
discovery.seed_hosts: ["192.168.88.118:9300"] 
cluster.initial_master_nodes: ["192.168.88.118:9300"]
```

> `network.publish_host: 192.168.0.1`
> 设置其它节点和该节点交互的ip地址，如果不设置它会自动判断，值必须是个真实的ip地址。`network.host`这个参数是用来同时设置`bind_host`和`publish_host`两个参数。

> 你可以通过为 `cluster.initial_master_nodes` 参数设置一系列符合主节点条件的节点的主机名或 IP 地址来引导启动集群。你可以在命令行或elasticsearch.yml 中提供这些信息。你还需要配置发现子系统，这样节点就知道如何找到彼此。
>
> 如果未设置 `initial_master_nodes`，那么在启动新节点时会尝试发现已有的集群。如果节点找不到可以加入的集群，将定期记录警告消息。
>
> 当您第一次启动全新的Elasticsearch集群时，也就是新版本的ES时，会出现一个集群引导步骤，该步骤确定在第一次轮询中统计的合格节点集。在开发模式下，如果未配置发现设置，则此步骤由节点本身自动执行。由于此自动引导本质上是不安全的，因此当您在生产模式下启动全新集群时，必须明确列出符合主要条件的节点。使用该 `cluster.initial_master_nodes` 属性设置此列表 。

> elasticsearch.yml详细配置.
>
> ```sh
> # 配置elasticsearch的集群名称，默认是elasticsearch。建议修改成一个有意义的名称
> cluster.name: 
> 
> # 节点名，通常一台物理服务器就是一个节点，es会默认随机指定一个名字，建议指定一个有意义的名称，方便管理一个或多个节点组成一个cluster集群，集群是一个逻辑的概念，节点是物理概念
> node.name: 
> 
> # 设置配置文件的存储路径，tar或zip包安装默认在es根目录下的config文件夹，rpm安装默认在/etc/elasticsearch
> path.conf: 
> 
> # 设置索引数据的存储路径，默认是es根目录下的data文件夹，可以设置多个存储路径，用逗号隔开
> path.data:
> 
> # 设置日志文件的存储路径，默认是es根目录下的logs文件夹
> path.logs: 
> 
> # 设置插件的存放路径，默认是es根目录下的plugins文件夹
> path.plugins: 
> 
> # 设置为true可以锁住ES使用的内存，避免内存与swap分区交换数据。
> bootstrap.memory_lock: true 
> 
> # 设置绑定主机的ip地址，设置为0.0.0.0表示绑定任何ip，允许外网访问，生产环境建议设置为具体的ip。
> network.host:  
> 
> # 设置对外服务的http端口，默认为9200
> http.port: 9200
> 
> # 集群结点之间通信端口
> transport.tcp.port: 9300 
> 
> # 指定该节点是否有资格被选举成为master结点，默认是true，如果原来的master宕机会重新选举新的master
> node.master:
> 
> # 指定该节点是否存储索引数据，默认为true。
> node.data: 
> 
> # es7.x 之后新增的配置，节点发现
> discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"] 
> 
> # es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master 
> cluster.initial_master_nodes: ["node1", "node2","node3"] 
> 
> # 单机允许的最大存储结点数，通常单机启动一个结点建议设置为1，开发环境如果单机启动多个节点可设置大于1
> node.max_local_storage_nodes:
> ```

10.配置ES内存

```sh
vi   /es/elasticsearch-7.9.1/config/jvm.options
# 在配置文件中设置
-Xms8g
-Xmx8g 
根据生产情况设定。建议-Xms与-Xmx配置成一样，同时不要超过32G，一些文档说是30.5G 参考文献:https://www.elastic.co/cn/blog/a-heap-of-trouble
```

> 默认都是1g

11.现在就可以访问我们es的地址了 http://ip+9200

12.访问es心跳

http://ip+9200/_cat/health
单节点启动一般不用检测心跳，因为单节点启动一般访问端口就可以了

13.es后台启动

```sh
./bin/elasticsearch -d
```

我们不后台启动的话，退出es直接就挂掉了



## kibana部署

> 注意，版本需要对应，kibana需要跟ES版本对应。

创建属于kibana的文件夹

```sh
mkdir kibana
```

下载7.9.1版本的kibana 并解压

```sh
tar -zxvf kibana-7.9.1-linux-x86_64.tar.gz
```

修改配置文件

```sh
# vi /etc/kibana/kibana.yml
# 修改配置
server.port: 5601 # 这个不一定需要配置
server.host: "192.168.56.12" # 配置IP 
elasticsearch.hosts: ["http://192.168.56.15:9200"] 
i18n.locale: "zh-CN" # 默认是英文，这个是中文设置
```

启动kibana服务。

```sh
./bin/kibana
```

查看kibana的PID

```sh
ps -ef| grep node
```

查看界面:

http://ip+5601

ES并发机制 

部署完毕了之后，我们就可以看到我们想要的界面了。

## ES的并发控制

那么这个时候我们来思考一个问题，我们现在先打开一个淘宝网站，然后这个淘宝网站有非常多的商品，我们可以搜索一下书籍，然后点开第一个商品的详情页，然后我们发现有一个库存的属性，我们可以看见，我们是不是现在把库存购买完毕之后就没有了

总结：

- 乐观锁适用于写比较少的情况下(多读场景)，即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。 
- 悲观锁适用于读比较少的情况下(多写场景)，如果是多写的情况，一般会经常产生冲突，如果使用乐观锁，就会导致上层应用会不断的进行retry(重试)，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。

那么我们的ES是选择的哪种方式进行操作的呢? 

自然，我们的ES会选择我们的乐观锁的方式，因为我们的ES明显属于多读场景，而且一般多读场景的数据是不会需要频繁改动的

那么我们就来测一下，首先，我们现在可以构造一条数据

(1)先构造一条数据出来

```json
PUT /test_index/_doc/7
{
	"test_field": "mm数据1" 
}
```

(2)模拟两个客户端，反正我们的kibana可以打开多个客户端。假设他们都获取到了同一条数据

```json
GET /test_index/_doc/7
{
  "_index" : "test_index",
  "_type" : "_doc",
  "_id" : "7",
  "_version" : 2,
  "_seq_no" : 1,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
		"test_field" : "mm数据1" 
  }
}
```

(3)其中一个客户端，先更新了一下这个数据，不带上版本号，我们会发现，更新成功

```json
PUT /test_index/_doc/7
{
"test_field": "mm数据2" 
}
```

我们发现他仅仅修改了版本号以及一个叫_seq_no的东西，还有我们的filed。这个时候按照我们的常规逻辑，我们只需要新增一个版本号就万事大吉了，确保说，es中的数据的版本号，跟客户端中的数据的版本号是相同的，才能修改

(4)我们发送请求

```json
PUT /test_index/_doc/7?version=5
{
"test_filed":"mm数据3" 
}
// 结果我们发现出事了，报错了。

{
  "error" : {
      "root_cause" : [
        {
          "type" : "action_request_validation_exception",
          "reason" : "Validation Failed: 1: internal versioning can not be used for optimistic concurrency control. Please use `if_seq_no` and `if_primary_term` instead;" 
        }
      ],
      "type" : "action_request_validation_exception",
      "reason" : "Validation Failed: 1: internal versioning can not be used for optimistic concurrency control. Please use `if_seq_no` and `if_primary_term` instead;" 
  },
  "status" : 400
}
```

他说我们的版本号已经过时了

结果我们发现报错了，他的错误提示什么呢?

> 验证失败:1:内部版本控制不能用于乐观并发控制。请改用“if-seq-no”和“if-primary-term”;

这两个参数到底是干什么的呢?

官方地址: https://www.elastic.co/guide/en/elasticsearch/reference/6.8/breaking-changes-6.7.html#breaking-changes-6.7

> ![](https://notebook1.oss-cn-shenzhen.aliyuncs.com/img/Elasticsearch/20201220144815.png)
>
> **索引更改
> ** **不赞成使用版本控制进行乐观并发控制**
>
> internal 如果主文档发生故障时未完全复制索引文档，则version可能无法唯一标识文档的版本。因此，用于乐观并发控制是不安全的，已弃用，并且该选项在Elasticsearch 7.0.0中将不再可用。请改用 `if_seq_no` 和 `if_primary_term` 参数.

那么这两个参数又是干什么的呢? 

`if_seq_no` 和 `if_primary_term`，官方文档已经有比较详细的叙述，

官方地址: https://www.elastic.co/guide/en/elasticsearch/reference/6.7/optimistic-concurrency-control.html

这里我说下简单的理解方式，对于`if_primary_term`记录的就是具体的哪个主分片，而`if_seq_no`这个参数起的作用和旧版本中的 *version if_primary_term* 这个参数主要是提高并发的性能，因为每个`document`都只会在某一个主分片中，所以由所在主分片分配序列号比由之前通过一个参数version，相当于由整个ES集群分配版本号要来的更好。

> To ensure an older version of a document doesn’t overwrite a newer version, every operation performed to a document is assigned a sequence number by the primary shard that coordinates that change. The sequence number is increased with each operation and thus newer operations are guaranteed to have a higher sequence number than older operations. Elasticsearch can then use the sequence number of operations to make sure a newer document version is never overridden by a change that has a smaller sequence number assigned to it. 
>
> 简单翻译就是为确保较旧版本的文档不会覆盖较新版本，对文档执行的每个操作都会由协调该更 改的主分片分配序列号。每次操作都会增加序列号，因此保证较新的操作具有比旧操作更高的序 列号。然后，Elasticsearch可以使用序列号操作来确保更新的文档版本永远不会被分配给它的序 列号更小的更改覆盖。

5)在乐观锁成功阻止并发问题之后，尝试正确的完成更新

```json
PUT /test_index/_doc/7?if_seq_no=1&if_primary_term=1
{
	"test_field": "mm数据3" 
}
现在显示更新成功。
```

(6)现在我们另外一个客户端尝试去更新我们的数据

```json
PUT /test_index/_doc/7?if_seq_no=1&if_primary_term=1
{
"test_field": "mm数据4" }
{
	"error": {
	"root_cause": [
      {
        "type": "version_conflict_engine_exception",
        "reason": "[test_type][7]: version conflict, current version [2] is different than the one provided [1]",
        "index_uuid": "6m0G7yx7R1KECWWGnfH1sw",
        "shard": "3",
        "index": "test_index"
      } 
  ],
    "type": "version_conflict_engine_exception",
    "reason": "[test_type][7]: version conflict, current version [2] is different than the one provided [1]",
    "index_uuid": "6m0G7yx7R1KECWWGnfH1sw",
    "shard": "3",
    "index": "test_index"
  },
  "status": 409
}
```

这个时候我们发现我们报错了，因为我们的现在的if_seq_no并不是ES库中的那个。 

当我们基于最新的数据和版本号，去进行修改，修改后，带上最新的版本号，可能这个步骤会需要反复执行好几次，才能成功，特别是在多线程并发更新同一条数据很频繁的情况下

```json
PUT /test_index/_doc/7?if_seq_no=2&if_primary_term=1
{
	"test_field": "mm数据4" 
}


{
  "_index": "test_index",
  "_type": "test_type",
  "_id": "7",
  "_version": 3,
  "result": "updated",
  "_shards": {
    "total": 2,
    "successful": 1,
    "failed": 0
	},
  "created": false
}
```

当你进行删除操作的时候，我们的`if_seq_no`依然会继续增加，当你再次创建的时候他的ID还会再次增加1，`if_seq_no`递增属于整个index，而不是单个文档

**基于external version进行乐观锁的并发控制(6.7版本已移除)**

ES版本号是存储在自己的数据库中的，不可以由开发人员自己控制。

主要是因为:

https://www.elastic.co/guide/en/elasticsearch/reference/6.2/docs-update.html

The update API does not support versioning other than internal
 External (version types external and external_gte) or forced (version type force) versioning is not supported by the update API as it would result in Elasticsearch version numbers being out of sync with the external system. 

更新API不支持外部(版本类型external和external_gte)或强制(版本类型force)版本控制，因为它会导致Elasticsearch版本号与外部系统不同步。说句实话，我觉得是借口。我觉得本质的理由是ES的版本控制现在可能不能做成你想怎么样就怎么样的自由组件，毕竟现在技术都往收费走了，所以不能让你们自己开发了，都用我的。

## 集群部署

好了，接下来我们就可以尝试部署集群了，这个部署可以保证你们小集群(也就是10台ES服务器以内能够正常使用)， 我们先下载es 7.9.1的压缩包，部署到3个节点上面去，然后呢，我们回去模拟生产环境讲我们ES的3节点集群给启动起来，包括用kibana访问。

我们接下来就开始考虑问题了，首先我们会考虑什么问题呢?

其实我们第一个考虑的问题就是我们在多台机器上，每台机器部署一个es进程，每台机器都启动一个es 进程，你怎么让多台机器上的多个es进程，互相发现对方，然后完美的组成一个生产环境的es集群呢?

这其实也就是问我们的ES是如何做服务发现的呢?

默认情况下，es进程会绑定在自己的本地回环地址上，也就是127.0.0.1，然后扫描本机上的 9300~9305端口号，尝试跟那些端口上启动的其他es进程进行通信，然后组成一个集群。这对于在本机上搭建es集群的开发环境是很方便的。但是对于生产环境下的集群是不行的，需要将每台es进程绑定在一个非回环的ip地址上，也就是我们当前服务器的ip上，才能跟其他节点进行通信，同时需要使用集群发现机制来跟其他节点上的es node进行通信。

OK，废话不多说，我们先部署一个ES集群。

建议各位生产环境下的配置最好是将date跟log保存在其他目录下，不然的话我们一旦遇到ES版本更新或者其他情况，就会丢失生产数据。这可不是小问题。他在配置文件中有两个配置，可以直接将我们的数据配置到其他目录下。一般我习惯直接在root下面直接创建一个date以及一个logs，

然后我们可以在elasticSearch.yml配置文件里面进行配置

```sh
vi /es/elasticsearch.7.9.1/config/elasticsearch.yml
# 增加配置
path.data: /data
path.logs: /logs
```

然后除了这些我们还需要配置一些其他的东西，比如第一台

```sh
# 集群名称
cluster.name: es
# 节点名称
node.name: node1 
# 是不是有资格主节点
node.master: true
#是否存储数据
node.data: true
# 最大集群节点数 
node.max_local_storage_nodes: 3 
# ip地址 你的IP地址，请自行配置
network.host: 0.0.0.0
# 端口
http.port: 9201
# 内部节点之间沟通端口
transport.tcp.port: 9700
# es7.x 之后新增的配置，节点发现
discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"] 
# es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master 
cluster.initial_master_nodes: ["node1", "node2","node3"]
# 数据和存储路径 
path.data: /data 
path.logs: /logs
```

这个时候我们可以来启动我们的ES节点1，我们可以发现started，启动成功，同时可以从日志中看到: `master not discovered yet` 。也就是还没有发现主节点

这个时候我们去启动我们早就准备好的第二台集群，我们先来进行我们配置文件的设置，我们的配置文件只需要更改我们的端口号。启动一下。

第二台服务器配置文件

```sh
# 集群名称
cluster.name: es
# 节点名称
node.name: node2
# 是不是有资格主节点
node.master: true
# 是否存储数据
node.data: true
# 最大集群节点数
node.max_local_storage_nodes: 3
# ip地址 你的IP地址，请自行配置
network.host: 0.0.0.0
# 端口
http.port: 9202
# 内部节点之间沟通端口
transport.tcp.port: 9800
# es7.x 之后新增的配置，节点发现
discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"] 
# es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master 
cluster.initial_master_nodes: ["node1", "node2","node3"]
# 数据和存储路径
path.data: /data
path.logs: /logs
```

可以从日志中看到:

master not discovered yet。还没有发现主节点.

master node changed.已经选举出主节点 current【node1】

访问集群状态信息 http://192.168.149.135:9201/_cat/health?v 成功

```sh
健康状况结果解释:

cluster 集群名称 
status 集群状态
	- green 代表健康; 
	- yellow 代表分配了所有主分片，但至少缺少一个副本，此时集群数据仍旧完整; 
	= red 代表部分主分片不可用，可能已经丢失数据。

node.total 代表在线的节点总数量 
node.data 代表在线的数据节点的数量
shards 存活的分片数量
pri 存活的主分片数量 正常情况下shards的数量是pri的两倍。 
relo 迁移中的分片数量，正常情况为 0
init 初始化中的分片数量 正常情况为 0 
unassign 未分配的分片 正常情况为 0 
pending_tasks 准备中的任务，任务指迁移分片等 正常情况为 0 
max_task_wait_time 任务最长等待时间 
active_shards_percent 正常分片百分比 正常情况为 100%
```

这个时候我们就可以启动我们的第三个ES节点， 第三台服务器配置文件

```sh
# 集群名称
cluster.name: es
# 节点名称
node.name: node3
# 是不是有资格主节点
node.master: true
# 是否存储数据
node.data: true
# 最大集群节点数
node.max_local_storage_nodes: 3
# ip地址 你的IP地址，请自行配置
network.host: 0.0.0.0
# 端口
http.port: 9203
# 内部节点之间沟通端口
transport.tcp.port: 9900
# es7.x 之后新增的配置，节点发现
discovery.seed_hosts: ["localhost:9700","localhost:9800","localhost:9900"] 
# es7.x 之后新增的配置，初始化一个新的集群时需要此配置来选举master 
cluster.initial_master_nodes: ["node1", "node2","node3"]
# 数据和存储路径
path.data: /data
path.logs: /logs
```

启动完成之后，我们去访问
集群状态信息 http://192.168.149.135:9201/_cat/health?v 成功。 

我们这个时候就能够看到集群的节点数已经变成了三个，我们的ES集群就已经成功搭建了

### 使用Kibana配置和管理集群

**集群配置**

因为之前我们在单机演示的时候也使用到了Kibana，我们直接将他修改成集群配置 

修改Kibana的集群配置

```sh
vi kibana-7.4.0-linux-x86_64-cluster/config/kibana.yml
加入下面的配置
elasticsearch.hosts: ["http://localhost:9201","http://localhost:9202","http://localhost:9203"] 
elasticsearch.requestTimeout: 99999
server.host: "0.0.0.0"
server.name: "kibana"
server.port: 5601
```

启动Kibana

```sh
./bin/kibana
```

这个时候我们就可以打开我们的kibana界面，进行我们的集群管理了。点开 Stack Monitoring 集群监控

首先我们可以点开我们的Stack Monitoring 集群监控页面，我们发现了我们ES版本号以及我们一些其他的数据。

同时我们还能看到我们的分片以及文档信息，这个时候，我们点开我们的Nodes，就可以查看我们节点的详细信息。我们会发现我们有一个status显示是green，这代表我们的集群是健康的。我们发现我们往下看，node2旁边是星星，这表示是我们的主节点。